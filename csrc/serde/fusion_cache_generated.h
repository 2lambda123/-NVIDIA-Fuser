// automatically generated by the FlatBuffers compiler, do not modify


#ifndef FLATBUFFERS_GENERATED_FUSIONCACHE_NVFUSER_SERDE_H_
#define FLATBUFFERS_GENERATED_FUSIONCACHE_NVFUSER_SERDE_H_

#include "flatbuffers/flatbuffers.h"

// Ensure the included flatbuffers.h is the same version as when this file was
// generated, otherwise it may not be compatible.
static_assert(FLATBUFFERS_VERSION_MAJOR == 23 &&
              FLATBUFFERS_VERSION_MINOR == 3 &&
              FLATBUFFERS_VERSION_REVISION == 3,
             "Non-compatible flatbuffers version included");

namespace nvfuser {
namespace serde {

struct State;

struct Scalar;
struct ScalarBuilder;
struct ScalarT;

struct BinaryOp;
struct BinaryOpBuilder;
struct BinaryOpT;

struct GetAttr;
struct GetAttrBuilder;
struct GetAttrT;

struct GetItem;
struct GetItemBuilder;
struct GetItemT;

struct GetMetaData;
struct GetMetaDataBuilder;
struct GetMetaDataT;

struct Merge;
struct MergeBuilder;
struct MergeT;

struct NamedScalar;
struct NamedScalarBuilder;
struct NamedScalarT;

struct Resize;
struct ResizeBuilder;
struct ResizeT;

struct Split;
struct SplitBuilder;
struct SplitT;

struct Swizzle2D;
struct Swizzle2DBuilder;
struct Swizzle2DT;

struct Symbolic;
struct SymbolicBuilder;
struct SymbolicT;

struct UnaryOp;
struct UnaryOpBuilder;
struct UnaryOpT;

struct Instruction;
struct InstructionBuilder;
struct InstructionT;

struct NaiveValueGenerator;
struct NaiveValueGeneratorBuilder;
struct NaiveValueGeneratorT;

struct IterationDomain;
struct IterationDomainBuilder;
struct IterationDomainT;

struct Domain;
struct DomainBuilder;
struct DomainT;

struct SymbolicTensor;
struct SymbolicTensorBuilder;
struct SymbolicTensorT;

struct AllocateBuffer;
struct AllocateBufferBuilder;
struct AllocateBufferT;

struct ScalarCpu;
struct ScalarCpuBuilder;
struct ScalarCpuT;

struct TensorArg;
struct TensorArgBuilder;
struct TensorArgT;

struct PolymorphicValue;
struct PolymorphicValueBuilder;
struct PolymorphicValueT;

struct KernelArgumentHolder;
struct KernelArgumentHolderBuilder;
struct KernelArgumentHolderT;

struct TensorShape;
struct TensorShapeBuilder;
struct TensorShapeT;

struct LaunchParams;
struct LaunchParamsBuilder;
struct LaunchParamsT;

struct GlobalBufferInfo;
struct GlobalBufferInfoBuilder;
struct GlobalBufferInfoT;

struct ExecutorEntry;
struct ExecutorEntryBuilder;
struct ExecutorEntryT;

struct At;
struct AtBuilder;
struct AtT;

struct BatchNorm;
struct BatchNormBuilder;
struct BatchNormT;

struct Broadcast;
struct BroadcastBuilder;
struct BroadcastT;

struct BroadcastInDim;
struct BroadcastInDimBuilder;
struct BroadcastInDimT;

struct Dtype;
struct DtypeBuilder;
struct DtypeT;

struct Dimension;
struct DimensionBuilder;
struct DimensionT;

struct Norm;
struct NormBuilder;
struct NormT;

struct Output;
struct OutputBuilder;
struct OutputT;

struct Pad;
struct PadBuilder;
struct PadT;

struct Permute;
struct PermuteBuilder;
struct PermuteT;

struct Reduction;
struct ReductionBuilder;
struct ReductionT;

struct Reshape;
struct ReshapeBuilder;
struct ReshapeT;

struct Size;
struct SizeBuilder;
struct SizeT;

struct Slice;
struct SliceBuilder;
struct SliceT;

struct Squeeze;
struct SqueezeBuilder;
struct SqueezeT;

struct Tensor;
struct TensorBuilder;
struct TensorT;

struct TensorCreation;
struct TensorCreationBuilder;
struct TensorCreationT;

struct TensorCreationSymbolic;
struct TensorCreationSymbolicBuilder;
struct TensorCreationSymbolicT;

struct Vector;
struct VectorBuilder;
struct VectorT;

struct KernelSummary;
struct KernelSummaryBuilder;
struct KernelSummaryT;

struct FusionExecutor;
struct FusionExecutorBuilder;
struct FusionExecutorT;

struct FusionKernelRuntime;
struct FusionKernelRuntimeBuilder;
struct FusionKernelRuntimeT;

struct EncodingEntry;

struct InputsIdLookup;
struct InputsIdLookupBuilder;
struct InputsIdLookupT;

struct KernelRuntimeState;
struct KernelRuntimeStateBuilder;
struct KernelRuntimeStateT;

struct FusionExecutorCache;
struct FusionExecutorCacheBuilder;
struct FusionExecutorCacheT;

struct RecordFunctor;
struct RecordFunctorBuilder;
struct RecordFunctorT;

struct TrieNode;
struct TrieNodeBuilder;
struct TrieNodeT;

struct FusionCache;
struct FusionCacheBuilder;
struct FusionCacheT;

enum DataType : int32_t {
  DataType_None = 0,
  DataType_Double = 1,
  DataType_Float = 2,
  DataType_Half = 3,
  DataType_Index = 4,
  DataType_Int = 5,
  DataType_Int32 = 6,
  DataType_Bool = 7,
  DataType_BFloat16 = 8,
  DataType_ComplexFloat = 9,
  DataType_ComplexDouble = 10,
  DataType_MIN = DataType_None,
  DataType_MAX = DataType_ComplexDouble
};

inline const DataType (&EnumValuesDataType())[11] {
  static const DataType values[] = {
    DataType_None,
    DataType_Double,
    DataType_Float,
    DataType_Half,
    DataType_Index,
    DataType_Int,
    DataType_Int32,
    DataType_Bool,
    DataType_BFloat16,
    DataType_ComplexFloat,
    DataType_ComplexDouble
  };
  return values;
}

inline const char * const *EnumNamesDataType() {
  static const char * const names[12] = {
    "None",
    "Double",
    "Float",
    "Half",
    "Index",
    "Int",
    "Int32",
    "Bool",
    "BFloat16",
    "ComplexFloat",
    "ComplexDouble",
    nullptr
  };
  return names;
}

inline const char *EnumNameDataType(DataType e) {
  if (::flatbuffers::IsOutRange(e, DataType_None, DataType_ComplexDouble)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesDataType()[index];
}

enum StateType : int32_t {
  StateType_None = 0,
  StateType_Scalar = 1,
  StateType_Vector = 2,
  StateType_Tensor = 3,
  StateType_MIN = StateType_None,
  StateType_MAX = StateType_Tensor
};

inline const StateType (&EnumValuesStateType())[4] {
  static const StateType values[] = {
    StateType_None,
    StateType_Scalar,
    StateType_Vector,
    StateType_Tensor
  };
  return values;
}

inline const char * const *EnumNamesStateType() {
  static const char * const names[5] = {
    "None",
    "Scalar",
    "Vector",
    "Tensor",
    nullptr
  };
  return names;
}

inline const char *EnumNameStateType(StateType e) {
  if (::flatbuffers::IsOutRange(e, StateType_None, StateType_Tensor)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesStateType()[index];
}

enum Contiguity : int32_t {
  Contiguity_None = 0,
  Contiguity_Strided = 1,
  Contiguity_Contiguous = 2,
  Contiguity_MIN = Contiguity_None,
  Contiguity_MAX = Contiguity_Contiguous
};

inline const Contiguity (&EnumValuesContiguity())[3] {
  static const Contiguity values[] = {
    Contiguity_None,
    Contiguity_Strided,
    Contiguity_Contiguous
  };
  return values;
}

inline const char * const *EnumNamesContiguity() {
  static const char * const names[4] = {
    "None",
    "Strided",
    "Contiguous",
    nullptr
  };
  return names;
}

inline const char *EnumNameContiguity(Contiguity e) {
  if (::flatbuffers::IsOutRange(e, Contiguity_None, Contiguity_Contiguous)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesContiguity()[index];
}

enum RecordType : int32_t {
  RecordType_Base = 0,
  RecordType_AtOp = 1,
  RecordType_BatchNormOp = 2,
  RecordType_BroadcastOp = 3,
  RecordType_BroadcastInDim = 4,
  RecordType_CastTv = 5,
  RecordType_CastVal = 6,
  RecordType_CatOp = 7,
  RecordType_End = 8,
  RecordType_FullOp = 9,
  RecordType_IotaOp = 10,
  RecordType_IndexSelectOp = 11,
  RecordType_TorchGatherOp = 12,
  RecordType_TakeAlongAxisOp = 13,
  RecordType_Unary_TV = 14,
  RecordType_Unary_VAL = 15,
  RecordType_Binary_TV = 16,
  RecordType_Binary_VAL = 17,
  RecordType_Binary_TV_VAL = 18,
  RecordType_Binary_VAL_TV = 19,
  RecordType_Ternary_TV = 20,
  RecordType_Ternary_VAL = 21,
  RecordType_Ternary_TV_TV_VAL = 22,
  RecordType_Ternary_TV_VAL_TV = 23,
  RecordType_Ternary_VAL_TV_TV = 24,
  RecordType_Ternary_VAL_VAL_TV = 25,
  RecordType_Ternary_TV_VAL_VAL = 26,
  RecordType_Ternary_VAL_TV_VAL = 27,
  RecordType_Ternary_Alpha_TV = 28,
  RecordType_Ternary_Alpha_VAL = 29,
  RecordType_Ternary_Alpha_TV_TV_VAL = 30,
  RecordType_Ternary_Alpha_TV_VAL_TV = 31,
  RecordType_Ternary_Alpha_VAL_TV_TV = 32,
  RecordType_Ternary_Alpha_VAL_VAL_TV = 33,
  RecordType_Ternary_Alpha_TV_VAL_VAL = 34,
  RecordType_Ternary_Alpha_VAL_TV_VAL = 35,
  RecordType_OutputTv = 36,
  RecordType_OutputVal = 37,
  RecordType_PadOp = 38,
  RecordType_PermuteOp = 39,
  RecordType_RandomOp = 40,
  RecordType_ReductionMax = 41,
  RecordType_ReductionMin = 42,
  RecordType_ReductionProd = 43,
  RecordType_ReductionSum = 44,
  RecordType_ReshapeOp = 45,
  RecordType_Scalar = 46,
  RecordType_ShapeOp = 47,
  RecordType_SizeOp = 48,
  RecordType_SliceOp = 49,
  RecordType_SqueezeOp = 50,
  RecordType_Start = 51,
  RecordType_Tensor = 52,
  RecordType_TensorSizes = 53,
  RecordType_VarianceOp = 54,
  RecordType_VarianceMeanOp = 55,
  RecordType_Vector = 56,
  RecordType_MIN = RecordType_Base,
  RecordType_MAX = RecordType_Vector
};

inline const RecordType (&EnumValuesRecordType())[57] {
  static const RecordType values[] = {
    RecordType_Base,
    RecordType_AtOp,
    RecordType_BatchNormOp,
    RecordType_BroadcastOp,
    RecordType_BroadcastInDim,
    RecordType_CastTv,
    RecordType_CastVal,
    RecordType_CatOp,
    RecordType_End,
    RecordType_FullOp,
    RecordType_IotaOp,
    RecordType_IndexSelectOp,
    RecordType_TorchGatherOp,
    RecordType_TakeAlongAxisOp,
    RecordType_Unary_TV,
    RecordType_Unary_VAL,
    RecordType_Binary_TV,
    RecordType_Binary_VAL,
    RecordType_Binary_TV_VAL,
    RecordType_Binary_VAL_TV,
    RecordType_Ternary_TV,
    RecordType_Ternary_VAL,
    RecordType_Ternary_TV_TV_VAL,
    RecordType_Ternary_TV_VAL_TV,
    RecordType_Ternary_VAL_TV_TV,
    RecordType_Ternary_VAL_VAL_TV,
    RecordType_Ternary_TV_VAL_VAL,
    RecordType_Ternary_VAL_TV_VAL,
    RecordType_Ternary_Alpha_TV,
    RecordType_Ternary_Alpha_VAL,
    RecordType_Ternary_Alpha_TV_TV_VAL,
    RecordType_Ternary_Alpha_TV_VAL_TV,
    RecordType_Ternary_Alpha_VAL_TV_TV,
    RecordType_Ternary_Alpha_VAL_VAL_TV,
    RecordType_Ternary_Alpha_TV_VAL_VAL,
    RecordType_Ternary_Alpha_VAL_TV_VAL,
    RecordType_OutputTv,
    RecordType_OutputVal,
    RecordType_PadOp,
    RecordType_PermuteOp,
    RecordType_RandomOp,
    RecordType_ReductionMax,
    RecordType_ReductionMin,
    RecordType_ReductionProd,
    RecordType_ReductionSum,
    RecordType_ReshapeOp,
    RecordType_Scalar,
    RecordType_ShapeOp,
    RecordType_SizeOp,
    RecordType_SliceOp,
    RecordType_SqueezeOp,
    RecordType_Start,
    RecordType_Tensor,
    RecordType_TensorSizes,
    RecordType_VarianceOp,
    RecordType_VarianceMeanOp,
    RecordType_Vector
  };
  return values;
}

inline const char * const *EnumNamesRecordType() {
  static const char * const names[58] = {
    "Base",
    "AtOp",
    "BatchNormOp",
    "BroadcastOp",
    "BroadcastInDim",
    "CastTv",
    "CastVal",
    "CatOp",
    "End",
    "FullOp",
    "IotaOp",
    "IndexSelectOp",
    "TorchGatherOp",
    "TakeAlongAxisOp",
    "Unary_TV",
    "Unary_VAL",
    "Binary_TV",
    "Binary_VAL",
    "Binary_TV_VAL",
    "Binary_VAL_TV",
    "Ternary_TV",
    "Ternary_VAL",
    "Ternary_TV_TV_VAL",
    "Ternary_TV_VAL_TV",
    "Ternary_VAL_TV_TV",
    "Ternary_VAL_VAL_TV",
    "Ternary_TV_VAL_VAL",
    "Ternary_VAL_TV_VAL",
    "Ternary_Alpha_TV",
    "Ternary_Alpha_VAL",
    "Ternary_Alpha_TV_TV_VAL",
    "Ternary_Alpha_TV_VAL_TV",
    "Ternary_Alpha_VAL_TV_TV",
    "Ternary_Alpha_VAL_VAL_TV",
    "Ternary_Alpha_TV_VAL_VAL",
    "Ternary_Alpha_VAL_TV_VAL",
    "OutputTv",
    "OutputVal",
    "PadOp",
    "PermuteOp",
    "RandomOp",
    "ReductionMax",
    "ReductionMin",
    "ReductionProd",
    "ReductionSum",
    "ReshapeOp",
    "Scalar",
    "ShapeOp",
    "SizeOp",
    "SliceOp",
    "SqueezeOp",
    "Start",
    "Tensor",
    "TensorSizes",
    "VarianceOp",
    "VarianceMeanOp",
    "Vector",
    nullptr
  };
  return names;
}

inline const char *EnumNameRecordType(RecordType e) {
  if (::flatbuffers::IsOutRange(e, RecordType_Base, RecordType_Vector)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesRecordType()[index];
}

enum UnaryOpType : int32_t {
  UnaryOpType_None = 0,
  UnaryOpType_Cast = 1,
  UnaryOpType_Neg = 2,
  UnaryOpType_MIN = UnaryOpType_None,
  UnaryOpType_MAX = UnaryOpType_Neg
};

inline const UnaryOpType (&EnumValuesUnaryOpType())[3] {
  static const UnaryOpType values[] = {
    UnaryOpType_None,
    UnaryOpType_Cast,
    UnaryOpType_Neg
  };
  return values;
}

inline const char * const *EnumNamesUnaryOpType() {
  static const char * const names[4] = {
    "None",
    "Cast",
    "Neg",
    nullptr
  };
  return names;
}

inline const char *EnumNameUnaryOpType(UnaryOpType e) {
  if (::flatbuffers::IsOutRange(e, UnaryOpType_None, UnaryOpType_Neg)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesUnaryOpType()[index];
}

enum BinaryOpType : int32_t {
  BinaryOpType_None = 0,
  BinaryOpType_Add = 1,
  BinaryOpType_CeilDiv = 2,
  BinaryOpType_Div = 3,
  BinaryOpType_Mod = 4,
  BinaryOpType_Mul = 5,
  BinaryOpType_Sub = 6,
  BinaryOpType_MIN = BinaryOpType_None,
  BinaryOpType_MAX = BinaryOpType_Sub
};

inline const BinaryOpType (&EnumValuesBinaryOpType())[7] {
  static const BinaryOpType values[] = {
    BinaryOpType_None,
    BinaryOpType_Add,
    BinaryOpType_CeilDiv,
    BinaryOpType_Div,
    BinaryOpType_Mod,
    BinaryOpType_Mul,
    BinaryOpType_Sub
  };
  return values;
}

inline const char * const *EnumNamesBinaryOpType() {
  static const char * const names[8] = {
    "None",
    "Add",
    "CeilDiv",
    "Div",
    "Mod",
    "Mul",
    "Sub",
    nullptr
  };
  return names;
}

inline const char *EnumNameBinaryOpType(BinaryOpType e) {
  if (::flatbuffers::IsOutRange(e, BinaryOpType_None, BinaryOpType_Sub)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesBinaryOpType()[index];
}

enum Swizzle2DType : int32_t {
  Swizzle2DType_None = 0,
  Swizzle2DType_ZShape = 1,
  Swizzle2DType_Xor = 2,
  Swizzle2DType_Shift = 3,
  Swizzle2DType_MIN = Swizzle2DType_None,
  Swizzle2DType_MAX = Swizzle2DType_Shift
};

inline const Swizzle2DType (&EnumValuesSwizzle2DType())[4] {
  static const Swizzle2DType values[] = {
    Swizzle2DType_None,
    Swizzle2DType_ZShape,
    Swizzle2DType_Xor,
    Swizzle2DType_Shift
  };
  return values;
}

inline const char * const *EnumNamesSwizzle2DType() {
  static const char * const names[5] = {
    "None",
    "ZShape",
    "Xor",
    "Shift",
    nullptr
  };
  return names;
}

inline const char *EnumNameSwizzle2DType(Swizzle2DType e) {
  if (::flatbuffers::IsOutRange(e, Swizzle2DType_None, Swizzle2DType_Shift)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesSwizzle2DType()[index];
}

enum SwizzleMode : int32_t {
  SwizzleMode_None = 0,
  SwizzleMode_Data = 1,
  SwizzleMode_Loop = 2,
  SwizzleMode_MIN = SwizzleMode_None,
  SwizzleMode_MAX = SwizzleMode_Loop
};

inline const SwizzleMode (&EnumValuesSwizzleMode())[3] {
  static const SwizzleMode values[] = {
    SwizzleMode_None,
    SwizzleMode_Data,
    SwizzleMode_Loop
  };
  return values;
}

inline const char * const *EnumNamesSwizzleMode() {
  static const char * const names[4] = {
    "None",
    "Data",
    "Loop",
    nullptr
  };
  return names;
}

inline const char *EnumNameSwizzleMode(SwizzleMode e) {
  if (::flatbuffers::IsOutRange(e, SwizzleMode_None, SwizzleMode_Loop)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesSwizzleMode()[index];
}

enum IterType : int32_t {
  IterType_Iteration = 0,
  IterType_Reduction = 1,
  IterType_Broadcast = 2,
  IterType_Gather = 3,
  IterType_Stride = 4,
  IterType_GatherScatter = 5,
  IterType_VectorComponent = 6,
  IterType_Symbolic = 7,
  IterType_MIN = IterType_Iteration,
  IterType_MAX = IterType_Symbolic
};

inline const IterType (&EnumValuesIterType())[8] {
  static const IterType values[] = {
    IterType_Iteration,
    IterType_Reduction,
    IterType_Broadcast,
    IterType_Gather,
    IterType_Stride,
    IterType_GatherScatter,
    IterType_VectorComponent,
    IterType_Symbolic
  };
  return values;
}

inline const char * const *EnumNamesIterType() {
  static const char * const names[9] = {
    "Iteration",
    "Reduction",
    "Broadcast",
    "Gather",
    "Stride",
    "GatherScatter",
    "VectorComponent",
    "Symbolic",
    nullptr
  };
  return names;
}

inline const char *EnumNameIterType(IterType e) {
  if (::flatbuffers::IsOutRange(e, IterType_Iteration, IterType_Symbolic)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesIterType()[index];
}

enum RecordData : uint8_t {
  RecordData_NONE = 0,
  RecordData_At = 1,
  RecordData_BatchNorm = 2,
  RecordData_Broadcast = 3,
  RecordData_BroadcastInDim = 4,
  RecordData_Dimension = 5,
  RecordData_Dtype = 6,
  RecordData_Norm = 7,
  RecordData_Output = 8,
  RecordData_Pad = 9,
  RecordData_Permute = 10,
  RecordData_Slice = 11,
  RecordData_Squeeze = 12,
  RecordData_Reduction = 13,
  RecordData_Reshape = 14,
  RecordData_Scalar = 15,
  RecordData_Size = 16,
  RecordData_Tensor = 17,
  RecordData_TensorCreation = 18,
  RecordData_TensorCreationSymbolic = 19,
  RecordData_Vector = 20,
  RecordData_MIN = RecordData_NONE,
  RecordData_MAX = RecordData_Vector
};

inline const RecordData (&EnumValuesRecordData())[21] {
  static const RecordData values[] = {
    RecordData_NONE,
    RecordData_At,
    RecordData_BatchNorm,
    RecordData_Broadcast,
    RecordData_BroadcastInDim,
    RecordData_Dimension,
    RecordData_Dtype,
    RecordData_Norm,
    RecordData_Output,
    RecordData_Pad,
    RecordData_Permute,
    RecordData_Slice,
    RecordData_Squeeze,
    RecordData_Reduction,
    RecordData_Reshape,
    RecordData_Scalar,
    RecordData_Size,
    RecordData_Tensor,
    RecordData_TensorCreation,
    RecordData_TensorCreationSymbolic,
    RecordData_Vector
  };
  return values;
}

inline const char * const *EnumNamesRecordData() {
  static const char * const names[22] = {
    "NONE",
    "At",
    "BatchNorm",
    "Broadcast",
    "BroadcastInDim",
    "Dimension",
    "Dtype",
    "Norm",
    "Output",
    "Pad",
    "Permute",
    "Slice",
    "Squeeze",
    "Reduction",
    "Reshape",
    "Scalar",
    "Size",
    "Tensor",
    "TensorCreation",
    "TensorCreationSymbolic",
    "Vector",
    nullptr
  };
  return names;
}

inline const char *EnumNameRecordData(RecordData e) {
  if (::flatbuffers::IsOutRange(e, RecordData_NONE, RecordData_Vector)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesRecordData()[index];
}

template<typename T> struct RecordDataTraits {
  static const RecordData enum_value = RecordData_NONE;
};

template<> struct RecordDataTraits<nvfuser::serde::At> {
  static const RecordData enum_value = RecordData_At;
};

template<> struct RecordDataTraits<nvfuser::serde::BatchNorm> {
  static const RecordData enum_value = RecordData_BatchNorm;
};

template<> struct RecordDataTraits<nvfuser::serde::Broadcast> {
  static const RecordData enum_value = RecordData_Broadcast;
};

template<> struct RecordDataTraits<nvfuser::serde::BroadcastInDim> {
  static const RecordData enum_value = RecordData_BroadcastInDim;
};

template<> struct RecordDataTraits<nvfuser::serde::Dimension> {
  static const RecordData enum_value = RecordData_Dimension;
};

template<> struct RecordDataTraits<nvfuser::serde::Dtype> {
  static const RecordData enum_value = RecordData_Dtype;
};

template<> struct RecordDataTraits<nvfuser::serde::Norm> {
  static const RecordData enum_value = RecordData_Norm;
};

template<> struct RecordDataTraits<nvfuser::serde::Output> {
  static const RecordData enum_value = RecordData_Output;
};

template<> struct RecordDataTraits<nvfuser::serde::Pad> {
  static const RecordData enum_value = RecordData_Pad;
};

template<> struct RecordDataTraits<nvfuser::serde::Permute> {
  static const RecordData enum_value = RecordData_Permute;
};

template<> struct RecordDataTraits<nvfuser::serde::Slice> {
  static const RecordData enum_value = RecordData_Slice;
};

template<> struct RecordDataTraits<nvfuser::serde::Squeeze> {
  static const RecordData enum_value = RecordData_Squeeze;
};

template<> struct RecordDataTraits<nvfuser::serde::Reduction> {
  static const RecordData enum_value = RecordData_Reduction;
};

template<> struct RecordDataTraits<nvfuser::serde::Reshape> {
  static const RecordData enum_value = RecordData_Reshape;
};

template<> struct RecordDataTraits<nvfuser::serde::Scalar> {
  static const RecordData enum_value = RecordData_Scalar;
};

template<> struct RecordDataTraits<nvfuser::serde::Size> {
  static const RecordData enum_value = RecordData_Size;
};

template<> struct RecordDataTraits<nvfuser::serde::Tensor> {
  static const RecordData enum_value = RecordData_Tensor;
};

template<> struct RecordDataTraits<nvfuser::serde::TensorCreation> {
  static const RecordData enum_value = RecordData_TensorCreation;
};

template<> struct RecordDataTraits<nvfuser::serde::TensorCreationSymbolic> {
  static const RecordData enum_value = RecordData_TensorCreationSymbolic;
};

template<> struct RecordDataTraits<nvfuser::serde::Vector> {
  static const RecordData enum_value = RecordData_Vector;
};

template<typename T> struct RecordDataUnionTraits {
  static const RecordData enum_value = RecordData_NONE;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::AtT> {
  static const RecordData enum_value = RecordData_At;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::BatchNormT> {
  static const RecordData enum_value = RecordData_BatchNorm;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::BroadcastT> {
  static const RecordData enum_value = RecordData_Broadcast;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::BroadcastInDimT> {
  static const RecordData enum_value = RecordData_BroadcastInDim;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::DimensionT> {
  static const RecordData enum_value = RecordData_Dimension;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::DtypeT> {
  static const RecordData enum_value = RecordData_Dtype;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::NormT> {
  static const RecordData enum_value = RecordData_Norm;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::OutputT> {
  static const RecordData enum_value = RecordData_Output;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::PadT> {
  static const RecordData enum_value = RecordData_Pad;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::PermuteT> {
  static const RecordData enum_value = RecordData_Permute;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::SliceT> {
  static const RecordData enum_value = RecordData_Slice;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::SqueezeT> {
  static const RecordData enum_value = RecordData_Squeeze;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::ReductionT> {
  static const RecordData enum_value = RecordData_Reduction;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::ReshapeT> {
  static const RecordData enum_value = RecordData_Reshape;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::ScalarT> {
  static const RecordData enum_value = RecordData_Scalar;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::SizeT> {
  static const RecordData enum_value = RecordData_Size;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::TensorT> {
  static const RecordData enum_value = RecordData_Tensor;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::TensorCreationT> {
  static const RecordData enum_value = RecordData_TensorCreation;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::TensorCreationSymbolicT> {
  static const RecordData enum_value = RecordData_TensorCreationSymbolic;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::VectorT> {
  static const RecordData enum_value = RecordData_Vector;
};

struct RecordDataUnion {
  RecordData type;
  void *value;

  RecordDataUnion() : type(RecordData_NONE), value(nullptr) {}
  RecordDataUnion(RecordDataUnion&& u) FLATBUFFERS_NOEXCEPT :
    type(RecordData_NONE), value(nullptr)
    { std::swap(type, u.type); std::swap(value, u.value); }
  RecordDataUnion(const RecordDataUnion &);
  RecordDataUnion &operator=(const RecordDataUnion &u)
    { RecordDataUnion t(u); std::swap(type, t.type); std::swap(value, t.value); return *this; }
  RecordDataUnion &operator=(RecordDataUnion &&u) FLATBUFFERS_NOEXCEPT
    { std::swap(type, u.type); std::swap(value, u.value); return *this; }
  ~RecordDataUnion() { Reset(); }

  void Reset();

  template <typename T>
  void Set(T&& val) {
    typedef typename std::remove_reference<T>::type RT;
    Reset();
    type = RecordDataUnionTraits<RT>::enum_value;
    if (type != RecordData_NONE) {
      value = new RT(std::forward<T>(val));
    }
  }

  static void *UnPack(const void *obj, RecordData type, const ::flatbuffers::resolver_function_t *resolver);
  ::flatbuffers::Offset<void> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr) const;

  nvfuser::serde::AtT *AsAt() {
    return type == RecordData_At ?
      reinterpret_cast<nvfuser::serde::AtT *>(value) : nullptr;
  }
  const nvfuser::serde::AtT *AsAt() const {
    return type == RecordData_At ?
      reinterpret_cast<const nvfuser::serde::AtT *>(value) : nullptr;
  }
  nvfuser::serde::BatchNormT *AsBatchNorm() {
    return type == RecordData_BatchNorm ?
      reinterpret_cast<nvfuser::serde::BatchNormT *>(value) : nullptr;
  }
  const nvfuser::serde::BatchNormT *AsBatchNorm() const {
    return type == RecordData_BatchNorm ?
      reinterpret_cast<const nvfuser::serde::BatchNormT *>(value) : nullptr;
  }
  nvfuser::serde::BroadcastT *AsBroadcast() {
    return type == RecordData_Broadcast ?
      reinterpret_cast<nvfuser::serde::BroadcastT *>(value) : nullptr;
  }
  const nvfuser::serde::BroadcastT *AsBroadcast() const {
    return type == RecordData_Broadcast ?
      reinterpret_cast<const nvfuser::serde::BroadcastT *>(value) : nullptr;
  }
  nvfuser::serde::BroadcastInDimT *AsBroadcastInDim() {
    return type == RecordData_BroadcastInDim ?
      reinterpret_cast<nvfuser::serde::BroadcastInDimT *>(value) : nullptr;
  }
  const nvfuser::serde::BroadcastInDimT *AsBroadcastInDim() const {
    return type == RecordData_BroadcastInDim ?
      reinterpret_cast<const nvfuser::serde::BroadcastInDimT *>(value) : nullptr;
  }
  nvfuser::serde::DimensionT *AsDimension() {
    return type == RecordData_Dimension ?
      reinterpret_cast<nvfuser::serde::DimensionT *>(value) : nullptr;
  }
  const nvfuser::serde::DimensionT *AsDimension() const {
    return type == RecordData_Dimension ?
      reinterpret_cast<const nvfuser::serde::DimensionT *>(value) : nullptr;
  }
  nvfuser::serde::DtypeT *AsDtype() {
    return type == RecordData_Dtype ?
      reinterpret_cast<nvfuser::serde::DtypeT *>(value) : nullptr;
  }
  const nvfuser::serde::DtypeT *AsDtype() const {
    return type == RecordData_Dtype ?
      reinterpret_cast<const nvfuser::serde::DtypeT *>(value) : nullptr;
  }
  nvfuser::serde::NormT *AsNorm() {
    return type == RecordData_Norm ?
      reinterpret_cast<nvfuser::serde::NormT *>(value) : nullptr;
  }
  const nvfuser::serde::NormT *AsNorm() const {
    return type == RecordData_Norm ?
      reinterpret_cast<const nvfuser::serde::NormT *>(value) : nullptr;
  }
  nvfuser::serde::OutputT *AsOutput() {
    return type == RecordData_Output ?
      reinterpret_cast<nvfuser::serde::OutputT *>(value) : nullptr;
  }
  const nvfuser::serde::OutputT *AsOutput() const {
    return type == RecordData_Output ?
      reinterpret_cast<const nvfuser::serde::OutputT *>(value) : nullptr;
  }
  nvfuser::serde::PadT *AsPad() {
    return type == RecordData_Pad ?
      reinterpret_cast<nvfuser::serde::PadT *>(value) : nullptr;
  }
  const nvfuser::serde::PadT *AsPad() const {
    return type == RecordData_Pad ?
      reinterpret_cast<const nvfuser::serde::PadT *>(value) : nullptr;
  }
  nvfuser::serde::PermuteT *AsPermute() {
    return type == RecordData_Permute ?
      reinterpret_cast<nvfuser::serde::PermuteT *>(value) : nullptr;
  }
  const nvfuser::serde::PermuteT *AsPermute() const {
    return type == RecordData_Permute ?
      reinterpret_cast<const nvfuser::serde::PermuteT *>(value) : nullptr;
  }
  nvfuser::serde::SliceT *AsSlice() {
    return type == RecordData_Slice ?
      reinterpret_cast<nvfuser::serde::SliceT *>(value) : nullptr;
  }
  const nvfuser::serde::SliceT *AsSlice() const {
    return type == RecordData_Slice ?
      reinterpret_cast<const nvfuser::serde::SliceT *>(value) : nullptr;
  }
  nvfuser::serde::SqueezeT *AsSqueeze() {
    return type == RecordData_Squeeze ?
      reinterpret_cast<nvfuser::serde::SqueezeT *>(value) : nullptr;
  }
  const nvfuser::serde::SqueezeT *AsSqueeze() const {
    return type == RecordData_Squeeze ?
      reinterpret_cast<const nvfuser::serde::SqueezeT *>(value) : nullptr;
  }
  nvfuser::serde::ReductionT *AsReduction() {
    return type == RecordData_Reduction ?
      reinterpret_cast<nvfuser::serde::ReductionT *>(value) : nullptr;
  }
  const nvfuser::serde::ReductionT *AsReduction() const {
    return type == RecordData_Reduction ?
      reinterpret_cast<const nvfuser::serde::ReductionT *>(value) : nullptr;
  }
  nvfuser::serde::ReshapeT *AsReshape() {
    return type == RecordData_Reshape ?
      reinterpret_cast<nvfuser::serde::ReshapeT *>(value) : nullptr;
  }
  const nvfuser::serde::ReshapeT *AsReshape() const {
    return type == RecordData_Reshape ?
      reinterpret_cast<const nvfuser::serde::ReshapeT *>(value) : nullptr;
  }
  nvfuser::serde::ScalarT *AsScalar() {
    return type == RecordData_Scalar ?
      reinterpret_cast<nvfuser::serde::ScalarT *>(value) : nullptr;
  }
  const nvfuser::serde::ScalarT *AsScalar() const {
    return type == RecordData_Scalar ?
      reinterpret_cast<const nvfuser::serde::ScalarT *>(value) : nullptr;
  }
  nvfuser::serde::SizeT *AsSize() {
    return type == RecordData_Size ?
      reinterpret_cast<nvfuser::serde::SizeT *>(value) : nullptr;
  }
  const nvfuser::serde::SizeT *AsSize() const {
    return type == RecordData_Size ?
      reinterpret_cast<const nvfuser::serde::SizeT *>(value) : nullptr;
  }
  nvfuser::serde::TensorT *AsTensor() {
    return type == RecordData_Tensor ?
      reinterpret_cast<nvfuser::serde::TensorT *>(value) : nullptr;
  }
  const nvfuser::serde::TensorT *AsTensor() const {
    return type == RecordData_Tensor ?
      reinterpret_cast<const nvfuser::serde::TensorT *>(value) : nullptr;
  }
  nvfuser::serde::TensorCreationT *AsTensorCreation() {
    return type == RecordData_TensorCreation ?
      reinterpret_cast<nvfuser::serde::TensorCreationT *>(value) : nullptr;
  }
  const nvfuser::serde::TensorCreationT *AsTensorCreation() const {
    return type == RecordData_TensorCreation ?
      reinterpret_cast<const nvfuser::serde::TensorCreationT *>(value) : nullptr;
  }
  nvfuser::serde::TensorCreationSymbolicT *AsTensorCreationSymbolic() {
    return type == RecordData_TensorCreationSymbolic ?
      reinterpret_cast<nvfuser::serde::TensorCreationSymbolicT *>(value) : nullptr;
  }
  const nvfuser::serde::TensorCreationSymbolicT *AsTensorCreationSymbolic() const {
    return type == RecordData_TensorCreationSymbolic ?
      reinterpret_cast<const nvfuser::serde::TensorCreationSymbolicT *>(value) : nullptr;
  }
  nvfuser::serde::VectorT *AsVector() {
    return type == RecordData_Vector ?
      reinterpret_cast<nvfuser::serde::VectorT *>(value) : nullptr;
  }
  const nvfuser::serde::VectorT *AsVector() const {
    return type == RecordData_Vector ?
      reinterpret_cast<const nvfuser::serde::VectorT *>(value) : nullptr;
  }
};

bool VerifyRecordData(::flatbuffers::Verifier &verifier, const void *obj, RecordData type);
bool VerifyRecordDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types);

enum PolymorphicValueData : uint8_t {
  PolymorphicValueData_NONE = 0,
  PolymorphicValueData_Scalar = 1,
  PolymorphicValueData_ScalarCpu = 2,
  PolymorphicValueData_TensorArg = 3,
  PolymorphicValueData_MIN = PolymorphicValueData_NONE,
  PolymorphicValueData_MAX = PolymorphicValueData_TensorArg
};

inline const PolymorphicValueData (&EnumValuesPolymorphicValueData())[4] {
  static const PolymorphicValueData values[] = {
    PolymorphicValueData_NONE,
    PolymorphicValueData_Scalar,
    PolymorphicValueData_ScalarCpu,
    PolymorphicValueData_TensorArg
  };
  return values;
}

inline const char * const *EnumNamesPolymorphicValueData() {
  static const char * const names[5] = {
    "NONE",
    "Scalar",
    "ScalarCpu",
    "TensorArg",
    nullptr
  };
  return names;
}

inline const char *EnumNamePolymorphicValueData(PolymorphicValueData e) {
  if (::flatbuffers::IsOutRange(e, PolymorphicValueData_NONE, PolymorphicValueData_TensorArg)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesPolymorphicValueData()[index];
}

template<typename T> struct PolymorphicValueDataTraits {
  static const PolymorphicValueData enum_value = PolymorphicValueData_NONE;
};

template<> struct PolymorphicValueDataTraits<nvfuser::serde::Scalar> {
  static const PolymorphicValueData enum_value = PolymorphicValueData_Scalar;
};

template<> struct PolymorphicValueDataTraits<nvfuser::serde::ScalarCpu> {
  static const PolymorphicValueData enum_value = PolymorphicValueData_ScalarCpu;
};

template<> struct PolymorphicValueDataTraits<nvfuser::serde::TensorArg> {
  static const PolymorphicValueData enum_value = PolymorphicValueData_TensorArg;
};

template<typename T> struct PolymorphicValueDataUnionTraits {
  static const PolymorphicValueData enum_value = PolymorphicValueData_NONE;
};

template<> struct PolymorphicValueDataUnionTraits<nvfuser::serde::ScalarT> {
  static const PolymorphicValueData enum_value = PolymorphicValueData_Scalar;
};

template<> struct PolymorphicValueDataUnionTraits<nvfuser::serde::ScalarCpuT> {
  static const PolymorphicValueData enum_value = PolymorphicValueData_ScalarCpu;
};

template<> struct PolymorphicValueDataUnionTraits<nvfuser::serde::TensorArgT> {
  static const PolymorphicValueData enum_value = PolymorphicValueData_TensorArg;
};

struct PolymorphicValueDataUnion {
  PolymorphicValueData type;
  void *value;

  PolymorphicValueDataUnion() : type(PolymorphicValueData_NONE), value(nullptr) {}
  PolymorphicValueDataUnion(PolymorphicValueDataUnion&& u) FLATBUFFERS_NOEXCEPT :
    type(PolymorphicValueData_NONE), value(nullptr)
    { std::swap(type, u.type); std::swap(value, u.value); }
  PolymorphicValueDataUnion(const PolymorphicValueDataUnion &);
  PolymorphicValueDataUnion &operator=(const PolymorphicValueDataUnion &u)
    { PolymorphicValueDataUnion t(u); std::swap(type, t.type); std::swap(value, t.value); return *this; }
  PolymorphicValueDataUnion &operator=(PolymorphicValueDataUnion &&u) FLATBUFFERS_NOEXCEPT
    { std::swap(type, u.type); std::swap(value, u.value); return *this; }
  ~PolymorphicValueDataUnion() { Reset(); }

  void Reset();

  template <typename T>
  void Set(T&& val) {
    typedef typename std::remove_reference<T>::type RT;
    Reset();
    type = PolymorphicValueDataUnionTraits<RT>::enum_value;
    if (type != PolymorphicValueData_NONE) {
      value = new RT(std::forward<T>(val));
    }
  }

  static void *UnPack(const void *obj, PolymorphicValueData type, const ::flatbuffers::resolver_function_t *resolver);
  ::flatbuffers::Offset<void> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr) const;

  nvfuser::serde::ScalarT *AsScalar() {
    return type == PolymorphicValueData_Scalar ?
      reinterpret_cast<nvfuser::serde::ScalarT *>(value) : nullptr;
  }
  const nvfuser::serde::ScalarT *AsScalar() const {
    return type == PolymorphicValueData_Scalar ?
      reinterpret_cast<const nvfuser::serde::ScalarT *>(value) : nullptr;
  }
  nvfuser::serde::ScalarCpuT *AsScalarCpu() {
    return type == PolymorphicValueData_ScalarCpu ?
      reinterpret_cast<nvfuser::serde::ScalarCpuT *>(value) : nullptr;
  }
  const nvfuser::serde::ScalarCpuT *AsScalarCpu() const {
    return type == PolymorphicValueData_ScalarCpu ?
      reinterpret_cast<const nvfuser::serde::ScalarCpuT *>(value) : nullptr;
  }
  nvfuser::serde::TensorArgT *AsTensorArg() {
    return type == PolymorphicValueData_TensorArg ?
      reinterpret_cast<nvfuser::serde::TensorArgT *>(value) : nullptr;
  }
  const nvfuser::serde::TensorArgT *AsTensorArg() const {
    return type == PolymorphicValueData_TensorArg ?
      reinterpret_cast<const nvfuser::serde::TensorArgT *>(value) : nullptr;
  }
};

bool VerifyPolymorphicValueData(::flatbuffers::Verifier &verifier, const void *obj, PolymorphicValueData type);
bool VerifyPolymorphicValueDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types);

enum InstructionData : uint8_t {
  InstructionData_NONE = 0,
  InstructionData_BinaryOp = 1,
  InstructionData_GetAttr = 2,
  InstructionData_GetItem = 3,
  InstructionData_GetMetaData = 4,
  InstructionData_Merge = 5,
  InstructionData_NamedScalar = 6,
  InstructionData_Resize = 7,
  InstructionData_Scalar = 8,
  InstructionData_Split = 9,
  InstructionData_Swizzle2D = 10,
  InstructionData_Symbolic = 11,
  InstructionData_UnaryOp = 12,
  InstructionData_MIN = InstructionData_NONE,
  InstructionData_MAX = InstructionData_UnaryOp
};

inline const InstructionData (&EnumValuesInstructionData())[13] {
  static const InstructionData values[] = {
    InstructionData_NONE,
    InstructionData_BinaryOp,
    InstructionData_GetAttr,
    InstructionData_GetItem,
    InstructionData_GetMetaData,
    InstructionData_Merge,
    InstructionData_NamedScalar,
    InstructionData_Resize,
    InstructionData_Scalar,
    InstructionData_Split,
    InstructionData_Swizzle2D,
    InstructionData_Symbolic,
    InstructionData_UnaryOp
  };
  return values;
}

inline const char * const *EnumNamesInstructionData() {
  static const char * const names[14] = {
    "NONE",
    "BinaryOp",
    "GetAttr",
    "GetItem",
    "GetMetaData",
    "Merge",
    "NamedScalar",
    "Resize",
    "Scalar",
    "Split",
    "Swizzle2D",
    "Symbolic",
    "UnaryOp",
    nullptr
  };
  return names;
}

inline const char *EnumNameInstructionData(InstructionData e) {
  if (::flatbuffers::IsOutRange(e, InstructionData_NONE, InstructionData_UnaryOp)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesInstructionData()[index];
}

template<typename T> struct InstructionDataTraits {
  static const InstructionData enum_value = InstructionData_NONE;
};

template<> struct InstructionDataTraits<nvfuser::serde::BinaryOp> {
  static const InstructionData enum_value = InstructionData_BinaryOp;
};

template<> struct InstructionDataTraits<nvfuser::serde::GetAttr> {
  static const InstructionData enum_value = InstructionData_GetAttr;
};

template<> struct InstructionDataTraits<nvfuser::serde::GetItem> {
  static const InstructionData enum_value = InstructionData_GetItem;
};

template<> struct InstructionDataTraits<nvfuser::serde::GetMetaData> {
  static const InstructionData enum_value = InstructionData_GetMetaData;
};

template<> struct InstructionDataTraits<nvfuser::serde::Merge> {
  static const InstructionData enum_value = InstructionData_Merge;
};

template<> struct InstructionDataTraits<nvfuser::serde::NamedScalar> {
  static const InstructionData enum_value = InstructionData_NamedScalar;
};

template<> struct InstructionDataTraits<nvfuser::serde::Resize> {
  static const InstructionData enum_value = InstructionData_Resize;
};

template<> struct InstructionDataTraits<nvfuser::serde::Scalar> {
  static const InstructionData enum_value = InstructionData_Scalar;
};

template<> struct InstructionDataTraits<nvfuser::serde::Split> {
  static const InstructionData enum_value = InstructionData_Split;
};

template<> struct InstructionDataTraits<nvfuser::serde::Swizzle2D> {
  static const InstructionData enum_value = InstructionData_Swizzle2D;
};

template<> struct InstructionDataTraits<nvfuser::serde::Symbolic> {
  static const InstructionData enum_value = InstructionData_Symbolic;
};

template<> struct InstructionDataTraits<nvfuser::serde::UnaryOp> {
  static const InstructionData enum_value = InstructionData_UnaryOp;
};

template<typename T> struct InstructionDataUnionTraits {
  static const InstructionData enum_value = InstructionData_NONE;
};

template<> struct InstructionDataUnionTraits<nvfuser::serde::BinaryOpT> {
  static const InstructionData enum_value = InstructionData_BinaryOp;
};

template<> struct InstructionDataUnionTraits<nvfuser::serde::GetAttrT> {
  static const InstructionData enum_value = InstructionData_GetAttr;
};

template<> struct InstructionDataUnionTraits<nvfuser::serde::GetItemT> {
  static const InstructionData enum_value = InstructionData_GetItem;
};

template<> struct InstructionDataUnionTraits<nvfuser::serde::GetMetaDataT> {
  static const InstructionData enum_value = InstructionData_GetMetaData;
};

template<> struct InstructionDataUnionTraits<nvfuser::serde::MergeT> {
  static const InstructionData enum_value = InstructionData_Merge;
};

template<> struct InstructionDataUnionTraits<nvfuser::serde::NamedScalarT> {
  static const InstructionData enum_value = InstructionData_NamedScalar;
};

template<> struct InstructionDataUnionTraits<nvfuser::serde::ResizeT> {
  static const InstructionData enum_value = InstructionData_Resize;
};

template<> struct InstructionDataUnionTraits<nvfuser::serde::ScalarT> {
  static const InstructionData enum_value = InstructionData_Scalar;
};

template<> struct InstructionDataUnionTraits<nvfuser::serde::SplitT> {
  static const InstructionData enum_value = InstructionData_Split;
};

template<> struct InstructionDataUnionTraits<nvfuser::serde::Swizzle2DT> {
  static const InstructionData enum_value = InstructionData_Swizzle2D;
};

template<> struct InstructionDataUnionTraits<nvfuser::serde::SymbolicT> {
  static const InstructionData enum_value = InstructionData_Symbolic;
};

template<> struct InstructionDataUnionTraits<nvfuser::serde::UnaryOpT> {
  static const InstructionData enum_value = InstructionData_UnaryOp;
};

struct InstructionDataUnion {
  InstructionData type;
  void *value;

  InstructionDataUnion() : type(InstructionData_NONE), value(nullptr) {}
  InstructionDataUnion(InstructionDataUnion&& u) FLATBUFFERS_NOEXCEPT :
    type(InstructionData_NONE), value(nullptr)
    { std::swap(type, u.type); std::swap(value, u.value); }
  InstructionDataUnion(const InstructionDataUnion &);
  InstructionDataUnion &operator=(const InstructionDataUnion &u)
    { InstructionDataUnion t(u); std::swap(type, t.type); std::swap(value, t.value); return *this; }
  InstructionDataUnion &operator=(InstructionDataUnion &&u) FLATBUFFERS_NOEXCEPT
    { std::swap(type, u.type); std::swap(value, u.value); return *this; }
  ~InstructionDataUnion() { Reset(); }

  void Reset();

  template <typename T>
  void Set(T&& val) {
    typedef typename std::remove_reference<T>::type RT;
    Reset();
    type = InstructionDataUnionTraits<RT>::enum_value;
    if (type != InstructionData_NONE) {
      value = new RT(std::forward<T>(val));
    }
  }

  static void *UnPack(const void *obj, InstructionData type, const ::flatbuffers::resolver_function_t *resolver);
  ::flatbuffers::Offset<void> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr) const;

  nvfuser::serde::BinaryOpT *AsBinaryOp() {
    return type == InstructionData_BinaryOp ?
      reinterpret_cast<nvfuser::serde::BinaryOpT *>(value) : nullptr;
  }
  const nvfuser::serde::BinaryOpT *AsBinaryOp() const {
    return type == InstructionData_BinaryOp ?
      reinterpret_cast<const nvfuser::serde::BinaryOpT *>(value) : nullptr;
  }
  nvfuser::serde::GetAttrT *AsGetAttr() {
    return type == InstructionData_GetAttr ?
      reinterpret_cast<nvfuser::serde::GetAttrT *>(value) : nullptr;
  }
  const nvfuser::serde::GetAttrT *AsGetAttr() const {
    return type == InstructionData_GetAttr ?
      reinterpret_cast<const nvfuser::serde::GetAttrT *>(value) : nullptr;
  }
  nvfuser::serde::GetItemT *AsGetItem() {
    return type == InstructionData_GetItem ?
      reinterpret_cast<nvfuser::serde::GetItemT *>(value) : nullptr;
  }
  const nvfuser::serde::GetItemT *AsGetItem() const {
    return type == InstructionData_GetItem ?
      reinterpret_cast<const nvfuser::serde::GetItemT *>(value) : nullptr;
  }
  nvfuser::serde::GetMetaDataT *AsGetMetaData() {
    return type == InstructionData_GetMetaData ?
      reinterpret_cast<nvfuser::serde::GetMetaDataT *>(value) : nullptr;
  }
  const nvfuser::serde::GetMetaDataT *AsGetMetaData() const {
    return type == InstructionData_GetMetaData ?
      reinterpret_cast<const nvfuser::serde::GetMetaDataT *>(value) : nullptr;
  }
  nvfuser::serde::MergeT *AsMerge() {
    return type == InstructionData_Merge ?
      reinterpret_cast<nvfuser::serde::MergeT *>(value) : nullptr;
  }
  const nvfuser::serde::MergeT *AsMerge() const {
    return type == InstructionData_Merge ?
      reinterpret_cast<const nvfuser::serde::MergeT *>(value) : nullptr;
  }
  nvfuser::serde::NamedScalarT *AsNamedScalar() {
    return type == InstructionData_NamedScalar ?
      reinterpret_cast<nvfuser::serde::NamedScalarT *>(value) : nullptr;
  }
  const nvfuser::serde::NamedScalarT *AsNamedScalar() const {
    return type == InstructionData_NamedScalar ?
      reinterpret_cast<const nvfuser::serde::NamedScalarT *>(value) : nullptr;
  }
  nvfuser::serde::ResizeT *AsResize() {
    return type == InstructionData_Resize ?
      reinterpret_cast<nvfuser::serde::ResizeT *>(value) : nullptr;
  }
  const nvfuser::serde::ResizeT *AsResize() const {
    return type == InstructionData_Resize ?
      reinterpret_cast<const nvfuser::serde::ResizeT *>(value) : nullptr;
  }
  nvfuser::serde::ScalarT *AsScalar() {
    return type == InstructionData_Scalar ?
      reinterpret_cast<nvfuser::serde::ScalarT *>(value) : nullptr;
  }
  const nvfuser::serde::ScalarT *AsScalar() const {
    return type == InstructionData_Scalar ?
      reinterpret_cast<const nvfuser::serde::ScalarT *>(value) : nullptr;
  }
  nvfuser::serde::SplitT *AsSplit() {
    return type == InstructionData_Split ?
      reinterpret_cast<nvfuser::serde::SplitT *>(value) : nullptr;
  }
  const nvfuser::serde::SplitT *AsSplit() const {
    return type == InstructionData_Split ?
      reinterpret_cast<const nvfuser::serde::SplitT *>(value) : nullptr;
  }
  nvfuser::serde::Swizzle2DT *AsSwizzle2D() {
    return type == InstructionData_Swizzle2D ?
      reinterpret_cast<nvfuser::serde::Swizzle2DT *>(value) : nullptr;
  }
  const nvfuser::serde::Swizzle2DT *AsSwizzle2D() const {
    return type == InstructionData_Swizzle2D ?
      reinterpret_cast<const nvfuser::serde::Swizzle2DT *>(value) : nullptr;
  }
  nvfuser::serde::SymbolicT *AsSymbolic() {
    return type == InstructionData_Symbolic ?
      reinterpret_cast<nvfuser::serde::SymbolicT *>(value) : nullptr;
  }
  const nvfuser::serde::SymbolicT *AsSymbolic() const {
    return type == InstructionData_Symbolic ?
      reinterpret_cast<const nvfuser::serde::SymbolicT *>(value) : nullptr;
  }
  nvfuser::serde::UnaryOpT *AsUnaryOp() {
    return type == InstructionData_UnaryOp ?
      reinterpret_cast<nvfuser::serde::UnaryOpT *>(value) : nullptr;
  }
  const nvfuser::serde::UnaryOpT *AsUnaryOp() const {
    return type == InstructionData_UnaryOp ?
      reinterpret_cast<const nvfuser::serde::UnaryOpT *>(value) : nullptr;
  }
};

bool VerifyInstructionData(::flatbuffers::Verifier &verifier, const void *obj, InstructionData type);
bool VerifyInstructionDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types);

FLATBUFFERS_MANUALLY_ALIGNED_STRUCT(4) State FLATBUFFERS_FINAL_CLASS {
 private:
  int32_t index_;
  int32_t type_;

 public:
  State()
      : index_(0),
        type_(0) {
  }
  State(int32_t _index, nvfuser::serde::StateType _type)
      : index_(::flatbuffers::EndianScalar(_index)),
        type_(::flatbuffers::EndianScalar(static_cast<int32_t>(_type))) {
  }
  int32_t index() const {
    return ::flatbuffers::EndianScalar(index_);
  }
  nvfuser::serde::StateType type() const {
    return static_cast<nvfuser::serde::StateType>(::flatbuffers::EndianScalar(type_));
  }
};
FLATBUFFERS_STRUCT_END(State, 8);

FLATBUFFERS_MANUALLY_ALIGNED_STRUCT(8) EncodingEntry FLATBUFFERS_FINAL_CLASS {
 private:
  uint64_t id_;
  uint64_t lru_iter_;

 public:
  EncodingEntry()
      : id_(0),
        lru_iter_(0) {
  }
  EncodingEntry(uint64_t _id, uint64_t _lru_iter)
      : id_(::flatbuffers::EndianScalar(_id)),
        lru_iter_(::flatbuffers::EndianScalar(_lru_iter)) {
  }
  uint64_t id() const {
    return ::flatbuffers::EndianScalar(id_);
  }
  uint64_t lru_iter() const {
    return ::flatbuffers::EndianScalar(lru_iter_);
  }
};
FLATBUFFERS_STRUCT_END(EncodingEntry, 16);

struct ScalarT : public ::flatbuffers::NativeTable {
  typedef Scalar TableType;
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None;
  bool has_value = false;
  nvfuser::serde::DataType value_type = nvfuser::serde::DataType_None;
  bool bool_value = false;
  int64_t long_value = 0;
  double double_value = 0.0;
  double real_value = 0.0;
  double imag_value = 0.0;
};

struct Scalar FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ScalarT NativeTableType;
  typedef ScalarBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4,
    VT_HAS_VALUE = 6,
    VT_VALUE_TYPE = 8,
    VT_BOOL_VALUE = 10,
    VT_LONG_VALUE = 12,
    VT_DOUBLE_VALUE = 14,
    VT_REAL_VALUE = 16,
    VT_IMAG_VALUE = 18
  };
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool has_value() const {
    return GetField<uint8_t>(VT_HAS_VALUE, 0) != 0;
  }
  nvfuser::serde::DataType value_type() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_VALUE_TYPE, 0));
  }
  bool bool_value() const {
    return GetField<uint8_t>(VT_BOOL_VALUE, 0) != 0;
  }
  int64_t long_value() const {
    return GetField<int64_t>(VT_LONG_VALUE, 0);
  }
  double double_value() const {
    return GetField<double>(VT_DOUBLE_VALUE, 0.0);
  }
  double real_value() const {
    return GetField<double>(VT_REAL_VALUE, 0.0);
  }
  double imag_value() const {
    return GetField<double>(VT_IMAG_VALUE, 0.0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_HAS_VALUE, 1) &&
           VerifyField<int32_t>(verifier, VT_VALUE_TYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_BOOL_VALUE, 1) &&
           VerifyField<int64_t>(verifier, VT_LONG_VALUE, 8) &&
           VerifyField<double>(verifier, VT_DOUBLE_VALUE, 8) &&
           VerifyField<double>(verifier, VT_REAL_VALUE, 8) &&
           VerifyField<double>(verifier, VT_IMAG_VALUE, 8) &&
           verifier.EndTable();
  }
  ScalarT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(ScalarT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Scalar> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ScalarT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct ScalarBuilder {
  typedef Scalar Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Scalar::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  void add_has_value(bool has_value) {
    fbb_.AddElement<uint8_t>(Scalar::VT_HAS_VALUE, static_cast<uint8_t>(has_value), 0);
  }
  void add_value_type(nvfuser::serde::DataType value_type) {
    fbb_.AddElement<int32_t>(Scalar::VT_VALUE_TYPE, static_cast<int32_t>(value_type), 0);
  }
  void add_bool_value(bool bool_value) {
    fbb_.AddElement<uint8_t>(Scalar::VT_BOOL_VALUE, static_cast<uint8_t>(bool_value), 0);
  }
  void add_long_value(int64_t long_value) {
    fbb_.AddElement<int64_t>(Scalar::VT_LONG_VALUE, long_value, 0);
  }
  void add_double_value(double double_value) {
    fbb_.AddElement<double>(Scalar::VT_DOUBLE_VALUE, double_value, 0.0);
  }
  void add_real_value(double real_value) {
    fbb_.AddElement<double>(Scalar::VT_REAL_VALUE, real_value, 0.0);
  }
  void add_imag_value(double imag_value) {
    fbb_.AddElement<double>(Scalar::VT_IMAG_VALUE, imag_value, 0.0);
  }
  explicit ScalarBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Scalar> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Scalar>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Scalar> CreateScalar(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None,
    bool has_value = false,
    nvfuser::serde::DataType value_type = nvfuser::serde::DataType_None,
    bool bool_value = false,
    int64_t long_value = 0,
    double double_value = 0.0,
    double real_value = 0.0,
    double imag_value = 0.0) {
  ScalarBuilder builder_(_fbb);
  builder_.add_imag_value(imag_value);
  builder_.add_real_value(real_value);
  builder_.add_double_value(double_value);
  builder_.add_long_value(long_value);
  builder_.add_value_type(value_type);
  builder_.add_dtype(dtype);
  builder_.add_bool_value(bool_value);
  builder_.add_has_value(has_value);
  return builder_.Finish();
}

::flatbuffers::Offset<Scalar> CreateScalar(::flatbuffers::FlatBufferBuilder &_fbb, const ScalarT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct BinaryOpT : public ::flatbuffers::NativeTable {
  typedef BinaryOp TableType;
  nvfuser::serde::BinaryOpType binary_type = nvfuser::serde::BinaryOpType_None;
  int64_t src0 = 0;
  int64_t src1 = 0;
  int64_t out = 0;
  std::string name{};
};

struct BinaryOp FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef BinaryOpT NativeTableType;
  typedef BinaryOpBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BINARY_TYPE = 4,
    VT_SRC0 = 6,
    VT_SRC1 = 8,
    VT_OUT = 10,
    VT_NAME = 12
  };
  nvfuser::serde::BinaryOpType binary_type() const {
    return static_cast<nvfuser::serde::BinaryOpType>(GetField<int32_t>(VT_BINARY_TYPE, 0));
  }
  int64_t src0() const {
    return GetField<int64_t>(VT_SRC0, 0);
  }
  int64_t src1() const {
    return GetField<int64_t>(VT_SRC1, 0);
  }
  int64_t out() const {
    return GetField<int64_t>(VT_OUT, 0);
  }
  const ::flatbuffers::String *name() const {
    return GetPointer<const ::flatbuffers::String *>(VT_NAME);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_BINARY_TYPE, 4) &&
           VerifyField<int64_t>(verifier, VT_SRC0, 8) &&
           VerifyField<int64_t>(verifier, VT_SRC1, 8) &&
           VerifyField<int64_t>(verifier, VT_OUT, 8) &&
           VerifyOffset(verifier, VT_NAME) &&
           verifier.VerifyString(name()) &&
           verifier.EndTable();
  }
  BinaryOpT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(BinaryOpT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<BinaryOp> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const BinaryOpT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct BinaryOpBuilder {
  typedef BinaryOp Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_binary_type(nvfuser::serde::BinaryOpType binary_type) {
    fbb_.AddElement<int32_t>(BinaryOp::VT_BINARY_TYPE, static_cast<int32_t>(binary_type), 0);
  }
  void add_src0(int64_t src0) {
    fbb_.AddElement<int64_t>(BinaryOp::VT_SRC0, src0, 0);
  }
  void add_src1(int64_t src1) {
    fbb_.AddElement<int64_t>(BinaryOp::VT_SRC1, src1, 0);
  }
  void add_out(int64_t out) {
    fbb_.AddElement<int64_t>(BinaryOp::VT_OUT, out, 0);
  }
  void add_name(::flatbuffers::Offset<::flatbuffers::String> name) {
    fbb_.AddOffset(BinaryOp::VT_NAME, name);
  }
  explicit BinaryOpBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<BinaryOp> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<BinaryOp>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<BinaryOp> CreateBinaryOp(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::BinaryOpType binary_type = nvfuser::serde::BinaryOpType_None,
    int64_t src0 = 0,
    int64_t src1 = 0,
    int64_t out = 0,
    ::flatbuffers::Offset<::flatbuffers::String> name = 0) {
  BinaryOpBuilder builder_(_fbb);
  builder_.add_out(out);
  builder_.add_src1(src1);
  builder_.add_src0(src0);
  builder_.add_name(name);
  builder_.add_binary_type(binary_type);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<BinaryOp> CreateBinaryOpDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::BinaryOpType binary_type = nvfuser::serde::BinaryOpType_None,
    int64_t src0 = 0,
    int64_t src1 = 0,
    int64_t out = 0,
    const char *name = nullptr) {
  auto name__ = name ? _fbb.CreateString(name) : 0;
  return nvfuser::serde::CreateBinaryOp(
      _fbb,
      binary_type,
      src0,
      src1,
      out,
      name__);
}

::flatbuffers::Offset<BinaryOp> CreateBinaryOp(::flatbuffers::FlatBufferBuilder &_fbb, const BinaryOpT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct GetAttrT : public ::flatbuffers::NativeTable {
  typedef GetAttr TableType;
  int64_t struct_ = 0;
  std::string attr{};
  int64_t out = 0;
};

struct GetAttr FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef GetAttrT NativeTableType;
  typedef GetAttrBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_STRUCT_ = 4,
    VT_ATTR = 6,
    VT_OUT = 8
  };
  int64_t struct_() const {
    return GetField<int64_t>(VT_STRUCT_, 0);
  }
  const ::flatbuffers::String *attr() const {
    return GetPointer<const ::flatbuffers::String *>(VT_ATTR);
  }
  int64_t out() const {
    return GetField<int64_t>(VT_OUT, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_STRUCT_, 8) &&
           VerifyOffset(verifier, VT_ATTR) &&
           verifier.VerifyString(attr()) &&
           VerifyField<int64_t>(verifier, VT_OUT, 8) &&
           verifier.EndTable();
  }
  GetAttrT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(GetAttrT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<GetAttr> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const GetAttrT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct GetAttrBuilder {
  typedef GetAttr Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_struct_(int64_t struct_) {
    fbb_.AddElement<int64_t>(GetAttr::VT_STRUCT_, struct_, 0);
  }
  void add_attr(::flatbuffers::Offset<::flatbuffers::String> attr) {
    fbb_.AddOffset(GetAttr::VT_ATTR, attr);
  }
  void add_out(int64_t out) {
    fbb_.AddElement<int64_t>(GetAttr::VT_OUT, out, 0);
  }
  explicit GetAttrBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<GetAttr> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<GetAttr>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<GetAttr> CreateGetAttr(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t struct_ = 0,
    ::flatbuffers::Offset<::flatbuffers::String> attr = 0,
    int64_t out = 0) {
  GetAttrBuilder builder_(_fbb);
  builder_.add_out(out);
  builder_.add_struct_(struct_);
  builder_.add_attr(attr);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<GetAttr> CreateGetAttrDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t struct_ = 0,
    const char *attr = nullptr,
    int64_t out = 0) {
  auto attr__ = attr ? _fbb.CreateString(attr) : 0;
  return nvfuser::serde::CreateGetAttr(
      _fbb,
      struct_,
      attr__,
      out);
}

::flatbuffers::Offset<GetAttr> CreateGetAttr(::flatbuffers::FlatBufferBuilder &_fbb, const GetAttrT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct GetItemT : public ::flatbuffers::NativeTable {
  typedef GetItem TableType;
  int64_t array = 0;
  int64_t index = 0;
  int64_t out = 0;
};

struct GetItem FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef GetItemT NativeTableType;
  typedef GetItemBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ARRAY = 4,
    VT_INDEX = 6,
    VT_OUT = 8
  };
  int64_t array() const {
    return GetField<int64_t>(VT_ARRAY, 0);
  }
  int64_t index() const {
    return GetField<int64_t>(VT_INDEX, 0);
  }
  int64_t out() const {
    return GetField<int64_t>(VT_OUT, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_ARRAY, 8) &&
           VerifyField<int64_t>(verifier, VT_INDEX, 8) &&
           VerifyField<int64_t>(verifier, VT_OUT, 8) &&
           verifier.EndTable();
  }
  GetItemT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(GetItemT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<GetItem> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const GetItemT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct GetItemBuilder {
  typedef GetItem Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_array(int64_t array) {
    fbb_.AddElement<int64_t>(GetItem::VT_ARRAY, array, 0);
  }
  void add_index(int64_t index) {
    fbb_.AddElement<int64_t>(GetItem::VT_INDEX, index, 0);
  }
  void add_out(int64_t out) {
    fbb_.AddElement<int64_t>(GetItem::VT_OUT, out, 0);
  }
  explicit GetItemBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<GetItem> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<GetItem>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<GetItem> CreateGetItem(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t array = 0,
    int64_t index = 0,
    int64_t out = 0) {
  GetItemBuilder builder_(_fbb);
  builder_.add_out(out);
  builder_.add_index(index);
  builder_.add_array(array);
  return builder_.Finish();
}

::flatbuffers::Offset<GetItem> CreateGetItem(::flatbuffers::FlatBufferBuilder &_fbb, const GetItemT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct GetMetaDataT : public ::flatbuffers::NativeTable {
  typedef GetMetaData TableType;
  int64_t in = 0;
  int64_t out = 0;
};

struct GetMetaData FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef GetMetaDataT NativeTableType;
  typedef GetMetaDataBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_IN = 4,
    VT_OUT = 6
  };
  int64_t in() const {
    return GetField<int64_t>(VT_IN, 0);
  }
  int64_t out() const {
    return GetField<int64_t>(VT_OUT, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_IN, 8) &&
           VerifyField<int64_t>(verifier, VT_OUT, 8) &&
           verifier.EndTable();
  }
  GetMetaDataT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(GetMetaDataT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<GetMetaData> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const GetMetaDataT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct GetMetaDataBuilder {
  typedef GetMetaData Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_in(int64_t in) {
    fbb_.AddElement<int64_t>(GetMetaData::VT_IN, in, 0);
  }
  void add_out(int64_t out) {
    fbb_.AddElement<int64_t>(GetMetaData::VT_OUT, out, 0);
  }
  explicit GetMetaDataBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<GetMetaData> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<GetMetaData>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<GetMetaData> CreateGetMetaData(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t in = 0,
    int64_t out = 0) {
  GetMetaDataBuilder builder_(_fbb);
  builder_.add_out(out);
  builder_.add_in(in);
  return builder_.Finish();
}

::flatbuffers::Offset<GetMetaData> CreateGetMetaData(::flatbuffers::FlatBufferBuilder &_fbb, const GetMetaDataT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct MergeT : public ::flatbuffers::NativeTable {
  typedef Merge TableType;
  int64_t inner = 0;
  int64_t outer = 0;
  int64_t out = 0;
};

struct Merge FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef MergeT NativeTableType;
  typedef MergeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INNER = 4,
    VT_OUTER = 6,
    VT_OUT = 8
  };
  int64_t inner() const {
    return GetField<int64_t>(VT_INNER, 0);
  }
  int64_t outer() const {
    return GetField<int64_t>(VT_OUTER, 0);
  }
  int64_t out() const {
    return GetField<int64_t>(VT_OUT, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_INNER, 8) &&
           VerifyField<int64_t>(verifier, VT_OUTER, 8) &&
           VerifyField<int64_t>(verifier, VT_OUT, 8) &&
           verifier.EndTable();
  }
  MergeT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(MergeT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Merge> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const MergeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct MergeBuilder {
  typedef Merge Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_inner(int64_t inner) {
    fbb_.AddElement<int64_t>(Merge::VT_INNER, inner, 0);
  }
  void add_outer(int64_t outer) {
    fbb_.AddElement<int64_t>(Merge::VT_OUTER, outer, 0);
  }
  void add_out(int64_t out) {
    fbb_.AddElement<int64_t>(Merge::VT_OUT, out, 0);
  }
  explicit MergeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Merge> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Merge>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Merge> CreateMerge(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t inner = 0,
    int64_t outer = 0,
    int64_t out = 0) {
  MergeBuilder builder_(_fbb);
  builder_.add_out(out);
  builder_.add_outer(outer);
  builder_.add_inner(inner);
  return builder_.Finish();
}

::flatbuffers::Offset<Merge> CreateMerge(::flatbuffers::FlatBufferBuilder &_fbb, const MergeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct NamedScalarT : public ::flatbuffers::NativeTable {
  typedef NamedScalar TableType;
  std::string name{};
};

struct NamedScalar FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef NamedScalarT NativeTableType;
  typedef NamedScalarBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_NAME = 4
  };
  const ::flatbuffers::String *name() const {
    return GetPointer<const ::flatbuffers::String *>(VT_NAME);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_NAME) &&
           verifier.VerifyString(name()) &&
           verifier.EndTable();
  }
  NamedScalarT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(NamedScalarT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<NamedScalar> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const NamedScalarT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct NamedScalarBuilder {
  typedef NamedScalar Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_name(::flatbuffers::Offset<::flatbuffers::String> name) {
    fbb_.AddOffset(NamedScalar::VT_NAME, name);
  }
  explicit NamedScalarBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<NamedScalar> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<NamedScalar>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<NamedScalar> CreateNamedScalar(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::String> name = 0) {
  NamedScalarBuilder builder_(_fbb);
  builder_.add_name(name);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<NamedScalar> CreateNamedScalarDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const char *name = nullptr) {
  auto name__ = name ? _fbb.CreateString(name) : 0;
  return nvfuser::serde::CreateNamedScalar(
      _fbb,
      name__);
}

::flatbuffers::Offset<NamedScalar> CreateNamedScalar(::flatbuffers::FlatBufferBuilder &_fbb, const NamedScalarT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct ResizeT : public ::flatbuffers::NativeTable {
  typedef Resize TableType;
  int64_t in = 0;
  int64_t left_expansion = 0;
  int64_t right_expansion = 0;
  int64_t out = 0;
};

struct Resize FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ResizeT NativeTableType;
  typedef ResizeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_IN = 4,
    VT_LEFT_EXPANSION = 6,
    VT_RIGHT_EXPANSION = 8,
    VT_OUT = 10
  };
  int64_t in() const {
    return GetField<int64_t>(VT_IN, 0);
  }
  int64_t left_expansion() const {
    return GetField<int64_t>(VT_LEFT_EXPANSION, 0);
  }
  int64_t right_expansion() const {
    return GetField<int64_t>(VT_RIGHT_EXPANSION, 0);
  }
  int64_t out() const {
    return GetField<int64_t>(VT_OUT, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_IN, 8) &&
           VerifyField<int64_t>(verifier, VT_LEFT_EXPANSION, 8) &&
           VerifyField<int64_t>(verifier, VT_RIGHT_EXPANSION, 8) &&
           VerifyField<int64_t>(verifier, VT_OUT, 8) &&
           verifier.EndTable();
  }
  ResizeT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(ResizeT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Resize> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ResizeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct ResizeBuilder {
  typedef Resize Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_in(int64_t in) {
    fbb_.AddElement<int64_t>(Resize::VT_IN, in, 0);
  }
  void add_left_expansion(int64_t left_expansion) {
    fbb_.AddElement<int64_t>(Resize::VT_LEFT_EXPANSION, left_expansion, 0);
  }
  void add_right_expansion(int64_t right_expansion) {
    fbb_.AddElement<int64_t>(Resize::VT_RIGHT_EXPANSION, right_expansion, 0);
  }
  void add_out(int64_t out) {
    fbb_.AddElement<int64_t>(Resize::VT_OUT, out, 0);
  }
  explicit ResizeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Resize> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Resize>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Resize> CreateResize(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t in = 0,
    int64_t left_expansion = 0,
    int64_t right_expansion = 0,
    int64_t out = 0) {
  ResizeBuilder builder_(_fbb);
  builder_.add_out(out);
  builder_.add_right_expansion(right_expansion);
  builder_.add_left_expansion(left_expansion);
  builder_.add_in(in);
  return builder_.Finish();
}

::flatbuffers::Offset<Resize> CreateResize(::flatbuffers::FlatBufferBuilder &_fbb, const ResizeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct SplitT : public ::flatbuffers::NativeTable {
  typedef Split TableType;
  int64_t in = 0;
  int64_t factor = 0;
  int64_t inner = 0;
  int64_t outer = 0;
  bool inner_split = false;
  bool trim_out_of_bounds = false;
};

struct Split FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SplitT NativeTableType;
  typedef SplitBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_IN = 4,
    VT_FACTOR = 6,
    VT_INNER = 8,
    VT_OUTER = 10,
    VT_INNER_SPLIT = 12,
    VT_TRIM_OUT_OF_BOUNDS = 14
  };
  int64_t in() const {
    return GetField<int64_t>(VT_IN, 0);
  }
  int64_t factor() const {
    return GetField<int64_t>(VT_FACTOR, 0);
  }
  int64_t inner() const {
    return GetField<int64_t>(VT_INNER, 0);
  }
  int64_t outer() const {
    return GetField<int64_t>(VT_OUTER, 0);
  }
  bool inner_split() const {
    return GetField<uint8_t>(VT_INNER_SPLIT, 0) != 0;
  }
  bool trim_out_of_bounds() const {
    return GetField<uint8_t>(VT_TRIM_OUT_OF_BOUNDS, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_IN, 8) &&
           VerifyField<int64_t>(verifier, VT_FACTOR, 8) &&
           VerifyField<int64_t>(verifier, VT_INNER, 8) &&
           VerifyField<int64_t>(verifier, VT_OUTER, 8) &&
           VerifyField<uint8_t>(verifier, VT_INNER_SPLIT, 1) &&
           VerifyField<uint8_t>(verifier, VT_TRIM_OUT_OF_BOUNDS, 1) &&
           verifier.EndTable();
  }
  SplitT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(SplitT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Split> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const SplitT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct SplitBuilder {
  typedef Split Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_in(int64_t in) {
    fbb_.AddElement<int64_t>(Split::VT_IN, in, 0);
  }
  void add_factor(int64_t factor) {
    fbb_.AddElement<int64_t>(Split::VT_FACTOR, factor, 0);
  }
  void add_inner(int64_t inner) {
    fbb_.AddElement<int64_t>(Split::VT_INNER, inner, 0);
  }
  void add_outer(int64_t outer) {
    fbb_.AddElement<int64_t>(Split::VT_OUTER, outer, 0);
  }
  void add_inner_split(bool inner_split) {
    fbb_.AddElement<uint8_t>(Split::VT_INNER_SPLIT, static_cast<uint8_t>(inner_split), 0);
  }
  void add_trim_out_of_bounds(bool trim_out_of_bounds) {
    fbb_.AddElement<uint8_t>(Split::VT_TRIM_OUT_OF_BOUNDS, static_cast<uint8_t>(trim_out_of_bounds), 0);
  }
  explicit SplitBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Split> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Split>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Split> CreateSplit(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t in = 0,
    int64_t factor = 0,
    int64_t inner = 0,
    int64_t outer = 0,
    bool inner_split = false,
    bool trim_out_of_bounds = false) {
  SplitBuilder builder_(_fbb);
  builder_.add_outer(outer);
  builder_.add_inner(inner);
  builder_.add_factor(factor);
  builder_.add_in(in);
  builder_.add_trim_out_of_bounds(trim_out_of_bounds);
  builder_.add_inner_split(inner_split);
  return builder_.Finish();
}

::flatbuffers::Offset<Split> CreateSplit(::flatbuffers::FlatBufferBuilder &_fbb, const SplitT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct Swizzle2DT : public ::flatbuffers::NativeTable {
  typedef Swizzle2D TableType;
  int64_t in_x = 0;
  int64_t in_y = 0;
  nvfuser::serde::Swizzle2DType swizzle_type = nvfuser::serde::Swizzle2DType_None;
  nvfuser::serde::SwizzleMode swizzle_mode = nvfuser::serde::SwizzleMode_None;
  int64_t out_x = 0;
  int64_t out_y = 0;
};

struct Swizzle2D FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef Swizzle2DT NativeTableType;
  typedef Swizzle2DBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_IN_X = 4,
    VT_IN_Y = 6,
    VT_SWIZZLE_TYPE = 8,
    VT_SWIZZLE_MODE = 10,
    VT_OUT_X = 12,
    VT_OUT_Y = 14
  };
  int64_t in_x() const {
    return GetField<int64_t>(VT_IN_X, 0);
  }
  int64_t in_y() const {
    return GetField<int64_t>(VT_IN_Y, 0);
  }
  nvfuser::serde::Swizzle2DType swizzle_type() const {
    return static_cast<nvfuser::serde::Swizzle2DType>(GetField<int32_t>(VT_SWIZZLE_TYPE, 0));
  }
  nvfuser::serde::SwizzleMode swizzle_mode() const {
    return static_cast<nvfuser::serde::SwizzleMode>(GetField<int32_t>(VT_SWIZZLE_MODE, 0));
  }
  int64_t out_x() const {
    return GetField<int64_t>(VT_OUT_X, 0);
  }
  int64_t out_y() const {
    return GetField<int64_t>(VT_OUT_Y, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_IN_X, 8) &&
           VerifyField<int64_t>(verifier, VT_IN_Y, 8) &&
           VerifyField<int32_t>(verifier, VT_SWIZZLE_TYPE, 4) &&
           VerifyField<int32_t>(verifier, VT_SWIZZLE_MODE, 4) &&
           VerifyField<int64_t>(verifier, VT_OUT_X, 8) &&
           VerifyField<int64_t>(verifier, VT_OUT_Y, 8) &&
           verifier.EndTable();
  }
  Swizzle2DT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(Swizzle2DT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Swizzle2D> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const Swizzle2DT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct Swizzle2DBuilder {
  typedef Swizzle2D Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_in_x(int64_t in_x) {
    fbb_.AddElement<int64_t>(Swizzle2D::VT_IN_X, in_x, 0);
  }
  void add_in_y(int64_t in_y) {
    fbb_.AddElement<int64_t>(Swizzle2D::VT_IN_Y, in_y, 0);
  }
  void add_swizzle_type(nvfuser::serde::Swizzle2DType swizzle_type) {
    fbb_.AddElement<int32_t>(Swizzle2D::VT_SWIZZLE_TYPE, static_cast<int32_t>(swizzle_type), 0);
  }
  void add_swizzle_mode(nvfuser::serde::SwizzleMode swizzle_mode) {
    fbb_.AddElement<int32_t>(Swizzle2D::VT_SWIZZLE_MODE, static_cast<int32_t>(swizzle_mode), 0);
  }
  void add_out_x(int64_t out_x) {
    fbb_.AddElement<int64_t>(Swizzle2D::VT_OUT_X, out_x, 0);
  }
  void add_out_y(int64_t out_y) {
    fbb_.AddElement<int64_t>(Swizzle2D::VT_OUT_Y, out_y, 0);
  }
  explicit Swizzle2DBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Swizzle2D> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Swizzle2D>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Swizzle2D> CreateSwizzle2D(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t in_x = 0,
    int64_t in_y = 0,
    nvfuser::serde::Swizzle2DType swizzle_type = nvfuser::serde::Swizzle2DType_None,
    nvfuser::serde::SwizzleMode swizzle_mode = nvfuser::serde::SwizzleMode_None,
    int64_t out_x = 0,
    int64_t out_y = 0) {
  Swizzle2DBuilder builder_(_fbb);
  builder_.add_out_y(out_y);
  builder_.add_out_x(out_x);
  builder_.add_in_y(in_y);
  builder_.add_in_x(in_x);
  builder_.add_swizzle_mode(swizzle_mode);
  builder_.add_swizzle_type(swizzle_type);
  return builder_.Finish();
}

::flatbuffers::Offset<Swizzle2D> CreateSwizzle2D(::flatbuffers::FlatBufferBuilder &_fbb, const Swizzle2DT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct SymbolicT : public ::flatbuffers::NativeTable {
  typedef Symbolic TableType;
  int64_t src0 = 0;
  std::string name{};
};

struct Symbolic FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SymbolicT NativeTableType;
  typedef SymbolicBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SRC0 = 4,
    VT_NAME = 6
  };
  int64_t src0() const {
    return GetField<int64_t>(VT_SRC0, 0);
  }
  const ::flatbuffers::String *name() const {
    return GetPointer<const ::flatbuffers::String *>(VT_NAME);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_SRC0, 8) &&
           VerifyOffset(verifier, VT_NAME) &&
           verifier.VerifyString(name()) &&
           verifier.EndTable();
  }
  SymbolicT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(SymbolicT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Symbolic> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const SymbolicT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct SymbolicBuilder {
  typedef Symbolic Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_src0(int64_t src0) {
    fbb_.AddElement<int64_t>(Symbolic::VT_SRC0, src0, 0);
  }
  void add_name(::flatbuffers::Offset<::flatbuffers::String> name) {
    fbb_.AddOffset(Symbolic::VT_NAME, name);
  }
  explicit SymbolicBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Symbolic> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Symbolic>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Symbolic> CreateSymbolic(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t src0 = 0,
    ::flatbuffers::Offset<::flatbuffers::String> name = 0) {
  SymbolicBuilder builder_(_fbb);
  builder_.add_src0(src0);
  builder_.add_name(name);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Symbolic> CreateSymbolicDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t src0 = 0,
    const char *name = nullptr) {
  auto name__ = name ? _fbb.CreateString(name) : 0;
  return nvfuser::serde::CreateSymbolic(
      _fbb,
      src0,
      name__);
}

::flatbuffers::Offset<Symbolic> CreateSymbolic(::flatbuffers::FlatBufferBuilder &_fbb, const SymbolicT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct UnaryOpT : public ::flatbuffers::NativeTable {
  typedef UnaryOp TableType;
  nvfuser::serde::UnaryOpType unary_type = nvfuser::serde::UnaryOpType_None;
  nvfuser::serde::DataType data_type = nvfuser::serde::DataType_None;
  int64_t src0 = 0;
  int64_t out = 0;
  std::string name{};
};

struct UnaryOp FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef UnaryOpT NativeTableType;
  typedef UnaryOpBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_UNARY_TYPE = 4,
    VT_DATA_TYPE = 6,
    VT_SRC0 = 8,
    VT_OUT = 10,
    VT_NAME = 12
  };
  nvfuser::serde::UnaryOpType unary_type() const {
    return static_cast<nvfuser::serde::UnaryOpType>(GetField<int32_t>(VT_UNARY_TYPE, 0));
  }
  nvfuser::serde::DataType data_type() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DATA_TYPE, 0));
  }
  int64_t src0() const {
    return GetField<int64_t>(VT_SRC0, 0);
  }
  int64_t out() const {
    return GetField<int64_t>(VT_OUT, 0);
  }
  const ::flatbuffers::String *name() const {
    return GetPointer<const ::flatbuffers::String *>(VT_NAME);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_UNARY_TYPE, 4) &&
           VerifyField<int32_t>(verifier, VT_DATA_TYPE, 4) &&
           VerifyField<int64_t>(verifier, VT_SRC0, 8) &&
           VerifyField<int64_t>(verifier, VT_OUT, 8) &&
           VerifyOffset(verifier, VT_NAME) &&
           verifier.VerifyString(name()) &&
           verifier.EndTable();
  }
  UnaryOpT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(UnaryOpT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<UnaryOp> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const UnaryOpT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct UnaryOpBuilder {
  typedef UnaryOp Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_unary_type(nvfuser::serde::UnaryOpType unary_type) {
    fbb_.AddElement<int32_t>(UnaryOp::VT_UNARY_TYPE, static_cast<int32_t>(unary_type), 0);
  }
  void add_data_type(nvfuser::serde::DataType data_type) {
    fbb_.AddElement<int32_t>(UnaryOp::VT_DATA_TYPE, static_cast<int32_t>(data_type), 0);
  }
  void add_src0(int64_t src0) {
    fbb_.AddElement<int64_t>(UnaryOp::VT_SRC0, src0, 0);
  }
  void add_out(int64_t out) {
    fbb_.AddElement<int64_t>(UnaryOp::VT_OUT, out, 0);
  }
  void add_name(::flatbuffers::Offset<::flatbuffers::String> name) {
    fbb_.AddOffset(UnaryOp::VT_NAME, name);
  }
  explicit UnaryOpBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<UnaryOp> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<UnaryOp>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<UnaryOp> CreateUnaryOp(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::UnaryOpType unary_type = nvfuser::serde::UnaryOpType_None,
    nvfuser::serde::DataType data_type = nvfuser::serde::DataType_None,
    int64_t src0 = 0,
    int64_t out = 0,
    ::flatbuffers::Offset<::flatbuffers::String> name = 0) {
  UnaryOpBuilder builder_(_fbb);
  builder_.add_out(out);
  builder_.add_src0(src0);
  builder_.add_name(name);
  builder_.add_data_type(data_type);
  builder_.add_unary_type(unary_type);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<UnaryOp> CreateUnaryOpDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::UnaryOpType unary_type = nvfuser::serde::UnaryOpType_None,
    nvfuser::serde::DataType data_type = nvfuser::serde::DataType_None,
    int64_t src0 = 0,
    int64_t out = 0,
    const char *name = nullptr) {
  auto name__ = name ? _fbb.CreateString(name) : 0;
  return nvfuser::serde::CreateUnaryOp(
      _fbb,
      unary_type,
      data_type,
      src0,
      out,
      name__);
}

::flatbuffers::Offset<UnaryOp> CreateUnaryOp(::flatbuffers::FlatBufferBuilder &_fbb, const UnaryOpT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct InstructionT : public ::flatbuffers::NativeTable {
  typedef Instruction TableType;
  nvfuser::serde::InstructionDataUnion data{};
};

struct Instruction FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef InstructionT NativeTableType;
  typedef InstructionBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DATA_TYPE = 4,
    VT_DATA = 6
  };
  nvfuser::serde::InstructionData data_type() const {
    return static_cast<nvfuser::serde::InstructionData>(GetField<uint8_t>(VT_DATA_TYPE, 0));
  }
  const void *data() const {
    return GetPointer<const void *>(VT_DATA);
  }
  template<typename T> const T *data_as() const;
  const nvfuser::serde::BinaryOp *data_as_BinaryOp() const {
    return data_type() == nvfuser::serde::InstructionData_BinaryOp ? static_cast<const nvfuser::serde::BinaryOp *>(data()) : nullptr;
  }
  const nvfuser::serde::GetAttr *data_as_GetAttr() const {
    return data_type() == nvfuser::serde::InstructionData_GetAttr ? static_cast<const nvfuser::serde::GetAttr *>(data()) : nullptr;
  }
  const nvfuser::serde::GetItem *data_as_GetItem() const {
    return data_type() == nvfuser::serde::InstructionData_GetItem ? static_cast<const nvfuser::serde::GetItem *>(data()) : nullptr;
  }
  const nvfuser::serde::GetMetaData *data_as_GetMetaData() const {
    return data_type() == nvfuser::serde::InstructionData_GetMetaData ? static_cast<const nvfuser::serde::GetMetaData *>(data()) : nullptr;
  }
  const nvfuser::serde::Merge *data_as_Merge() const {
    return data_type() == nvfuser::serde::InstructionData_Merge ? static_cast<const nvfuser::serde::Merge *>(data()) : nullptr;
  }
  const nvfuser::serde::NamedScalar *data_as_NamedScalar() const {
    return data_type() == nvfuser::serde::InstructionData_NamedScalar ? static_cast<const nvfuser::serde::NamedScalar *>(data()) : nullptr;
  }
  const nvfuser::serde::Resize *data_as_Resize() const {
    return data_type() == nvfuser::serde::InstructionData_Resize ? static_cast<const nvfuser::serde::Resize *>(data()) : nullptr;
  }
  const nvfuser::serde::Scalar *data_as_Scalar() const {
    return data_type() == nvfuser::serde::InstructionData_Scalar ? static_cast<const nvfuser::serde::Scalar *>(data()) : nullptr;
  }
  const nvfuser::serde::Split *data_as_Split() const {
    return data_type() == nvfuser::serde::InstructionData_Split ? static_cast<const nvfuser::serde::Split *>(data()) : nullptr;
  }
  const nvfuser::serde::Swizzle2D *data_as_Swizzle2D() const {
    return data_type() == nvfuser::serde::InstructionData_Swizzle2D ? static_cast<const nvfuser::serde::Swizzle2D *>(data()) : nullptr;
  }
  const nvfuser::serde::Symbolic *data_as_Symbolic() const {
    return data_type() == nvfuser::serde::InstructionData_Symbolic ? static_cast<const nvfuser::serde::Symbolic *>(data()) : nullptr;
  }
  const nvfuser::serde::UnaryOp *data_as_UnaryOp() const {
    return data_type() == nvfuser::serde::InstructionData_UnaryOp ? static_cast<const nvfuser::serde::UnaryOp *>(data()) : nullptr;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_DATA_TYPE, 1) &&
           VerifyOffset(verifier, VT_DATA) &&
           VerifyInstructionData(verifier, data(), data_type()) &&
           verifier.EndTable();
  }
  InstructionT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(InstructionT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Instruction> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const InstructionT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

template<> inline const nvfuser::serde::BinaryOp *Instruction::data_as<nvfuser::serde::BinaryOp>() const {
  return data_as_BinaryOp();
}

template<> inline const nvfuser::serde::GetAttr *Instruction::data_as<nvfuser::serde::GetAttr>() const {
  return data_as_GetAttr();
}

template<> inline const nvfuser::serde::GetItem *Instruction::data_as<nvfuser::serde::GetItem>() const {
  return data_as_GetItem();
}

template<> inline const nvfuser::serde::GetMetaData *Instruction::data_as<nvfuser::serde::GetMetaData>() const {
  return data_as_GetMetaData();
}

template<> inline const nvfuser::serde::Merge *Instruction::data_as<nvfuser::serde::Merge>() const {
  return data_as_Merge();
}

template<> inline const nvfuser::serde::NamedScalar *Instruction::data_as<nvfuser::serde::NamedScalar>() const {
  return data_as_NamedScalar();
}

template<> inline const nvfuser::serde::Resize *Instruction::data_as<nvfuser::serde::Resize>() const {
  return data_as_Resize();
}

template<> inline const nvfuser::serde::Scalar *Instruction::data_as<nvfuser::serde::Scalar>() const {
  return data_as_Scalar();
}

template<> inline const nvfuser::serde::Split *Instruction::data_as<nvfuser::serde::Split>() const {
  return data_as_Split();
}

template<> inline const nvfuser::serde::Swizzle2D *Instruction::data_as<nvfuser::serde::Swizzle2D>() const {
  return data_as_Swizzle2D();
}

template<> inline const nvfuser::serde::Symbolic *Instruction::data_as<nvfuser::serde::Symbolic>() const {
  return data_as_Symbolic();
}

template<> inline const nvfuser::serde::UnaryOp *Instruction::data_as<nvfuser::serde::UnaryOp>() const {
  return data_as_UnaryOp();
}

struct InstructionBuilder {
  typedef Instruction Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_data_type(nvfuser::serde::InstructionData data_type) {
    fbb_.AddElement<uint8_t>(Instruction::VT_DATA_TYPE, static_cast<uint8_t>(data_type), 0);
  }
  void add_data(::flatbuffers::Offset<void> data) {
    fbb_.AddOffset(Instruction::VT_DATA, data);
  }
  explicit InstructionBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Instruction> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Instruction>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Instruction> CreateInstruction(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::InstructionData data_type = nvfuser::serde::InstructionData_NONE,
    ::flatbuffers::Offset<void> data = 0) {
  InstructionBuilder builder_(_fbb);
  builder_.add_data(data);
  builder_.add_data_type(data_type);
  return builder_.Finish();
}

::flatbuffers::Offset<Instruction> CreateInstruction(::flatbuffers::FlatBufferBuilder &_fbb, const InstructionT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct NaiveValueGeneratorT : public ::flatbuffers::NativeTable {
  typedef NaiveValueGenerator TableType;
  std::vector<std::unique_ptr<nvfuser::serde::InstructionT>> instructions{};
  NaiveValueGeneratorT() = default;
  NaiveValueGeneratorT(const NaiveValueGeneratorT &o);
  NaiveValueGeneratorT(NaiveValueGeneratorT&&) FLATBUFFERS_NOEXCEPT = default;
  NaiveValueGeneratorT &operator=(NaiveValueGeneratorT o) FLATBUFFERS_NOEXCEPT;
};

struct NaiveValueGenerator FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef NaiveValueGeneratorT NativeTableType;
  typedef NaiveValueGeneratorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INSTRUCTIONS = 4
  };
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::Instruction>> *instructions() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::Instruction>> *>(VT_INSTRUCTIONS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_INSTRUCTIONS) &&
           verifier.VerifyVector(instructions()) &&
           verifier.VerifyVectorOfTables(instructions()) &&
           verifier.EndTable();
  }
  NaiveValueGeneratorT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(NaiveValueGeneratorT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<NaiveValueGenerator> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const NaiveValueGeneratorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct NaiveValueGeneratorBuilder {
  typedef NaiveValueGenerator Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_instructions(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::Instruction>>> instructions) {
    fbb_.AddOffset(NaiveValueGenerator::VT_INSTRUCTIONS, instructions);
  }
  explicit NaiveValueGeneratorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<NaiveValueGenerator> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<NaiveValueGenerator>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<NaiveValueGenerator> CreateNaiveValueGenerator(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::Instruction>>> instructions = 0) {
  NaiveValueGeneratorBuilder builder_(_fbb);
  builder_.add_instructions(instructions);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<NaiveValueGenerator> CreateNaiveValueGeneratorDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::Instruction>> *instructions = nullptr) {
  auto instructions__ = instructions ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::Instruction>>(*instructions) : 0;
  return nvfuser::serde::CreateNaiveValueGenerator(
      _fbb,
      instructions__);
}

::flatbuffers::Offset<NaiveValueGenerator> CreateNaiveValueGenerator(::flatbuffers::FlatBufferBuilder &_fbb, const NaiveValueGeneratorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct IterationDomainT : public ::flatbuffers::NativeTable {
  typedef IterationDomain TableType;
  int64_t extent = 0;
};

struct IterationDomain FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef IterationDomainT NativeTableType;
  typedef IterationDomainBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_EXTENT = 4
  };
  int64_t extent() const {
    return GetField<int64_t>(VT_EXTENT, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_EXTENT, 8) &&
           verifier.EndTable();
  }
  IterationDomainT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(IterationDomainT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<IterationDomain> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const IterationDomainT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct IterationDomainBuilder {
  typedef IterationDomain Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_extent(int64_t extent) {
    fbb_.AddElement<int64_t>(IterationDomain::VT_EXTENT, extent, 0);
  }
  explicit IterationDomainBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<IterationDomain> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<IterationDomain>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<IterationDomain> CreateIterationDomain(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t extent = 0) {
  IterationDomainBuilder builder_(_fbb);
  builder_.add_extent(extent);
  return builder_.Finish();
}

::flatbuffers::Offset<IterationDomain> CreateIterationDomain(::flatbuffers::FlatBufferBuilder &_fbb, const IterationDomainT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct DomainT : public ::flatbuffers::NativeTable {
  typedef Domain TableType;
  std::vector<std::unique_ptr<nvfuser::serde::IterationDomainT>> dims{};
  DomainT() = default;
  DomainT(const DomainT &o);
  DomainT(DomainT&&) FLATBUFFERS_NOEXCEPT = default;
  DomainT &operator=(DomainT o) FLATBUFFERS_NOEXCEPT;
};

struct Domain FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef DomainT NativeTableType;
  typedef DomainBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIMS = 4
  };
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::IterationDomain>> *dims() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::IterationDomain>> *>(VT_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_DIMS) &&
           verifier.VerifyVector(dims()) &&
           verifier.VerifyVectorOfTables(dims()) &&
           verifier.EndTable();
  }
  DomainT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(DomainT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Domain> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const DomainT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct DomainBuilder {
  typedef Domain Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dims(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::IterationDomain>>> dims) {
    fbb_.AddOffset(Domain::VT_DIMS, dims);
  }
  explicit DomainBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Domain> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Domain>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Domain> CreateDomain(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::IterationDomain>>> dims = 0) {
  DomainBuilder builder_(_fbb);
  builder_.add_dims(dims);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Domain> CreateDomainDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::IterationDomain>> *dims = nullptr) {
  auto dims__ = dims ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::IterationDomain>>(*dims) : 0;
  return nvfuser::serde::CreateDomain(
      _fbb,
      dims__);
}

::flatbuffers::Offset<Domain> CreateDomain(::flatbuffers::FlatBufferBuilder &_fbb, const DomainT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct SymbolicTensorT : public ::flatbuffers::NativeTable {
  typedef SymbolicTensor TableType;
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None;
  std::unique_ptr<nvfuser::serde::DomainT> root{};
  std::unique_ptr<nvfuser::serde::DomainT> rfactor{};
  std::unique_ptr<nvfuser::serde::DomainT> allocate{};
  std::unique_ptr<nvfuser::serde::DomainT> leaf{};
  SymbolicTensorT() = default;
  SymbolicTensorT(const SymbolicTensorT &o);
  SymbolicTensorT(SymbolicTensorT&&) FLATBUFFERS_NOEXCEPT = default;
  SymbolicTensorT &operator=(SymbolicTensorT o) FLATBUFFERS_NOEXCEPT;
};

struct SymbolicTensor FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SymbolicTensorT NativeTableType;
  typedef SymbolicTensorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4,
    VT_ROOT = 6,
    VT_RFACTOR = 8,
    VT_ALLOCATE = 10,
    VT_LEAF = 12
  };
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  const nvfuser::serde::Domain *root() const {
    return GetPointer<const nvfuser::serde::Domain *>(VT_ROOT);
  }
  const nvfuser::serde::Domain *rfactor() const {
    return GetPointer<const nvfuser::serde::Domain *>(VT_RFACTOR);
  }
  const nvfuser::serde::Domain *allocate() const {
    return GetPointer<const nvfuser::serde::Domain *>(VT_ALLOCATE);
  }
  const nvfuser::serde::Domain *leaf() const {
    return GetPointer<const nvfuser::serde::Domain *>(VT_LEAF);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           VerifyOffset(verifier, VT_ROOT) &&
           verifier.VerifyTable(root()) &&
           VerifyOffset(verifier, VT_RFACTOR) &&
           verifier.VerifyTable(rfactor()) &&
           VerifyOffset(verifier, VT_ALLOCATE) &&
           verifier.VerifyTable(allocate()) &&
           VerifyOffset(verifier, VT_LEAF) &&
           verifier.VerifyTable(leaf()) &&
           verifier.EndTable();
  }
  SymbolicTensorT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(SymbolicTensorT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<SymbolicTensor> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const SymbolicTensorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct SymbolicTensorBuilder {
  typedef SymbolicTensor Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(SymbolicTensor::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  void add_root(::flatbuffers::Offset<nvfuser::serde::Domain> root) {
    fbb_.AddOffset(SymbolicTensor::VT_ROOT, root);
  }
  void add_rfactor(::flatbuffers::Offset<nvfuser::serde::Domain> rfactor) {
    fbb_.AddOffset(SymbolicTensor::VT_RFACTOR, rfactor);
  }
  void add_allocate(::flatbuffers::Offset<nvfuser::serde::Domain> allocate) {
    fbb_.AddOffset(SymbolicTensor::VT_ALLOCATE, allocate);
  }
  void add_leaf(::flatbuffers::Offset<nvfuser::serde::Domain> leaf) {
    fbb_.AddOffset(SymbolicTensor::VT_LEAF, leaf);
  }
  explicit SymbolicTensorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<SymbolicTensor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<SymbolicTensor>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<SymbolicTensor> CreateSymbolicTensor(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None,
    ::flatbuffers::Offset<nvfuser::serde::Domain> root = 0,
    ::flatbuffers::Offset<nvfuser::serde::Domain> rfactor = 0,
    ::flatbuffers::Offset<nvfuser::serde::Domain> allocate = 0,
    ::flatbuffers::Offset<nvfuser::serde::Domain> leaf = 0) {
  SymbolicTensorBuilder builder_(_fbb);
  builder_.add_leaf(leaf);
  builder_.add_allocate(allocate);
  builder_.add_rfactor(rfactor);
  builder_.add_root(root);
  builder_.add_dtype(dtype);
  return builder_.Finish();
}

::flatbuffers::Offset<SymbolicTensor> CreateSymbolicTensor(::flatbuffers::FlatBufferBuilder &_fbb, const SymbolicTensorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct AllocateBufferT : public ::flatbuffers::NativeTable {
  typedef AllocateBuffer TableType;
  std::unique_ptr<nvfuser::serde::SymbolicTensorT> tv{};
  std::vector<int64_t> shape{};
  bool zero_init = false;
  AllocateBufferT() = default;
  AllocateBufferT(const AllocateBufferT &o);
  AllocateBufferT(AllocateBufferT&&) FLATBUFFERS_NOEXCEPT = default;
  AllocateBufferT &operator=(AllocateBufferT o) FLATBUFFERS_NOEXCEPT;
};

struct AllocateBuffer FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef AllocateBufferT NativeTableType;
  typedef AllocateBufferBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TV = 4,
    VT_SHAPE = 6,
    VT_ZERO_INIT = 8
  };
  const nvfuser::serde::SymbolicTensor *tv() const {
    return GetPointer<const nvfuser::serde::SymbolicTensor *>(VT_TV);
  }
  const ::flatbuffers::Vector<int64_t> *shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SHAPE);
  }
  bool zero_init() const {
    return GetField<uint8_t>(VT_ZERO_INIT, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_TV) &&
           verifier.VerifyTable(tv()) &&
           VerifyOffset(verifier, VT_SHAPE) &&
           verifier.VerifyVector(shape()) &&
           VerifyField<uint8_t>(verifier, VT_ZERO_INIT, 1) &&
           verifier.EndTable();
  }
  AllocateBufferT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(AllocateBufferT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<AllocateBuffer> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const AllocateBufferT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct AllocateBufferBuilder {
  typedef AllocateBuffer Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_tv(::flatbuffers::Offset<nvfuser::serde::SymbolicTensor> tv) {
    fbb_.AddOffset(AllocateBuffer::VT_TV, tv);
  }
  void add_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape) {
    fbb_.AddOffset(AllocateBuffer::VT_SHAPE, shape);
  }
  void add_zero_init(bool zero_init) {
    fbb_.AddElement<uint8_t>(AllocateBuffer::VT_ZERO_INIT, static_cast<uint8_t>(zero_init), 0);
  }
  explicit AllocateBufferBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<AllocateBuffer> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<AllocateBuffer>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<AllocateBuffer> CreateAllocateBuffer(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::SymbolicTensor> tv = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape = 0,
    bool zero_init = false) {
  AllocateBufferBuilder builder_(_fbb);
  builder_.add_shape(shape);
  builder_.add_tv(tv);
  builder_.add_zero_init(zero_init);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<AllocateBuffer> CreateAllocateBufferDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::SymbolicTensor> tv = 0,
    const std::vector<int64_t> *shape = nullptr,
    bool zero_init = false) {
  auto shape__ = shape ? _fbb.CreateVector<int64_t>(*shape) : 0;
  return nvfuser::serde::CreateAllocateBuffer(
      _fbb,
      tv,
      shape__,
      zero_init);
}

::flatbuffers::Offset<AllocateBuffer> CreateAllocateBuffer(::flatbuffers::FlatBufferBuilder &_fbb, const AllocateBufferT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct ScalarCpuT : public ::flatbuffers::NativeTable {
  typedef ScalarCpu TableType;
  std::unique_ptr<nvfuser::serde::ScalarT> scalar_value{};
  ScalarCpuT() = default;
  ScalarCpuT(const ScalarCpuT &o);
  ScalarCpuT(ScalarCpuT&&) FLATBUFFERS_NOEXCEPT = default;
  ScalarCpuT &operator=(ScalarCpuT o) FLATBUFFERS_NOEXCEPT;
};

struct ScalarCpu FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ScalarCpuT NativeTableType;
  typedef ScalarCpuBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SCALAR_VALUE = 4
  };
  const nvfuser::serde::Scalar *scalar_value() const {
    return GetPointer<const nvfuser::serde::Scalar *>(VT_SCALAR_VALUE);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SCALAR_VALUE) &&
           verifier.VerifyTable(scalar_value()) &&
           verifier.EndTable();
  }
  ScalarCpuT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(ScalarCpuT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<ScalarCpu> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ScalarCpuT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct ScalarCpuBuilder {
  typedef ScalarCpu Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_scalar_value(::flatbuffers::Offset<nvfuser::serde::Scalar> scalar_value) {
    fbb_.AddOffset(ScalarCpu::VT_SCALAR_VALUE, scalar_value);
  }
  explicit ScalarCpuBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<ScalarCpu> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<ScalarCpu>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<ScalarCpu> CreateScalarCpu(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::Scalar> scalar_value = 0) {
  ScalarCpuBuilder builder_(_fbb);
  builder_.add_scalar_value(scalar_value);
  return builder_.Finish();
}

::flatbuffers::Offset<ScalarCpu> CreateScalarCpu(::flatbuffers::FlatBufferBuilder &_fbb, const ScalarCpuT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct TensorArgT : public ::flatbuffers::NativeTable {
  typedef TensorArg TableType;
  uint64_t ptr = 0;
  std::vector<int64_t> sizes{};
  std::vector<int64_t> strides{};
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None;
};

struct TensorArg FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorArgT NativeTableType;
  typedef TensorArgBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_PTR = 4,
    VT_SIZES = 6,
    VT_STRIDES = 8,
    VT_DTYPE = 10
  };
  uint64_t ptr() const {
    return GetField<uint64_t>(VT_PTR, 0);
  }
  const ::flatbuffers::Vector<int64_t> *sizes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SIZES);
  }
  const ::flatbuffers::Vector<int64_t> *strides() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDES);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_PTR, 8) &&
           VerifyOffset(verifier, VT_SIZES) &&
           verifier.VerifyVector(sizes()) &&
           VerifyOffset(verifier, VT_STRIDES) &&
           verifier.VerifyVector(strides()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
  TensorArgT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(TensorArgT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<TensorArg> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorArgT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct TensorArgBuilder {
  typedef TensorArg Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_ptr(uint64_t ptr) {
    fbb_.AddElement<uint64_t>(TensorArg::VT_PTR, ptr, 0);
  }
  void add_sizes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes) {
    fbb_.AddOffset(TensorArg::VT_SIZES, sizes);
  }
  void add_strides(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides) {
    fbb_.AddOffset(TensorArg::VT_STRIDES, strides);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(TensorArg::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit TensorArgBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorArg> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorArg>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorArg> CreateTensorArg(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t ptr = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  TensorArgBuilder builder_(_fbb);
  builder_.add_ptr(ptr);
  builder_.add_dtype(dtype);
  builder_.add_strides(strides);
  builder_.add_sizes(sizes);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TensorArg> CreateTensorArgDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t ptr = 0,
    const std::vector<int64_t> *sizes = nullptr,
    const std::vector<int64_t> *strides = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  auto sizes__ = sizes ? _fbb.CreateVector<int64_t>(*sizes) : 0;
  auto strides__ = strides ? _fbb.CreateVector<int64_t>(*strides) : 0;
  return nvfuser::serde::CreateTensorArg(
      _fbb,
      ptr,
      sizes__,
      strides__,
      dtype);
}

::flatbuffers::Offset<TensorArg> CreateTensorArg(::flatbuffers::FlatBufferBuilder &_fbb, const TensorArgT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct PolymorphicValueT : public ::flatbuffers::NativeTable {
  typedef PolymorphicValue TableType;
  nvfuser::serde::PolymorphicValueDataUnion data{};
};

struct PolymorphicValue FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef PolymorphicValueT NativeTableType;
  typedef PolymorphicValueBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DATA_TYPE = 4,
    VT_DATA = 6
  };
  nvfuser::serde::PolymorphicValueData data_type() const {
    return static_cast<nvfuser::serde::PolymorphicValueData>(GetField<uint8_t>(VT_DATA_TYPE, 0));
  }
  const void *data() const {
    return GetPointer<const void *>(VT_DATA);
  }
  template<typename T> const T *data_as() const;
  const nvfuser::serde::Scalar *data_as_Scalar() const {
    return data_type() == nvfuser::serde::PolymorphicValueData_Scalar ? static_cast<const nvfuser::serde::Scalar *>(data()) : nullptr;
  }
  const nvfuser::serde::ScalarCpu *data_as_ScalarCpu() const {
    return data_type() == nvfuser::serde::PolymorphicValueData_ScalarCpu ? static_cast<const nvfuser::serde::ScalarCpu *>(data()) : nullptr;
  }
  const nvfuser::serde::TensorArg *data_as_TensorArg() const {
    return data_type() == nvfuser::serde::PolymorphicValueData_TensorArg ? static_cast<const nvfuser::serde::TensorArg *>(data()) : nullptr;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_DATA_TYPE, 1) &&
           VerifyOffset(verifier, VT_DATA) &&
           VerifyPolymorphicValueData(verifier, data(), data_type()) &&
           verifier.EndTable();
  }
  PolymorphicValueT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(PolymorphicValueT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<PolymorphicValue> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const PolymorphicValueT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

template<> inline const nvfuser::serde::Scalar *PolymorphicValue::data_as<nvfuser::serde::Scalar>() const {
  return data_as_Scalar();
}

template<> inline const nvfuser::serde::ScalarCpu *PolymorphicValue::data_as<nvfuser::serde::ScalarCpu>() const {
  return data_as_ScalarCpu();
}

template<> inline const nvfuser::serde::TensorArg *PolymorphicValue::data_as<nvfuser::serde::TensorArg>() const {
  return data_as_TensorArg();
}

struct PolymorphicValueBuilder {
  typedef PolymorphicValue Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_data_type(nvfuser::serde::PolymorphicValueData data_type) {
    fbb_.AddElement<uint8_t>(PolymorphicValue::VT_DATA_TYPE, static_cast<uint8_t>(data_type), 0);
  }
  void add_data(::flatbuffers::Offset<void> data) {
    fbb_.AddOffset(PolymorphicValue::VT_DATA, data);
  }
  explicit PolymorphicValueBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<PolymorphicValue> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<PolymorphicValue>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<PolymorphicValue> CreatePolymorphicValue(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::PolymorphicValueData data_type = nvfuser::serde::PolymorphicValueData_NONE,
    ::flatbuffers::Offset<void> data = 0) {
  PolymorphicValueBuilder builder_(_fbb);
  builder_.add_data(data);
  builder_.add_data_type(data_type);
  return builder_.Finish();
}

::flatbuffers::Offset<PolymorphicValue> CreatePolymorphicValue(::flatbuffers::FlatBufferBuilder &_fbb, const PolymorphicValueT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct KernelArgumentHolderT : public ::flatbuffers::NativeTable {
  typedef KernelArgumentHolder TableType;
  std::vector<std::unique_ptr<nvfuser::serde::PolymorphicValueT>> arguments{};
  int8_t device_index = 0;
  uint64_t cache_id = 0;
  KernelArgumentHolderT() = default;
  KernelArgumentHolderT(const KernelArgumentHolderT &o);
  KernelArgumentHolderT(KernelArgumentHolderT&&) FLATBUFFERS_NOEXCEPT = default;
  KernelArgumentHolderT &operator=(KernelArgumentHolderT o) FLATBUFFERS_NOEXCEPT;
};

struct KernelArgumentHolder FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef KernelArgumentHolderT NativeTableType;
  typedef KernelArgumentHolderBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ARGUMENTS = 4,
    VT_DEVICE_INDEX = 6,
    VT_CACHE_ID = 8
  };
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>> *arguments() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>> *>(VT_ARGUMENTS);
  }
  int8_t device_index() const {
    return GetField<int8_t>(VT_DEVICE_INDEX, 0);
  }
  uint64_t cache_id() const {
    return GetField<uint64_t>(VT_CACHE_ID, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ARGUMENTS) &&
           verifier.VerifyVector(arguments()) &&
           verifier.VerifyVectorOfTables(arguments()) &&
           VerifyField<int8_t>(verifier, VT_DEVICE_INDEX, 1) &&
           VerifyField<uint64_t>(verifier, VT_CACHE_ID, 8) &&
           verifier.EndTable();
  }
  KernelArgumentHolderT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(KernelArgumentHolderT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<KernelArgumentHolder> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const KernelArgumentHolderT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct KernelArgumentHolderBuilder {
  typedef KernelArgumentHolder Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_arguments(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>>> arguments) {
    fbb_.AddOffset(KernelArgumentHolder::VT_ARGUMENTS, arguments);
  }
  void add_device_index(int8_t device_index) {
    fbb_.AddElement<int8_t>(KernelArgumentHolder::VT_DEVICE_INDEX, device_index, 0);
  }
  void add_cache_id(uint64_t cache_id) {
    fbb_.AddElement<uint64_t>(KernelArgumentHolder::VT_CACHE_ID, cache_id, 0);
  }
  explicit KernelArgumentHolderBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<KernelArgumentHolder> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<KernelArgumentHolder>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<KernelArgumentHolder> CreateKernelArgumentHolder(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>>> arguments = 0,
    int8_t device_index = 0,
    uint64_t cache_id = 0) {
  KernelArgumentHolderBuilder builder_(_fbb);
  builder_.add_cache_id(cache_id);
  builder_.add_arguments(arguments);
  builder_.add_device_index(device_index);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<KernelArgumentHolder> CreateKernelArgumentHolderDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>> *arguments = nullptr,
    int8_t device_index = 0,
    uint64_t cache_id = 0) {
  auto arguments__ = arguments ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>>(*arguments) : 0;
  return nvfuser::serde::CreateKernelArgumentHolder(
      _fbb,
      arguments__,
      device_index,
      cache_id);
}

::flatbuffers::Offset<KernelArgumentHolder> CreateKernelArgumentHolder(::flatbuffers::FlatBufferBuilder &_fbb, const KernelArgumentHolderT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct TensorShapeT : public ::flatbuffers::NativeTable {
  typedef TensorShape TableType;
  std::vector<int64_t> shape{};
};

struct TensorShape FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorShapeT NativeTableType;
  typedef TensorShapeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SHAPE = 4
  };
  const ::flatbuffers::Vector<int64_t> *shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SHAPE);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SHAPE) &&
           verifier.VerifyVector(shape()) &&
           verifier.EndTable();
  }
  TensorShapeT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(TensorShapeT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<TensorShape> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorShapeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct TensorShapeBuilder {
  typedef TensorShape Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape) {
    fbb_.AddOffset(TensorShape::VT_SHAPE, shape);
  }
  explicit TensorShapeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorShape> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorShape>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorShape> CreateTensorShape(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape = 0) {
  TensorShapeBuilder builder_(_fbb);
  builder_.add_shape(shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TensorShape> CreateTensorShapeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *shape = nullptr) {
  auto shape__ = shape ? _fbb.CreateVector<int64_t>(*shape) : 0;
  return nvfuser::serde::CreateTensorShape(
      _fbb,
      shape__);
}

::flatbuffers::Offset<TensorShape> CreateTensorShape(::flatbuffers::FlatBufferBuilder &_fbb, const TensorShapeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct LaunchParamsT : public ::flatbuffers::NativeTable {
  typedef LaunchParams TableType;
  int64_t gdimx = 0;
  int64_t gdimy = 0;
  int64_t gdimz = 0;
  int64_t bdimx = 0;
  int64_t bdimy = 0;
  int64_t bdimz = 0;
  int64_t smem = 0;
  std::vector<std::unique_ptr<nvfuser::serde::TensorShapeT>> output_sizes{};
  LaunchParamsT() = default;
  LaunchParamsT(const LaunchParamsT &o);
  LaunchParamsT(LaunchParamsT&&) FLATBUFFERS_NOEXCEPT = default;
  LaunchParamsT &operator=(LaunchParamsT o) FLATBUFFERS_NOEXCEPT;
};

struct LaunchParams FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef LaunchParamsT NativeTableType;
  typedef LaunchParamsBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_GDIMX = 4,
    VT_GDIMY = 6,
    VT_GDIMZ = 8,
    VT_BDIMX = 10,
    VT_BDIMY = 12,
    VT_BDIMZ = 14,
    VT_SMEM = 16,
    VT_OUTPUT_SIZES = 18
  };
  int64_t gdimx() const {
    return GetField<int64_t>(VT_GDIMX, 0);
  }
  int64_t gdimy() const {
    return GetField<int64_t>(VT_GDIMY, 0);
  }
  int64_t gdimz() const {
    return GetField<int64_t>(VT_GDIMZ, 0);
  }
  int64_t bdimx() const {
    return GetField<int64_t>(VT_BDIMX, 0);
  }
  int64_t bdimy() const {
    return GetField<int64_t>(VT_BDIMY, 0);
  }
  int64_t bdimz() const {
    return GetField<int64_t>(VT_BDIMZ, 0);
  }
  int64_t smem() const {
    return GetField<int64_t>(VT_SMEM, 0);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>> *output_sizes() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>> *>(VT_OUTPUT_SIZES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_GDIMX, 8) &&
           VerifyField<int64_t>(verifier, VT_GDIMY, 8) &&
           VerifyField<int64_t>(verifier, VT_GDIMZ, 8) &&
           VerifyField<int64_t>(verifier, VT_BDIMX, 8) &&
           VerifyField<int64_t>(verifier, VT_BDIMY, 8) &&
           VerifyField<int64_t>(verifier, VT_BDIMZ, 8) &&
           VerifyField<int64_t>(verifier, VT_SMEM, 8) &&
           VerifyOffset(verifier, VT_OUTPUT_SIZES) &&
           verifier.VerifyVector(output_sizes()) &&
           verifier.VerifyVectorOfTables(output_sizes()) &&
           verifier.EndTable();
  }
  LaunchParamsT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(LaunchParamsT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<LaunchParams> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const LaunchParamsT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct LaunchParamsBuilder {
  typedef LaunchParams Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_gdimx(int64_t gdimx) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_GDIMX, gdimx, 0);
  }
  void add_gdimy(int64_t gdimy) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_GDIMY, gdimy, 0);
  }
  void add_gdimz(int64_t gdimz) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_GDIMZ, gdimz, 0);
  }
  void add_bdimx(int64_t bdimx) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_BDIMX, bdimx, 0);
  }
  void add_bdimy(int64_t bdimy) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_BDIMY, bdimy, 0);
  }
  void add_bdimz(int64_t bdimz) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_BDIMZ, bdimz, 0);
  }
  void add_smem(int64_t smem) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_SMEM, smem, 0);
  }
  void add_output_sizes(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>>> output_sizes) {
    fbb_.AddOffset(LaunchParams::VT_OUTPUT_SIZES, output_sizes);
  }
  explicit LaunchParamsBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<LaunchParams> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<LaunchParams>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<LaunchParams> CreateLaunchParams(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t gdimx = 0,
    int64_t gdimy = 0,
    int64_t gdimz = 0,
    int64_t bdimx = 0,
    int64_t bdimy = 0,
    int64_t bdimz = 0,
    int64_t smem = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>>> output_sizes = 0) {
  LaunchParamsBuilder builder_(_fbb);
  builder_.add_smem(smem);
  builder_.add_bdimz(bdimz);
  builder_.add_bdimy(bdimy);
  builder_.add_bdimx(bdimx);
  builder_.add_gdimz(gdimz);
  builder_.add_gdimy(gdimy);
  builder_.add_gdimx(gdimx);
  builder_.add_output_sizes(output_sizes);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<LaunchParams> CreateLaunchParamsDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t gdimx = 0,
    int64_t gdimy = 0,
    int64_t gdimz = 0,
    int64_t bdimx = 0,
    int64_t bdimy = 0,
    int64_t bdimz = 0,
    int64_t smem = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>> *output_sizes = nullptr) {
  auto output_sizes__ = output_sizes ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::TensorShape>>(*output_sizes) : 0;
  return nvfuser::serde::CreateLaunchParams(
      _fbb,
      gdimx,
      gdimy,
      gdimz,
      bdimx,
      bdimy,
      bdimz,
      smem,
      output_sizes__);
}

::flatbuffers::Offset<LaunchParams> CreateLaunchParams(::flatbuffers::FlatBufferBuilder &_fbb, const LaunchParamsT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct GlobalBufferInfoT : public ::flatbuffers::NativeTable {
  typedef GlobalBufferInfo TableType;
  int64_t tv = -1LL;
  std::vector<int64_t> sizes{};
  std::vector<int64_t> strides{};
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None;
  bool zero_init = false;
  bool is_profile_buffer = false;
  bool is_fusion_output = false;
};

struct GlobalBufferInfo FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef GlobalBufferInfoT NativeTableType;
  typedef GlobalBufferInfoBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TV = 4,
    VT_SIZES = 6,
    VT_STRIDES = 8,
    VT_DTYPE = 10,
    VT_ZERO_INIT = 12,
    VT_IS_PROFILE_BUFFER = 14,
    VT_IS_FUSION_OUTPUT = 16
  };
  int64_t tv() const {
    return GetField<int64_t>(VT_TV, -1LL);
  }
  const ::flatbuffers::Vector<int64_t> *sizes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SIZES);
  }
  const ::flatbuffers::Vector<int64_t> *strides() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDES);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool zero_init() const {
    return GetField<uint8_t>(VT_ZERO_INIT, 0) != 0;
  }
  bool is_profile_buffer() const {
    return GetField<uint8_t>(VT_IS_PROFILE_BUFFER, 0) != 0;
  }
  bool is_fusion_output() const {
    return GetField<uint8_t>(VT_IS_FUSION_OUTPUT, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_TV, 8) &&
           VerifyOffset(verifier, VT_SIZES) &&
           verifier.VerifyVector(sizes()) &&
           VerifyOffset(verifier, VT_STRIDES) &&
           verifier.VerifyVector(strides()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_ZERO_INIT, 1) &&
           VerifyField<uint8_t>(verifier, VT_IS_PROFILE_BUFFER, 1) &&
           VerifyField<uint8_t>(verifier, VT_IS_FUSION_OUTPUT, 1) &&
           verifier.EndTable();
  }
  GlobalBufferInfoT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(GlobalBufferInfoT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<GlobalBufferInfo> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const GlobalBufferInfoT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct GlobalBufferInfoBuilder {
  typedef GlobalBufferInfo Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_tv(int64_t tv) {
    fbb_.AddElement<int64_t>(GlobalBufferInfo::VT_TV, tv, -1LL);
  }
  void add_sizes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes) {
    fbb_.AddOffset(GlobalBufferInfo::VT_SIZES, sizes);
  }
  void add_strides(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides) {
    fbb_.AddOffset(GlobalBufferInfo::VT_STRIDES, strides);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(GlobalBufferInfo::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  void add_zero_init(bool zero_init) {
    fbb_.AddElement<uint8_t>(GlobalBufferInfo::VT_ZERO_INIT, static_cast<uint8_t>(zero_init), 0);
  }
  void add_is_profile_buffer(bool is_profile_buffer) {
    fbb_.AddElement<uint8_t>(GlobalBufferInfo::VT_IS_PROFILE_BUFFER, static_cast<uint8_t>(is_profile_buffer), 0);
  }
  void add_is_fusion_output(bool is_fusion_output) {
    fbb_.AddElement<uint8_t>(GlobalBufferInfo::VT_IS_FUSION_OUTPUT, static_cast<uint8_t>(is_fusion_output), 0);
  }
  explicit GlobalBufferInfoBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<GlobalBufferInfo> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<GlobalBufferInfo>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<GlobalBufferInfo> CreateGlobalBufferInfo(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t tv = -1LL,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None,
    bool zero_init = false,
    bool is_profile_buffer = false,
    bool is_fusion_output = false) {
  GlobalBufferInfoBuilder builder_(_fbb);
  builder_.add_tv(tv);
  builder_.add_dtype(dtype);
  builder_.add_strides(strides);
  builder_.add_sizes(sizes);
  builder_.add_is_fusion_output(is_fusion_output);
  builder_.add_is_profile_buffer(is_profile_buffer);
  builder_.add_zero_init(zero_init);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<GlobalBufferInfo> CreateGlobalBufferInfoDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t tv = -1LL,
    const std::vector<int64_t> *sizes = nullptr,
    const std::vector<int64_t> *strides = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None,
    bool zero_init = false,
    bool is_profile_buffer = false,
    bool is_fusion_output = false) {
  auto sizes__ = sizes ? _fbb.CreateVector<int64_t>(*sizes) : 0;
  auto strides__ = strides ? _fbb.CreateVector<int64_t>(*strides) : 0;
  return nvfuser::serde::CreateGlobalBufferInfo(
      _fbb,
      tv,
      sizes__,
      strides__,
      dtype,
      zero_init,
      is_profile_buffer,
      is_fusion_output);
}

::flatbuffers::Offset<GlobalBufferInfo> CreateGlobalBufferInfo(::flatbuffers::FlatBufferBuilder &_fbb, const GlobalBufferInfoT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct ExecutorEntryT : public ::flatbuffers::NativeTable {
  typedef ExecutorEntry TableType;
  bool init = false;
  std::unique_ptr<nvfuser::serde::LaunchParamsT> launch_params{};
  std::vector<int32_t> output_aliases{};
  std::vector<int32_t> input_aliases{};
  std::vector<std::unique_ptr<nvfuser::serde::GlobalBufferInfoT>> outputs{};
  std::vector<std::unique_ptr<nvfuser::serde::GlobalBufferInfoT>> intermediates{};
  ExecutorEntryT() = default;
  ExecutorEntryT(const ExecutorEntryT &o);
  ExecutorEntryT(ExecutorEntryT&&) FLATBUFFERS_NOEXCEPT = default;
  ExecutorEntryT &operator=(ExecutorEntryT o) FLATBUFFERS_NOEXCEPT;
};

struct ExecutorEntry FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ExecutorEntryT NativeTableType;
  typedef ExecutorEntryBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INIT = 4,
    VT_LAUNCH_PARAMS = 6,
    VT_OUTPUT_ALIASES = 8,
    VT_INPUT_ALIASES = 10,
    VT_OUTPUTS = 12,
    VT_INTERMEDIATES = 14
  };
  bool init() const {
    return GetField<uint8_t>(VT_INIT, 0) != 0;
  }
  const nvfuser::serde::LaunchParams *launch_params() const {
    return GetPointer<const nvfuser::serde::LaunchParams *>(VT_LAUNCH_PARAMS);
  }
  const ::flatbuffers::Vector<int32_t> *output_aliases() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_OUTPUT_ALIASES);
  }
  const ::flatbuffers::Vector<int32_t> *input_aliases() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_INPUT_ALIASES);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *outputs() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *>(VT_OUTPUTS);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *intermediates() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *>(VT_INTERMEDIATES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_INIT, 1) &&
           VerifyOffset(verifier, VT_LAUNCH_PARAMS) &&
           verifier.VerifyTable(launch_params()) &&
           VerifyOffset(verifier, VT_OUTPUT_ALIASES) &&
           verifier.VerifyVector(output_aliases()) &&
           VerifyOffset(verifier, VT_INPUT_ALIASES) &&
           verifier.VerifyVector(input_aliases()) &&
           VerifyOffset(verifier, VT_OUTPUTS) &&
           verifier.VerifyVector(outputs()) &&
           verifier.VerifyVectorOfTables(outputs()) &&
           VerifyOffset(verifier, VT_INTERMEDIATES) &&
           verifier.VerifyVector(intermediates()) &&
           verifier.VerifyVectorOfTables(intermediates()) &&
           verifier.EndTable();
  }
  ExecutorEntryT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(ExecutorEntryT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<ExecutorEntry> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ExecutorEntryT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct ExecutorEntryBuilder {
  typedef ExecutorEntry Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_init(bool init) {
    fbb_.AddElement<uint8_t>(ExecutorEntry::VT_INIT, static_cast<uint8_t>(init), 0);
  }
  void add_launch_params(::flatbuffers::Offset<nvfuser::serde::LaunchParams> launch_params) {
    fbb_.AddOffset(ExecutorEntry::VT_LAUNCH_PARAMS, launch_params);
  }
  void add_output_aliases(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> output_aliases) {
    fbb_.AddOffset(ExecutorEntry::VT_OUTPUT_ALIASES, output_aliases);
  }
  void add_input_aliases(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> input_aliases) {
    fbb_.AddOffset(ExecutorEntry::VT_INPUT_ALIASES, input_aliases);
  }
  void add_outputs(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> outputs) {
    fbb_.AddOffset(ExecutorEntry::VT_OUTPUTS, outputs);
  }
  void add_intermediates(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> intermediates) {
    fbb_.AddOffset(ExecutorEntry::VT_INTERMEDIATES, intermediates);
  }
  explicit ExecutorEntryBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<ExecutorEntry> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<ExecutorEntry>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<ExecutorEntry> CreateExecutorEntry(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool init = false,
    ::flatbuffers::Offset<nvfuser::serde::LaunchParams> launch_params = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> output_aliases = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> input_aliases = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> outputs = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> intermediates = 0) {
  ExecutorEntryBuilder builder_(_fbb);
  builder_.add_intermediates(intermediates);
  builder_.add_outputs(outputs);
  builder_.add_input_aliases(input_aliases);
  builder_.add_output_aliases(output_aliases);
  builder_.add_launch_params(launch_params);
  builder_.add_init(init);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<ExecutorEntry> CreateExecutorEntryDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool init = false,
    ::flatbuffers::Offset<nvfuser::serde::LaunchParams> launch_params = 0,
    const std::vector<int32_t> *output_aliases = nullptr,
    const std::vector<int32_t> *input_aliases = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *outputs = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *intermediates = nullptr) {
  auto output_aliases__ = output_aliases ? _fbb.CreateVector<int32_t>(*output_aliases) : 0;
  auto input_aliases__ = input_aliases ? _fbb.CreateVector<int32_t>(*input_aliases) : 0;
  auto outputs__ = outputs ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>(*outputs) : 0;
  auto intermediates__ = intermediates ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>(*intermediates) : 0;
  return nvfuser::serde::CreateExecutorEntry(
      _fbb,
      init,
      launch_params,
      output_aliases__,
      input_aliases__,
      outputs__,
      intermediates__);
}

::flatbuffers::Offset<ExecutorEntry> CreateExecutorEntry(::flatbuffers::FlatBufferBuilder &_fbb, const ExecutorEntryT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct AtT : public ::flatbuffers::NativeTable {
  typedef At TableType;
  int64_t index = 0;
};

struct At FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef AtT NativeTableType;
  typedef AtBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INDEX = 4
  };
  int64_t index() const {
    return GetField<int64_t>(VT_INDEX, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_INDEX, 8) &&
           verifier.EndTable();
  }
  AtT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(AtT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<At> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const AtT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct AtBuilder {
  typedef At Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_index(int64_t index) {
    fbb_.AddElement<int64_t>(At::VT_INDEX, index, 0);
  }
  explicit AtBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<At> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<At>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<At> CreateAt(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t index = 0) {
  AtBuilder builder_(_fbb);
  builder_.add_index(index);
  return builder_.Finish();
}

::flatbuffers::Offset<At> CreateAt(::flatbuffers::FlatBufferBuilder &_fbb, const AtT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct BatchNormT : public ::flatbuffers::NativeTable {
  typedef BatchNorm TableType;
  bool training = false;
  bool channels_last = false;
};

struct BatchNorm FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef BatchNormT NativeTableType;
  typedef BatchNormBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TRAINING = 4,
    VT_CHANNELS_LAST = 6
  };
  bool training() const {
    return GetField<uint8_t>(VT_TRAINING, 0) != 0;
  }
  bool channels_last() const {
    return GetField<uint8_t>(VT_CHANNELS_LAST, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_TRAINING, 1) &&
           VerifyField<uint8_t>(verifier, VT_CHANNELS_LAST, 1) &&
           verifier.EndTable();
  }
  BatchNormT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(BatchNormT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<BatchNorm> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const BatchNormT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct BatchNormBuilder {
  typedef BatchNorm Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_training(bool training) {
    fbb_.AddElement<uint8_t>(BatchNorm::VT_TRAINING, static_cast<uint8_t>(training), 0);
  }
  void add_channels_last(bool channels_last) {
    fbb_.AddElement<uint8_t>(BatchNorm::VT_CHANNELS_LAST, static_cast<uint8_t>(channels_last), 0);
  }
  explicit BatchNormBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<BatchNorm> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<BatchNorm>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<BatchNorm> CreateBatchNorm(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool training = false,
    bool channels_last = false) {
  BatchNormBuilder builder_(_fbb);
  builder_.add_channels_last(channels_last);
  builder_.add_training(training);
  return builder_.Finish();
}

::flatbuffers::Offset<BatchNorm> CreateBatchNorm(::flatbuffers::FlatBufferBuilder &_fbb, const BatchNormT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct BroadcastT : public ::flatbuffers::NativeTable {
  typedef Broadcast TableType;
  std::vector<bool> broadcast_dims{};
};

struct Broadcast FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef BroadcastT NativeTableType;
  typedef BroadcastBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BROADCAST_DIMS = 4
  };
  const ::flatbuffers::Vector<uint8_t> *broadcast_dims() const {
    return GetPointer<const ::flatbuffers::Vector<uint8_t> *>(VT_BROADCAST_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_BROADCAST_DIMS) &&
           verifier.VerifyVector(broadcast_dims()) &&
           verifier.EndTable();
  }
  BroadcastT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(BroadcastT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Broadcast> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const BroadcastT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct BroadcastBuilder {
  typedef Broadcast Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_broadcast_dims(::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> broadcast_dims) {
    fbb_.AddOffset(Broadcast::VT_BROADCAST_DIMS, broadcast_dims);
  }
  explicit BroadcastBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Broadcast> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Broadcast>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Broadcast> CreateBroadcast(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> broadcast_dims = 0) {
  BroadcastBuilder builder_(_fbb);
  builder_.add_broadcast_dims(broadcast_dims);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Broadcast> CreateBroadcastDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<uint8_t> *broadcast_dims = nullptr) {
  auto broadcast_dims__ = broadcast_dims ? _fbb.CreateVector<uint8_t>(*broadcast_dims) : 0;
  return nvfuser::serde::CreateBroadcast(
      _fbb,
      broadcast_dims__);
}

::flatbuffers::Offset<Broadcast> CreateBroadcast(::flatbuffers::FlatBufferBuilder &_fbb, const BroadcastT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct BroadcastInDimT : public ::flatbuffers::NativeTable {
  typedef BroadcastInDim TableType;
  uint64_t output_size = 0;
  std::vector<int64_t> broadcast_dims{};
};

struct BroadcastInDim FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef BroadcastInDimT NativeTableType;
  typedef BroadcastInDimBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OUTPUT_SIZE = 4,
    VT_BROADCAST_DIMS = 6
  };
  uint64_t output_size() const {
    return GetField<uint64_t>(VT_OUTPUT_SIZE, 0);
  }
  const ::flatbuffers::Vector<int64_t> *broadcast_dims() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_BROADCAST_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_OUTPUT_SIZE, 8) &&
           VerifyOffset(verifier, VT_BROADCAST_DIMS) &&
           verifier.VerifyVector(broadcast_dims()) &&
           verifier.EndTable();
  }
  BroadcastInDimT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(BroadcastInDimT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<BroadcastInDim> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const BroadcastInDimT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct BroadcastInDimBuilder {
  typedef BroadcastInDim Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_output_size(uint64_t output_size) {
    fbb_.AddElement<uint64_t>(BroadcastInDim::VT_OUTPUT_SIZE, output_size, 0);
  }
  void add_broadcast_dims(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> broadcast_dims) {
    fbb_.AddOffset(BroadcastInDim::VT_BROADCAST_DIMS, broadcast_dims);
  }
  explicit BroadcastInDimBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<BroadcastInDim> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<BroadcastInDim>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<BroadcastInDim> CreateBroadcastInDim(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t output_size = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> broadcast_dims = 0) {
  BroadcastInDimBuilder builder_(_fbb);
  builder_.add_output_size(output_size);
  builder_.add_broadcast_dims(broadcast_dims);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<BroadcastInDim> CreateBroadcastInDimDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t output_size = 0,
    const std::vector<int64_t> *broadcast_dims = nullptr) {
  auto broadcast_dims__ = broadcast_dims ? _fbb.CreateVector<int64_t>(*broadcast_dims) : 0;
  return nvfuser::serde::CreateBroadcastInDim(
      _fbb,
      output_size,
      broadcast_dims__);
}

::flatbuffers::Offset<BroadcastInDim> CreateBroadcastInDim(::flatbuffers::FlatBufferBuilder &_fbb, const BroadcastInDimT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct DtypeT : public ::flatbuffers::NativeTable {
  typedef Dtype TableType;
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None;
};

struct Dtype FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef DtypeT NativeTableType;
  typedef DtypeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4
  };
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
  DtypeT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(DtypeT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Dtype> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const DtypeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct DtypeBuilder {
  typedef Dtype Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Dtype::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit DtypeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Dtype> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Dtype>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Dtype> CreateDtype(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  DtypeBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  return builder_.Finish();
}

::flatbuffers::Offset<Dtype> CreateDtype(::flatbuffers::FlatBufferBuilder &_fbb, const DtypeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct DimensionT : public ::flatbuffers::NativeTable {
  typedef Dimension TableType;
  int64_t dim = 0;
};

struct Dimension FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef DimensionT NativeTableType;
  typedef DimensionBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIM = 4
  };
  int64_t dim() const {
    return GetField<int64_t>(VT_DIM, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DIM, 8) &&
           verifier.EndTable();
  }
  DimensionT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(DimensionT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Dimension> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const DimensionT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct DimensionBuilder {
  typedef Dimension Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dim(int64_t dim) {
    fbb_.AddElement<int64_t>(Dimension::VT_DIM, dim, 0);
  }
  explicit DimensionBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Dimension> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Dimension>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Dimension> CreateDimension(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dim = 0) {
  DimensionBuilder builder_(_fbb);
  builder_.add_dim(dim);
  return builder_.Finish();
}

::flatbuffers::Offset<Dimension> CreateDimension(::flatbuffers::FlatBufferBuilder &_fbb, const DimensionT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct NormT : public ::flatbuffers::NativeTable {
  typedef Norm TableType;
  std::vector<int32_t> axes{};
  int64_t correction = 0;
  bool keep_dim = false;
};

struct Norm FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef NormT NativeTableType;
  typedef NormBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXES = 4,
    VT_CORRECTION = 6,
    VT_KEEP_DIM = 8
  };
  const ::flatbuffers::Vector<int32_t> *axes() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_AXES);
  }
  int64_t correction() const {
    return GetField<int64_t>(VT_CORRECTION, 0);
  }
  bool keep_dim() const {
    return GetField<uint8_t>(VT_KEEP_DIM, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_AXES) &&
           verifier.VerifyVector(axes()) &&
           VerifyField<int64_t>(verifier, VT_CORRECTION, 8) &&
           VerifyField<uint8_t>(verifier, VT_KEEP_DIM, 1) &&
           verifier.EndTable();
  }
  NormT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(NormT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Norm> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const NormT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct NormBuilder {
  typedef Norm Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_axes(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> axes) {
    fbb_.AddOffset(Norm::VT_AXES, axes);
  }
  void add_correction(int64_t correction) {
    fbb_.AddElement<int64_t>(Norm::VT_CORRECTION, correction, 0);
  }
  void add_keep_dim(bool keep_dim) {
    fbb_.AddElement<uint8_t>(Norm::VT_KEEP_DIM, static_cast<uint8_t>(keep_dim), 0);
  }
  explicit NormBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Norm> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Norm>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Norm> CreateNorm(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> axes = 0,
    int64_t correction = 0,
    bool keep_dim = false) {
  NormBuilder builder_(_fbb);
  builder_.add_correction(correction);
  builder_.add_axes(axes);
  builder_.add_keep_dim(keep_dim);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Norm> CreateNormDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int32_t> *axes = nullptr,
    int64_t correction = 0,
    bool keep_dim = false) {
  auto axes__ = axes ? _fbb.CreateVector<int32_t>(*axes) : 0;
  return nvfuser::serde::CreateNorm(
      _fbb,
      axes__,
      correction,
      keep_dim);
}

::flatbuffers::Offset<Norm> CreateNorm(::flatbuffers::FlatBufferBuilder &_fbb, const NormT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct OutputT : public ::flatbuffers::NativeTable {
  typedef Output TableType;
  std::vector<int64_t> stride_order{};
};

struct Output FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef OutputT NativeTableType;
  typedef OutputBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_STRIDE_ORDER = 4
  };
  const ::flatbuffers::Vector<int64_t> *stride_order() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDE_ORDER);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_STRIDE_ORDER) &&
           verifier.VerifyVector(stride_order()) &&
           verifier.EndTable();
  }
  OutputT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(OutputT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Output> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const OutputT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct OutputBuilder {
  typedef Output Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_stride_order(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> stride_order) {
    fbb_.AddOffset(Output::VT_STRIDE_ORDER, stride_order);
  }
  explicit OutputBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Output> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Output>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Output> CreateOutput(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> stride_order = 0) {
  OutputBuilder builder_(_fbb);
  builder_.add_stride_order(stride_order);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Output> CreateOutputDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *stride_order = nullptr) {
  auto stride_order__ = stride_order ? _fbb.CreateVector<int64_t>(*stride_order) : 0;
  return nvfuser::serde::CreateOutput(
      _fbb,
      stride_order__);
}

::flatbuffers::Offset<Output> CreateOutput(::flatbuffers::FlatBufferBuilder &_fbb, const OutputT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct PadT : public ::flatbuffers::NativeTable {
  typedef Pad TableType;
  std::vector<int64_t> pad_widths{};
};

struct Pad FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef PadT NativeTableType;
  typedef PadBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_PAD_WIDTHS = 4
  };
  const ::flatbuffers::Vector<int64_t> *pad_widths() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_PAD_WIDTHS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_PAD_WIDTHS) &&
           verifier.VerifyVector(pad_widths()) &&
           verifier.EndTable();
  }
  PadT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(PadT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Pad> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const PadT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct PadBuilder {
  typedef Pad Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_pad_widths(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> pad_widths) {
    fbb_.AddOffset(Pad::VT_PAD_WIDTHS, pad_widths);
  }
  explicit PadBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Pad> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Pad>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Pad> CreatePad(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> pad_widths = 0) {
  PadBuilder builder_(_fbb);
  builder_.add_pad_widths(pad_widths);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Pad> CreatePadDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *pad_widths = nullptr) {
  auto pad_widths__ = pad_widths ? _fbb.CreateVector<int64_t>(*pad_widths) : 0;
  return nvfuser::serde::CreatePad(
      _fbb,
      pad_widths__);
}

::flatbuffers::Offset<Pad> CreatePad(::flatbuffers::FlatBufferBuilder &_fbb, const PadT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct PermuteT : public ::flatbuffers::NativeTable {
  typedef Permute TableType;
  std::vector<int64_t> dims{};
};

struct Permute FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef PermuteT NativeTableType;
  typedef PermuteBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIMS = 4
  };
  const ::flatbuffers::Vector<int64_t> *dims() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_DIMS) &&
           verifier.VerifyVector(dims()) &&
           verifier.EndTable();
  }
  PermuteT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(PermuteT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Permute> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const PermuteT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct PermuteBuilder {
  typedef Permute Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dims(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> dims) {
    fbb_.AddOffset(Permute::VT_DIMS, dims);
  }
  explicit PermuteBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Permute> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Permute>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Permute> CreatePermute(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> dims = 0) {
  PermuteBuilder builder_(_fbb);
  builder_.add_dims(dims);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Permute> CreatePermuteDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *dims = nullptr) {
  auto dims__ = dims ? _fbb.CreateVector<int64_t>(*dims) : 0;
  return nvfuser::serde::CreatePermute(
      _fbb,
      dims__);
}

::flatbuffers::Offset<Permute> CreatePermute(::flatbuffers::FlatBufferBuilder &_fbb, const PermuteT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct ReductionT : public ::flatbuffers::NativeTable {
  typedef Reduction TableType;
  std::vector<int32_t> axes{};
  bool keep_dim = false;
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None;
};

struct Reduction FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ReductionT NativeTableType;
  typedef ReductionBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXES = 4,
    VT_KEEP_DIM = 6,
    VT_DTYPE = 8
  };
  const ::flatbuffers::Vector<int32_t> *axes() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_AXES);
  }
  bool keep_dim() const {
    return GetField<uint8_t>(VT_KEEP_DIM, 0) != 0;
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_AXES) &&
           verifier.VerifyVector(axes()) &&
           VerifyField<uint8_t>(verifier, VT_KEEP_DIM, 1) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
  ReductionT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(ReductionT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Reduction> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ReductionT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct ReductionBuilder {
  typedef Reduction Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_axes(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> axes) {
    fbb_.AddOffset(Reduction::VT_AXES, axes);
  }
  void add_keep_dim(bool keep_dim) {
    fbb_.AddElement<uint8_t>(Reduction::VT_KEEP_DIM, static_cast<uint8_t>(keep_dim), 0);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Reduction::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit ReductionBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Reduction> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Reduction>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Reduction> CreateReduction(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> axes = 0,
    bool keep_dim = false,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  ReductionBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_axes(axes);
  builder_.add_keep_dim(keep_dim);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Reduction> CreateReductionDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int32_t> *axes = nullptr,
    bool keep_dim = false,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  auto axes__ = axes ? _fbb.CreateVector<int32_t>(*axes) : 0;
  return nvfuser::serde::CreateReduction(
      _fbb,
      axes__,
      keep_dim,
      dtype);
}

::flatbuffers::Offset<Reduction> CreateReduction(::flatbuffers::FlatBufferBuilder &_fbb, const ReductionT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct ReshapeT : public ::flatbuffers::NativeTable {
  typedef Reshape TableType;
  std::vector<int64_t> original_shape{};
  std::vector<int64_t> new_shape{};
};

struct Reshape FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ReshapeT NativeTableType;
  typedef ReshapeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ORIGINAL_SHAPE = 4,
    VT_NEW_SHAPE = 6
  };
  const ::flatbuffers::Vector<int64_t> *original_shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_ORIGINAL_SHAPE);
  }
  const ::flatbuffers::Vector<int64_t> *new_shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_NEW_SHAPE);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ORIGINAL_SHAPE) &&
           verifier.VerifyVector(original_shape()) &&
           VerifyOffset(verifier, VT_NEW_SHAPE) &&
           verifier.VerifyVector(new_shape()) &&
           verifier.EndTable();
  }
  ReshapeT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(ReshapeT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Reshape> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ReshapeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct ReshapeBuilder {
  typedef Reshape Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_original_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> original_shape) {
    fbb_.AddOffset(Reshape::VT_ORIGINAL_SHAPE, original_shape);
  }
  void add_new_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> new_shape) {
    fbb_.AddOffset(Reshape::VT_NEW_SHAPE, new_shape);
  }
  explicit ReshapeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Reshape> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Reshape>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Reshape> CreateReshape(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> original_shape = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> new_shape = 0) {
  ReshapeBuilder builder_(_fbb);
  builder_.add_new_shape(new_shape);
  builder_.add_original_shape(original_shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Reshape> CreateReshapeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *original_shape = nullptr,
    const std::vector<int64_t> *new_shape = nullptr) {
  auto original_shape__ = original_shape ? _fbb.CreateVector<int64_t>(*original_shape) : 0;
  auto new_shape__ = new_shape ? _fbb.CreateVector<int64_t>(*new_shape) : 0;
  return nvfuser::serde::CreateReshape(
      _fbb,
      original_shape__,
      new_shape__);
}

::flatbuffers::Offset<Reshape> CreateReshape(::flatbuffers::FlatBufferBuilder &_fbb, const ReshapeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct SizeT : public ::flatbuffers::NativeTable {
  typedef Size TableType;
  int64_t dim = 0;
};

struct Size FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SizeT NativeTableType;
  typedef SizeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIM = 4
  };
  int64_t dim() const {
    return GetField<int64_t>(VT_DIM, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DIM, 8) &&
           verifier.EndTable();
  }
  SizeT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(SizeT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Size> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const SizeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct SizeBuilder {
  typedef Size Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dim(int64_t dim) {
    fbb_.AddElement<int64_t>(Size::VT_DIM, dim, 0);
  }
  explicit SizeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Size> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Size>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Size> CreateSize(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dim = 0) {
  SizeBuilder builder_(_fbb);
  builder_.add_dim(dim);
  return builder_.Finish();
}

::flatbuffers::Offset<Size> CreateSize(::flatbuffers::FlatBufferBuilder &_fbb, const SizeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct SliceT : public ::flatbuffers::NativeTable {
  typedef Slice TableType;
  std::vector<int64_t> start_indices{};
  std::vector<int64_t> end_indices{};
  std::vector<int64_t> strides{};
};

struct Slice FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SliceT NativeTableType;
  typedef SliceBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_START_INDICES = 4,
    VT_END_INDICES = 6,
    VT_STRIDES = 8
  };
  const ::flatbuffers::Vector<int64_t> *start_indices() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_START_INDICES);
  }
  const ::flatbuffers::Vector<int64_t> *end_indices() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_END_INDICES);
  }
  const ::flatbuffers::Vector<int64_t> *strides() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_START_INDICES) &&
           verifier.VerifyVector(start_indices()) &&
           VerifyOffset(verifier, VT_END_INDICES) &&
           verifier.VerifyVector(end_indices()) &&
           VerifyOffset(verifier, VT_STRIDES) &&
           verifier.VerifyVector(strides()) &&
           verifier.EndTable();
  }
  SliceT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(SliceT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Slice> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const SliceT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct SliceBuilder {
  typedef Slice Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_start_indices(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> start_indices) {
    fbb_.AddOffset(Slice::VT_START_INDICES, start_indices);
  }
  void add_end_indices(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> end_indices) {
    fbb_.AddOffset(Slice::VT_END_INDICES, end_indices);
  }
  void add_strides(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides) {
    fbb_.AddOffset(Slice::VT_STRIDES, strides);
  }
  explicit SliceBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Slice> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Slice>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Slice> CreateSlice(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> start_indices = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> end_indices = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides = 0) {
  SliceBuilder builder_(_fbb);
  builder_.add_strides(strides);
  builder_.add_end_indices(end_indices);
  builder_.add_start_indices(start_indices);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Slice> CreateSliceDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *start_indices = nullptr,
    const std::vector<int64_t> *end_indices = nullptr,
    const std::vector<int64_t> *strides = nullptr) {
  auto start_indices__ = start_indices ? _fbb.CreateVector<int64_t>(*start_indices) : 0;
  auto end_indices__ = end_indices ? _fbb.CreateVector<int64_t>(*end_indices) : 0;
  auto strides__ = strides ? _fbb.CreateVector<int64_t>(*strides) : 0;
  return nvfuser::serde::CreateSlice(
      _fbb,
      start_indices__,
      end_indices__,
      strides__);
}

::flatbuffers::Offset<Slice> CreateSlice(::flatbuffers::FlatBufferBuilder &_fbb, const SliceT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct SqueezeT : public ::flatbuffers::NativeTable {
  typedef Squeeze TableType;
  std::vector<int64_t> original_shape{};
  std::vector<int64_t> squeeze_dims{};
};

struct Squeeze FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SqueezeT NativeTableType;
  typedef SqueezeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ORIGINAL_SHAPE = 4,
    VT_SQUEEZE_DIMS = 6
  };
  const ::flatbuffers::Vector<int64_t> *original_shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_ORIGINAL_SHAPE);
  }
  const ::flatbuffers::Vector<int64_t> *squeeze_dims() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SQUEEZE_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ORIGINAL_SHAPE) &&
           verifier.VerifyVector(original_shape()) &&
           VerifyOffset(verifier, VT_SQUEEZE_DIMS) &&
           verifier.VerifyVector(squeeze_dims()) &&
           verifier.EndTable();
  }
  SqueezeT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(SqueezeT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Squeeze> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const SqueezeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct SqueezeBuilder {
  typedef Squeeze Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_original_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> original_shape) {
    fbb_.AddOffset(Squeeze::VT_ORIGINAL_SHAPE, original_shape);
  }
  void add_squeeze_dims(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> squeeze_dims) {
    fbb_.AddOffset(Squeeze::VT_SQUEEZE_DIMS, squeeze_dims);
  }
  explicit SqueezeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Squeeze> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Squeeze>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Squeeze> CreateSqueeze(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> original_shape = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> squeeze_dims = 0) {
  SqueezeBuilder builder_(_fbb);
  builder_.add_squeeze_dims(squeeze_dims);
  builder_.add_original_shape(original_shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Squeeze> CreateSqueezeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *original_shape = nullptr,
    const std::vector<int64_t> *squeeze_dims = nullptr) {
  auto original_shape__ = original_shape ? _fbb.CreateVector<int64_t>(*original_shape) : 0;
  auto squeeze_dims__ = squeeze_dims ? _fbb.CreateVector<int64_t>(*squeeze_dims) : 0;
  return nvfuser::serde::CreateSqueeze(
      _fbb,
      original_shape__,
      squeeze_dims__);
}

::flatbuffers::Offset<Squeeze> CreateSqueeze(::flatbuffers::FlatBufferBuilder &_fbb, const SqueezeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct TensorT : public ::flatbuffers::NativeTable {
  typedef Tensor TableType;
  std::vector<int64_t> sizes{};
  std::vector<nvfuser::serde::Contiguity> contiguity{};
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None;
  bool is_cpu = false;
};

struct Tensor FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorT NativeTableType;
  typedef TensorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SIZES = 4,
    VT_CONTIGUITY = 6,
    VT_DTYPE = 8,
    VT_IS_CPU = 10
  };
  const ::flatbuffers::Vector<int64_t> *sizes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SIZES);
  }
  const ::flatbuffers::Vector<int32_t> *contiguity() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_CONTIGUITY);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool is_cpu() const {
    return GetField<uint8_t>(VT_IS_CPU, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SIZES) &&
           verifier.VerifyVector(sizes()) &&
           VerifyOffset(verifier, VT_CONTIGUITY) &&
           verifier.VerifyVector(contiguity()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_IS_CPU, 1) &&
           verifier.EndTable();
  }
  TensorT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(TensorT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Tensor> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct TensorBuilder {
  typedef Tensor Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_sizes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes) {
    fbb_.AddOffset(Tensor::VT_SIZES, sizes);
  }
  void add_contiguity(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> contiguity) {
    fbb_.AddOffset(Tensor::VT_CONTIGUITY, contiguity);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Tensor::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  void add_is_cpu(bool is_cpu) {
    fbb_.AddElement<uint8_t>(Tensor::VT_IS_CPU, static_cast<uint8_t>(is_cpu), 0);
  }
  explicit TensorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Tensor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Tensor>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Tensor> CreateTensor(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> contiguity = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None,
    bool is_cpu = false) {
  TensorBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_contiguity(contiguity);
  builder_.add_sizes(sizes);
  builder_.add_is_cpu(is_cpu);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Tensor> CreateTensorDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *sizes = nullptr,
    const std::vector<int32_t> *contiguity = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None,
    bool is_cpu = false) {
  auto sizes__ = sizes ? _fbb.CreateVector<int64_t>(*sizes) : 0;
  auto contiguity__ = contiguity ? _fbb.CreateVector<int32_t>(*contiguity) : 0;
  return nvfuser::serde::CreateTensor(
      _fbb,
      sizes__,
      contiguity__,
      dtype,
      is_cpu);
}

::flatbuffers::Offset<Tensor> CreateTensor(::flatbuffers::FlatBufferBuilder &_fbb, const TensorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct TensorCreationT : public ::flatbuffers::NativeTable {
  typedef TensorCreation TableType;
  std::vector<int64_t> shape{};
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None;
};

struct TensorCreation FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorCreationT NativeTableType;
  typedef TensorCreationBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SHAPE = 4,
    VT_DTYPE = 6
  };
  const ::flatbuffers::Vector<int64_t> *shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SHAPE);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SHAPE) &&
           verifier.VerifyVector(shape()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
  TensorCreationT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(TensorCreationT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<TensorCreation> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorCreationT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct TensorCreationBuilder {
  typedef TensorCreation Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape) {
    fbb_.AddOffset(TensorCreation::VT_SHAPE, shape);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(TensorCreation::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit TensorCreationBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorCreation> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorCreation>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorCreation> CreateTensorCreation(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  TensorCreationBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_shape(shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TensorCreation> CreateTensorCreationDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *shape = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  auto shape__ = shape ? _fbb.CreateVector<int64_t>(*shape) : 0;
  return nvfuser::serde::CreateTensorCreation(
      _fbb,
      shape__,
      dtype);
}

::flatbuffers::Offset<TensorCreation> CreateTensorCreation(::flatbuffers::FlatBufferBuilder &_fbb, const TensorCreationT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct TensorCreationSymbolicT : public ::flatbuffers::NativeTable {
  typedef TensorCreationSymbolic TableType;
  std::vector<nvfuser::serde::State> shape{};
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None;
};

struct TensorCreationSymbolic FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorCreationSymbolicT NativeTableType;
  typedef TensorCreationSymbolicBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SHAPE = 4,
    VT_DTYPE = 6
  };
  const ::flatbuffers::Vector<const nvfuser::serde::State *> *shape() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::State *> *>(VT_SHAPE);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SHAPE) &&
           verifier.VerifyVector(shape()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
  TensorCreationSymbolicT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(TensorCreationSymbolicT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<TensorCreationSymbolic> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorCreationSymbolicT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct TensorCreationSymbolicBuilder {
  typedef TensorCreationSymbolic Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_shape(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> shape) {
    fbb_.AddOffset(TensorCreationSymbolic::VT_SHAPE, shape);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(TensorCreationSymbolic::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit TensorCreationSymbolicBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorCreationSymbolic> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorCreationSymbolic>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorCreationSymbolic> CreateTensorCreationSymbolic(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> shape = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  TensorCreationSymbolicBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_shape(shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TensorCreationSymbolic> CreateTensorCreationSymbolicDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<nvfuser::serde::State> *shape = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  auto shape__ = shape ? _fbb.CreateVectorOfStructs<nvfuser::serde::State>(*shape) : 0;
  return nvfuser::serde::CreateTensorCreationSymbolic(
      _fbb,
      shape__,
      dtype);
}

::flatbuffers::Offset<TensorCreationSymbolic> CreateTensorCreationSymbolic(::flatbuffers::FlatBufferBuilder &_fbb, const TensorCreationSymbolicT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct VectorT : public ::flatbuffers::NativeTable {
  typedef Vector TableType;
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None;
};

struct Vector FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef VectorT NativeTableType;
  typedef VectorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4
  };
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
  VectorT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(VectorT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Vector> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const VectorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct VectorBuilder {
  typedef Vector Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Vector::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit VectorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Vector> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Vector>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Vector> CreateVector(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  VectorBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  return builder_.Finish();
}

::flatbuffers::Offset<Vector> CreateVector(::flatbuffers::FlatBufferBuilder &_fbb, const VectorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct KernelSummaryT : public ::flatbuffers::NativeTable {
  typedef KernelSummary TableType;
  bool has_cooperative_grid_reduction = false;
  bool has_dynamic_local_memory_allocations = false;
  bool has_block_reductions = false;
  bool has_grid_reductions = false;
  bool has_block_broadcasts = false;
  bool has_grid_broadcasts = false;
  bool has_block_welford = false;
  bool has_grid_welford = false;
  bool has_outer_grouped_grid_welford = false;
  nvfuser::serde::DataType largest_smem_data_type = nvfuser::serde::DataType_None;
  int32_t outer_grouped_grid_welford_largest_smem_size = 0;
  std::unique_ptr<nvfuser::serde::NaiveValueGeneratorT> generator{};
  std::vector<std::unique_ptr<nvfuser::serde::AllocateBufferT>> global_allocations{};
  KernelSummaryT() = default;
  KernelSummaryT(const KernelSummaryT &o);
  KernelSummaryT(KernelSummaryT&&) FLATBUFFERS_NOEXCEPT = default;
  KernelSummaryT &operator=(KernelSummaryT o) FLATBUFFERS_NOEXCEPT;
};

struct KernelSummary FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef KernelSummaryT NativeTableType;
  typedef KernelSummaryBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_HAS_COOPERATIVE_GRID_REDUCTION = 4,
    VT_HAS_DYNAMIC_LOCAL_MEMORY_ALLOCATIONS = 6,
    VT_HAS_BLOCK_REDUCTIONS = 8,
    VT_HAS_GRID_REDUCTIONS = 10,
    VT_HAS_BLOCK_BROADCASTS = 12,
    VT_HAS_GRID_BROADCASTS = 14,
    VT_HAS_BLOCK_WELFORD = 16,
    VT_HAS_GRID_WELFORD = 18,
    VT_HAS_OUTER_GROUPED_GRID_WELFORD = 20,
    VT_LARGEST_SMEM_DATA_TYPE = 22,
    VT_OUTER_GROUPED_GRID_WELFORD_LARGEST_SMEM_SIZE = 24,
    VT_GENERATOR = 26,
    VT_GLOBAL_ALLOCATIONS = 28
  };
  bool has_cooperative_grid_reduction() const {
    return GetField<uint8_t>(VT_HAS_COOPERATIVE_GRID_REDUCTION, 0) != 0;
  }
  bool has_dynamic_local_memory_allocations() const {
    return GetField<uint8_t>(VT_HAS_DYNAMIC_LOCAL_MEMORY_ALLOCATIONS, 0) != 0;
  }
  bool has_block_reductions() const {
    return GetField<uint8_t>(VT_HAS_BLOCK_REDUCTIONS, 0) != 0;
  }
  bool has_grid_reductions() const {
    return GetField<uint8_t>(VT_HAS_GRID_REDUCTIONS, 0) != 0;
  }
  bool has_block_broadcasts() const {
    return GetField<uint8_t>(VT_HAS_BLOCK_BROADCASTS, 0) != 0;
  }
  bool has_grid_broadcasts() const {
    return GetField<uint8_t>(VT_HAS_GRID_BROADCASTS, 0) != 0;
  }
  bool has_block_welford() const {
    return GetField<uint8_t>(VT_HAS_BLOCK_WELFORD, 0) != 0;
  }
  bool has_grid_welford() const {
    return GetField<uint8_t>(VT_HAS_GRID_WELFORD, 0) != 0;
  }
  bool has_outer_grouped_grid_welford() const {
    return GetField<uint8_t>(VT_HAS_OUTER_GROUPED_GRID_WELFORD, 0) != 0;
  }
  nvfuser::serde::DataType largest_smem_data_type() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_LARGEST_SMEM_DATA_TYPE, 0));
  }
  int32_t outer_grouped_grid_welford_largest_smem_size() const {
    return GetField<int32_t>(VT_OUTER_GROUPED_GRID_WELFORD_LARGEST_SMEM_SIZE, 0);
  }
  const nvfuser::serde::NaiveValueGenerator *generator() const {
    return GetPointer<const nvfuser::serde::NaiveValueGenerator *>(VT_GENERATOR);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>> *global_allocations() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>> *>(VT_GLOBAL_ALLOCATIONS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_HAS_COOPERATIVE_GRID_REDUCTION, 1) &&
           VerifyField<uint8_t>(verifier, VT_HAS_DYNAMIC_LOCAL_MEMORY_ALLOCATIONS, 1) &&
           VerifyField<uint8_t>(verifier, VT_HAS_BLOCK_REDUCTIONS, 1) &&
           VerifyField<uint8_t>(verifier, VT_HAS_GRID_REDUCTIONS, 1) &&
           VerifyField<uint8_t>(verifier, VT_HAS_BLOCK_BROADCASTS, 1) &&
           VerifyField<uint8_t>(verifier, VT_HAS_GRID_BROADCASTS, 1) &&
           VerifyField<uint8_t>(verifier, VT_HAS_BLOCK_WELFORD, 1) &&
           VerifyField<uint8_t>(verifier, VT_HAS_GRID_WELFORD, 1) &&
           VerifyField<uint8_t>(verifier, VT_HAS_OUTER_GROUPED_GRID_WELFORD, 1) &&
           VerifyField<int32_t>(verifier, VT_LARGEST_SMEM_DATA_TYPE, 4) &&
           VerifyField<int32_t>(verifier, VT_OUTER_GROUPED_GRID_WELFORD_LARGEST_SMEM_SIZE, 4) &&
           VerifyOffset(verifier, VT_GENERATOR) &&
           verifier.VerifyTable(generator()) &&
           VerifyOffset(verifier, VT_GLOBAL_ALLOCATIONS) &&
           verifier.VerifyVector(global_allocations()) &&
           verifier.VerifyVectorOfTables(global_allocations()) &&
           verifier.EndTable();
  }
  KernelSummaryT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(KernelSummaryT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<KernelSummary> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const KernelSummaryT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct KernelSummaryBuilder {
  typedef KernelSummary Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_has_cooperative_grid_reduction(bool has_cooperative_grid_reduction) {
    fbb_.AddElement<uint8_t>(KernelSummary::VT_HAS_COOPERATIVE_GRID_REDUCTION, static_cast<uint8_t>(has_cooperative_grid_reduction), 0);
  }
  void add_has_dynamic_local_memory_allocations(bool has_dynamic_local_memory_allocations) {
    fbb_.AddElement<uint8_t>(KernelSummary::VT_HAS_DYNAMIC_LOCAL_MEMORY_ALLOCATIONS, static_cast<uint8_t>(has_dynamic_local_memory_allocations), 0);
  }
  void add_has_block_reductions(bool has_block_reductions) {
    fbb_.AddElement<uint8_t>(KernelSummary::VT_HAS_BLOCK_REDUCTIONS, static_cast<uint8_t>(has_block_reductions), 0);
  }
  void add_has_grid_reductions(bool has_grid_reductions) {
    fbb_.AddElement<uint8_t>(KernelSummary::VT_HAS_GRID_REDUCTIONS, static_cast<uint8_t>(has_grid_reductions), 0);
  }
  void add_has_block_broadcasts(bool has_block_broadcasts) {
    fbb_.AddElement<uint8_t>(KernelSummary::VT_HAS_BLOCK_BROADCASTS, static_cast<uint8_t>(has_block_broadcasts), 0);
  }
  void add_has_grid_broadcasts(bool has_grid_broadcasts) {
    fbb_.AddElement<uint8_t>(KernelSummary::VT_HAS_GRID_BROADCASTS, static_cast<uint8_t>(has_grid_broadcasts), 0);
  }
  void add_has_block_welford(bool has_block_welford) {
    fbb_.AddElement<uint8_t>(KernelSummary::VT_HAS_BLOCK_WELFORD, static_cast<uint8_t>(has_block_welford), 0);
  }
  void add_has_grid_welford(bool has_grid_welford) {
    fbb_.AddElement<uint8_t>(KernelSummary::VT_HAS_GRID_WELFORD, static_cast<uint8_t>(has_grid_welford), 0);
  }
  void add_has_outer_grouped_grid_welford(bool has_outer_grouped_grid_welford) {
    fbb_.AddElement<uint8_t>(KernelSummary::VT_HAS_OUTER_GROUPED_GRID_WELFORD, static_cast<uint8_t>(has_outer_grouped_grid_welford), 0);
  }
  void add_largest_smem_data_type(nvfuser::serde::DataType largest_smem_data_type) {
    fbb_.AddElement<int32_t>(KernelSummary::VT_LARGEST_SMEM_DATA_TYPE, static_cast<int32_t>(largest_smem_data_type), 0);
  }
  void add_outer_grouped_grid_welford_largest_smem_size(int32_t outer_grouped_grid_welford_largest_smem_size) {
    fbb_.AddElement<int32_t>(KernelSummary::VT_OUTER_GROUPED_GRID_WELFORD_LARGEST_SMEM_SIZE, outer_grouped_grid_welford_largest_smem_size, 0);
  }
  void add_generator(::flatbuffers::Offset<nvfuser::serde::NaiveValueGenerator> generator) {
    fbb_.AddOffset(KernelSummary::VT_GENERATOR, generator);
  }
  void add_global_allocations(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>>> global_allocations) {
    fbb_.AddOffset(KernelSummary::VT_GLOBAL_ALLOCATIONS, global_allocations);
  }
  explicit KernelSummaryBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<KernelSummary> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<KernelSummary>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<KernelSummary> CreateKernelSummary(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool has_cooperative_grid_reduction = false,
    bool has_dynamic_local_memory_allocations = false,
    bool has_block_reductions = false,
    bool has_grid_reductions = false,
    bool has_block_broadcasts = false,
    bool has_grid_broadcasts = false,
    bool has_block_welford = false,
    bool has_grid_welford = false,
    bool has_outer_grouped_grid_welford = false,
    nvfuser::serde::DataType largest_smem_data_type = nvfuser::serde::DataType_None,
    int32_t outer_grouped_grid_welford_largest_smem_size = 0,
    ::flatbuffers::Offset<nvfuser::serde::NaiveValueGenerator> generator = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>>> global_allocations = 0) {
  KernelSummaryBuilder builder_(_fbb);
  builder_.add_global_allocations(global_allocations);
  builder_.add_generator(generator);
  builder_.add_outer_grouped_grid_welford_largest_smem_size(outer_grouped_grid_welford_largest_smem_size);
  builder_.add_largest_smem_data_type(largest_smem_data_type);
  builder_.add_has_outer_grouped_grid_welford(has_outer_grouped_grid_welford);
  builder_.add_has_grid_welford(has_grid_welford);
  builder_.add_has_block_welford(has_block_welford);
  builder_.add_has_grid_broadcasts(has_grid_broadcasts);
  builder_.add_has_block_broadcasts(has_block_broadcasts);
  builder_.add_has_grid_reductions(has_grid_reductions);
  builder_.add_has_block_reductions(has_block_reductions);
  builder_.add_has_dynamic_local_memory_allocations(has_dynamic_local_memory_allocations);
  builder_.add_has_cooperative_grid_reduction(has_cooperative_grid_reduction);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<KernelSummary> CreateKernelSummaryDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool has_cooperative_grid_reduction = false,
    bool has_dynamic_local_memory_allocations = false,
    bool has_block_reductions = false,
    bool has_grid_reductions = false,
    bool has_block_broadcasts = false,
    bool has_grid_broadcasts = false,
    bool has_block_welford = false,
    bool has_grid_welford = false,
    bool has_outer_grouped_grid_welford = false,
    nvfuser::serde::DataType largest_smem_data_type = nvfuser::serde::DataType_None,
    int32_t outer_grouped_grid_welford_largest_smem_size = 0,
    ::flatbuffers::Offset<nvfuser::serde::NaiveValueGenerator> generator = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>> *global_allocations = nullptr) {
  auto global_allocations__ = global_allocations ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>>(*global_allocations) : 0;
  return nvfuser::serde::CreateKernelSummary(
      _fbb,
      has_cooperative_grid_reduction,
      has_dynamic_local_memory_allocations,
      has_block_reductions,
      has_grid_reductions,
      has_block_broadcasts,
      has_grid_broadcasts,
      has_block_welford,
      has_grid_welford,
      has_outer_grouped_grid_welford,
      largest_smem_data_type,
      outer_grouped_grid_welford_largest_smem_size,
      generator,
      global_allocations__);
}

::flatbuffers::Offset<KernelSummary> CreateKernelSummary(::flatbuffers::FlatBufferBuilder &_fbb, const KernelSummaryT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct FusionExecutorT : public ::flatbuffers::NativeTable {
  typedef FusionExecutor TableType;
  int64_t device_smem_limit = 0;
  int64_t block_size_high_water_mark = 0;
  int64_t maxrregcount_high_water_mark = 0;
  int64_t warp_size = 0;
  int64_t fusion_id = 0;
  int64_t fusion_id_counter = 0;
  std::string kernel_code{};
  std::vector<uint64_t> executor_entry_lookup_keys{};
  std::vector<std::unique_ptr<nvfuser::serde::ExecutorEntryT>> executor_entry_lookup_values{};
  nvfuser::serde::DataType index_type = nvfuser::serde::DataType_None;
  std::unique_ptr<nvfuser::serde::KernelSummaryT> summary{};
  FusionExecutorT() = default;
  FusionExecutorT(const FusionExecutorT &o);
  FusionExecutorT(FusionExecutorT&&) FLATBUFFERS_NOEXCEPT = default;
  FusionExecutorT &operator=(FusionExecutorT o) FLATBUFFERS_NOEXCEPT;
};

struct FusionExecutor FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionExecutorT NativeTableType;
  typedef FusionExecutorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DEVICE_SMEM_LIMIT = 4,
    VT_BLOCK_SIZE_HIGH_WATER_MARK = 6,
    VT_MAXRREGCOUNT_HIGH_WATER_MARK = 8,
    VT_WARP_SIZE = 10,
    VT_FUSION_ID = 12,
    VT_FUSION_ID_COUNTER = 14,
    VT_KERNEL_CODE = 16,
    VT_EXECUTOR_ENTRY_LOOKUP_KEYS = 18,
    VT_EXECUTOR_ENTRY_LOOKUP_VALUES = 20,
    VT_INDEX_TYPE = 22,
    VT_SUMMARY = 24
  };
  int64_t device_smem_limit() const {
    return GetField<int64_t>(VT_DEVICE_SMEM_LIMIT, 0);
  }
  int64_t block_size_high_water_mark() const {
    return GetField<int64_t>(VT_BLOCK_SIZE_HIGH_WATER_MARK, 0);
  }
  int64_t maxrregcount_high_water_mark() const {
    return GetField<int64_t>(VT_MAXRREGCOUNT_HIGH_WATER_MARK, 0);
  }
  int64_t warp_size() const {
    return GetField<int64_t>(VT_WARP_SIZE, 0);
  }
  int64_t fusion_id() const {
    return GetField<int64_t>(VT_FUSION_ID, 0);
  }
  int64_t fusion_id_counter() const {
    return GetField<int64_t>(VT_FUSION_ID_COUNTER, 0);
  }
  const ::flatbuffers::String *kernel_code() const {
    return GetPointer<const ::flatbuffers::String *>(VT_KERNEL_CODE);
  }
  const ::flatbuffers::Vector<uint64_t> *executor_entry_lookup_keys() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_EXECUTOR_ENTRY_LOOKUP_KEYS);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>> *executor_entry_lookup_values() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>> *>(VT_EXECUTOR_ENTRY_LOOKUP_VALUES);
  }
  nvfuser::serde::DataType index_type() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_INDEX_TYPE, 0));
  }
  const nvfuser::serde::KernelSummary *summary() const {
    return GetPointer<const nvfuser::serde::KernelSummary *>(VT_SUMMARY);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DEVICE_SMEM_LIMIT, 8) &&
           VerifyField<int64_t>(verifier, VT_BLOCK_SIZE_HIGH_WATER_MARK, 8) &&
           VerifyField<int64_t>(verifier, VT_MAXRREGCOUNT_HIGH_WATER_MARK, 8) &&
           VerifyField<int64_t>(verifier, VT_WARP_SIZE, 8) &&
           VerifyField<int64_t>(verifier, VT_FUSION_ID, 8) &&
           VerifyField<int64_t>(verifier, VT_FUSION_ID_COUNTER, 8) &&
           VerifyOffset(verifier, VT_KERNEL_CODE) &&
           verifier.VerifyString(kernel_code()) &&
           VerifyOffset(verifier, VT_EXECUTOR_ENTRY_LOOKUP_KEYS) &&
           verifier.VerifyVector(executor_entry_lookup_keys()) &&
           VerifyOffset(verifier, VT_EXECUTOR_ENTRY_LOOKUP_VALUES) &&
           verifier.VerifyVector(executor_entry_lookup_values()) &&
           verifier.VerifyVectorOfTables(executor_entry_lookup_values()) &&
           VerifyField<int32_t>(verifier, VT_INDEX_TYPE, 4) &&
           VerifyOffset(verifier, VT_SUMMARY) &&
           verifier.VerifyTable(summary()) &&
           verifier.EndTable();
  }
  FusionExecutorT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(FusionExecutorT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<FusionExecutor> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const FusionExecutorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct FusionExecutorBuilder {
  typedef FusionExecutor Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_device_smem_limit(int64_t device_smem_limit) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_DEVICE_SMEM_LIMIT, device_smem_limit, 0);
  }
  void add_block_size_high_water_mark(int64_t block_size_high_water_mark) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_BLOCK_SIZE_HIGH_WATER_MARK, block_size_high_water_mark, 0);
  }
  void add_maxrregcount_high_water_mark(int64_t maxrregcount_high_water_mark) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_MAXRREGCOUNT_HIGH_WATER_MARK, maxrregcount_high_water_mark, 0);
  }
  void add_warp_size(int64_t warp_size) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_WARP_SIZE, warp_size, 0);
  }
  void add_fusion_id(int64_t fusion_id) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_FUSION_ID, fusion_id, 0);
  }
  void add_fusion_id_counter(int64_t fusion_id_counter) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_FUSION_ID_COUNTER, fusion_id_counter, 0);
  }
  void add_kernel_code(::flatbuffers::Offset<::flatbuffers::String> kernel_code) {
    fbb_.AddOffset(FusionExecutor::VT_KERNEL_CODE, kernel_code);
  }
  void add_executor_entry_lookup_keys(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> executor_entry_lookup_keys) {
    fbb_.AddOffset(FusionExecutor::VT_EXECUTOR_ENTRY_LOOKUP_KEYS, executor_entry_lookup_keys);
  }
  void add_executor_entry_lookup_values(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>>> executor_entry_lookup_values) {
    fbb_.AddOffset(FusionExecutor::VT_EXECUTOR_ENTRY_LOOKUP_VALUES, executor_entry_lookup_values);
  }
  void add_index_type(nvfuser::serde::DataType index_type) {
    fbb_.AddElement<int32_t>(FusionExecutor::VT_INDEX_TYPE, static_cast<int32_t>(index_type), 0);
  }
  void add_summary(::flatbuffers::Offset<nvfuser::serde::KernelSummary> summary) {
    fbb_.AddOffset(FusionExecutor::VT_SUMMARY, summary);
  }
  explicit FusionExecutorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionExecutor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionExecutor>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionExecutor> CreateFusionExecutor(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t device_smem_limit = 0,
    int64_t block_size_high_water_mark = 0,
    int64_t maxrregcount_high_water_mark = 0,
    int64_t warp_size = 0,
    int64_t fusion_id = 0,
    int64_t fusion_id_counter = 0,
    ::flatbuffers::Offset<::flatbuffers::String> kernel_code = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> executor_entry_lookup_keys = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>>> executor_entry_lookup_values = 0,
    nvfuser::serde::DataType index_type = nvfuser::serde::DataType_None,
    ::flatbuffers::Offset<nvfuser::serde::KernelSummary> summary = 0) {
  FusionExecutorBuilder builder_(_fbb);
  builder_.add_fusion_id_counter(fusion_id_counter);
  builder_.add_fusion_id(fusion_id);
  builder_.add_warp_size(warp_size);
  builder_.add_maxrregcount_high_water_mark(maxrregcount_high_water_mark);
  builder_.add_block_size_high_water_mark(block_size_high_water_mark);
  builder_.add_device_smem_limit(device_smem_limit);
  builder_.add_summary(summary);
  builder_.add_index_type(index_type);
  builder_.add_executor_entry_lookup_values(executor_entry_lookup_values);
  builder_.add_executor_entry_lookup_keys(executor_entry_lookup_keys);
  builder_.add_kernel_code(kernel_code);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionExecutor> CreateFusionExecutorDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t device_smem_limit = 0,
    int64_t block_size_high_water_mark = 0,
    int64_t maxrregcount_high_water_mark = 0,
    int64_t warp_size = 0,
    int64_t fusion_id = 0,
    int64_t fusion_id_counter = 0,
    const char *kernel_code = nullptr,
    const std::vector<uint64_t> *executor_entry_lookup_keys = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>> *executor_entry_lookup_values = nullptr,
    nvfuser::serde::DataType index_type = nvfuser::serde::DataType_None,
    ::flatbuffers::Offset<nvfuser::serde::KernelSummary> summary = 0) {
  auto kernel_code__ = kernel_code ? _fbb.CreateString(kernel_code) : 0;
  auto executor_entry_lookup_keys__ = executor_entry_lookup_keys ? _fbb.CreateVector<uint64_t>(*executor_entry_lookup_keys) : 0;
  auto executor_entry_lookup_values__ = executor_entry_lookup_values ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>>(*executor_entry_lookup_values) : 0;
  return nvfuser::serde::CreateFusionExecutor(
      _fbb,
      device_smem_limit,
      block_size_high_water_mark,
      maxrregcount_high_water_mark,
      warp_size,
      fusion_id,
      fusion_id_counter,
      kernel_code__,
      executor_entry_lookup_keys__,
      executor_entry_lookup_values__,
      index_type,
      summary);
}

::flatbuffers::Offset<FusionExecutor> CreateFusionExecutor(::flatbuffers::FlatBufferBuilder &_fbb, const FusionExecutorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct FusionKernelRuntimeT : public ::flatbuffers::NativeTable {
  typedef FusionKernelRuntime TableType;
  std::unique_ptr<nvfuser::serde::KernelArgumentHolderT> args{};
  std::vector<std::unique_ptr<nvfuser::serde::FusionExecutorT>> executors{};
  FusionKernelRuntimeT() = default;
  FusionKernelRuntimeT(const FusionKernelRuntimeT &o);
  FusionKernelRuntimeT(FusionKernelRuntimeT&&) FLATBUFFERS_NOEXCEPT = default;
  FusionKernelRuntimeT &operator=(FusionKernelRuntimeT o) FLATBUFFERS_NOEXCEPT;
};

struct FusionKernelRuntime FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionKernelRuntimeT NativeTableType;
  typedef FusionKernelRuntimeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ARGS = 4,
    VT_EXECUTORS = 6
  };
  const nvfuser::serde::KernelArgumentHolder *args() const {
    return GetPointer<const nvfuser::serde::KernelArgumentHolder *>(VT_ARGS);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>> *executors() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>> *>(VT_EXECUTORS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ARGS) &&
           verifier.VerifyTable(args()) &&
           VerifyOffset(verifier, VT_EXECUTORS) &&
           verifier.VerifyVector(executors()) &&
           verifier.VerifyVectorOfTables(executors()) &&
           verifier.EndTable();
  }
  FusionKernelRuntimeT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(FusionKernelRuntimeT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<FusionKernelRuntime> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const FusionKernelRuntimeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct FusionKernelRuntimeBuilder {
  typedef FusionKernelRuntime Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_args(::flatbuffers::Offset<nvfuser::serde::KernelArgumentHolder> args) {
    fbb_.AddOffset(FusionKernelRuntime::VT_ARGS, args);
  }
  void add_executors(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>>> executors) {
    fbb_.AddOffset(FusionKernelRuntime::VT_EXECUTORS, executors);
  }
  explicit FusionKernelRuntimeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionKernelRuntime> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionKernelRuntime>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionKernelRuntime> CreateFusionKernelRuntime(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::KernelArgumentHolder> args = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>>> executors = 0) {
  FusionKernelRuntimeBuilder builder_(_fbb);
  builder_.add_executors(executors);
  builder_.add_args(args);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionKernelRuntime> CreateFusionKernelRuntimeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::KernelArgumentHolder> args = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>> *executors = nullptr) {
  auto executors__ = executors ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>>(*executors) : 0;
  return nvfuser::serde::CreateFusionKernelRuntime(
      _fbb,
      args,
      executors__);
}

::flatbuffers::Offset<FusionKernelRuntime> CreateFusionKernelRuntime(::flatbuffers::FlatBufferBuilder &_fbb, const FusionKernelRuntimeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct InputsIdLookupT : public ::flatbuffers::NativeTable {
  typedef InputsIdLookup TableType;
  uint64_t max_cache_size = 0;
  uint64_t current_id = 0;
  std::vector<std::string> lru_cache{};
  std::vector<std::string> encoding_lookup_keys{};
  std::vector<nvfuser::serde::EncodingEntry> encoding_lookup_values{};
};

struct InputsIdLookup FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef InputsIdLookupT NativeTableType;
  typedef InputsIdLookupBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_MAX_CACHE_SIZE = 4,
    VT_CURRENT_ID = 6,
    VT_LRU_CACHE = 8,
    VT_ENCODING_LOOKUP_KEYS = 10,
    VT_ENCODING_LOOKUP_VALUES = 12
  };
  uint64_t max_cache_size() const {
    return GetField<uint64_t>(VT_MAX_CACHE_SIZE, 0);
  }
  uint64_t current_id() const {
    return GetField<uint64_t>(VT_CURRENT_ID, 0);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *lru_cache() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *>(VT_LRU_CACHE);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *encoding_lookup_keys() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *>(VT_ENCODING_LOOKUP_KEYS);
  }
  const ::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *> *encoding_lookup_values() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *> *>(VT_ENCODING_LOOKUP_VALUES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_MAX_CACHE_SIZE, 8) &&
           VerifyField<uint64_t>(verifier, VT_CURRENT_ID, 8) &&
           VerifyOffset(verifier, VT_LRU_CACHE) &&
           verifier.VerifyVector(lru_cache()) &&
           verifier.VerifyVectorOfStrings(lru_cache()) &&
           VerifyOffset(verifier, VT_ENCODING_LOOKUP_KEYS) &&
           verifier.VerifyVector(encoding_lookup_keys()) &&
           verifier.VerifyVectorOfStrings(encoding_lookup_keys()) &&
           VerifyOffset(verifier, VT_ENCODING_LOOKUP_VALUES) &&
           verifier.VerifyVector(encoding_lookup_values()) &&
           verifier.EndTable();
  }
  InputsIdLookupT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(InputsIdLookupT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<InputsIdLookup> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const InputsIdLookupT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct InputsIdLookupBuilder {
  typedef InputsIdLookup Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_max_cache_size(uint64_t max_cache_size) {
    fbb_.AddElement<uint64_t>(InputsIdLookup::VT_MAX_CACHE_SIZE, max_cache_size, 0);
  }
  void add_current_id(uint64_t current_id) {
    fbb_.AddElement<uint64_t>(InputsIdLookup::VT_CURRENT_ID, current_id, 0);
  }
  void add_lru_cache(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> lru_cache) {
    fbb_.AddOffset(InputsIdLookup::VT_LRU_CACHE, lru_cache);
  }
  void add_encoding_lookup_keys(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> encoding_lookup_keys) {
    fbb_.AddOffset(InputsIdLookup::VT_ENCODING_LOOKUP_KEYS, encoding_lookup_keys);
  }
  void add_encoding_lookup_values(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *>> encoding_lookup_values) {
    fbb_.AddOffset(InputsIdLookup::VT_ENCODING_LOOKUP_VALUES, encoding_lookup_values);
  }
  explicit InputsIdLookupBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<InputsIdLookup> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<InputsIdLookup>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<InputsIdLookup> CreateInputsIdLookup(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_cache_size = 0,
    uint64_t current_id = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> lru_cache = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> encoding_lookup_keys = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *>> encoding_lookup_values = 0) {
  InputsIdLookupBuilder builder_(_fbb);
  builder_.add_current_id(current_id);
  builder_.add_max_cache_size(max_cache_size);
  builder_.add_encoding_lookup_values(encoding_lookup_values);
  builder_.add_encoding_lookup_keys(encoding_lookup_keys);
  builder_.add_lru_cache(lru_cache);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<InputsIdLookup> CreateInputsIdLookupDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_cache_size = 0,
    uint64_t current_id = 0,
    const std::vector<::flatbuffers::Offset<::flatbuffers::String>> *lru_cache = nullptr,
    const std::vector<::flatbuffers::Offset<::flatbuffers::String>> *encoding_lookup_keys = nullptr,
    const std::vector<nvfuser::serde::EncodingEntry> *encoding_lookup_values = nullptr) {
  auto lru_cache__ = lru_cache ? _fbb.CreateVector<::flatbuffers::Offset<::flatbuffers::String>>(*lru_cache) : 0;
  auto encoding_lookup_keys__ = encoding_lookup_keys ? _fbb.CreateVector<::flatbuffers::Offset<::flatbuffers::String>>(*encoding_lookup_keys) : 0;
  auto encoding_lookup_values__ = encoding_lookup_values ? _fbb.CreateVectorOfStructs<nvfuser::serde::EncodingEntry>(*encoding_lookup_values) : 0;
  return nvfuser::serde::CreateInputsIdLookup(
      _fbb,
      max_cache_size,
      current_id,
      lru_cache__,
      encoding_lookup_keys__,
      encoding_lookup_values__);
}

::flatbuffers::Offset<InputsIdLookup> CreateInputsIdLookup(::flatbuffers::FlatBufferBuilder &_fbb, const InputsIdLookupT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct KernelRuntimeStateT : public ::flatbuffers::NativeTable {
  typedef KernelRuntimeState TableType;
  uint64_t device_id = 0;
  bool has_dynamic_transform_info = false;
  std::vector<std::unique_ptr<nvfuser::serde::FusionKernelRuntimeT>> runtimes{};
  KernelRuntimeStateT() = default;
  KernelRuntimeStateT(const KernelRuntimeStateT &o);
  KernelRuntimeStateT(KernelRuntimeStateT&&) FLATBUFFERS_NOEXCEPT = default;
  KernelRuntimeStateT &operator=(KernelRuntimeStateT o) FLATBUFFERS_NOEXCEPT;
};

struct KernelRuntimeState FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef KernelRuntimeStateT NativeTableType;
  typedef KernelRuntimeStateBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DEVICE_ID = 4,
    VT_HAS_DYNAMIC_TRANSFORM_INFO = 6,
    VT_RUNTIMES = 8
  };
  uint64_t device_id() const {
    return GetField<uint64_t>(VT_DEVICE_ID, 0);
  }
  bool has_dynamic_transform_info() const {
    return GetField<uint8_t>(VT_HAS_DYNAMIC_TRANSFORM_INFO, 0) != 0;
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>> *runtimes() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>> *>(VT_RUNTIMES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_DEVICE_ID, 8) &&
           VerifyField<uint8_t>(verifier, VT_HAS_DYNAMIC_TRANSFORM_INFO, 1) &&
           VerifyOffset(verifier, VT_RUNTIMES) &&
           verifier.VerifyVector(runtimes()) &&
           verifier.VerifyVectorOfTables(runtimes()) &&
           verifier.EndTable();
  }
  KernelRuntimeStateT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(KernelRuntimeStateT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<KernelRuntimeState> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const KernelRuntimeStateT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct KernelRuntimeStateBuilder {
  typedef KernelRuntimeState Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_device_id(uint64_t device_id) {
    fbb_.AddElement<uint64_t>(KernelRuntimeState::VT_DEVICE_ID, device_id, 0);
  }
  void add_has_dynamic_transform_info(bool has_dynamic_transform_info) {
    fbb_.AddElement<uint8_t>(KernelRuntimeState::VT_HAS_DYNAMIC_TRANSFORM_INFO, static_cast<uint8_t>(has_dynamic_transform_info), 0);
  }
  void add_runtimes(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>>> runtimes) {
    fbb_.AddOffset(KernelRuntimeState::VT_RUNTIMES, runtimes);
  }
  explicit KernelRuntimeStateBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<KernelRuntimeState> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<KernelRuntimeState>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<KernelRuntimeState> CreateKernelRuntimeState(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t device_id = 0,
    bool has_dynamic_transform_info = false,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>>> runtimes = 0) {
  KernelRuntimeStateBuilder builder_(_fbb);
  builder_.add_device_id(device_id);
  builder_.add_runtimes(runtimes);
  builder_.add_has_dynamic_transform_info(has_dynamic_transform_info);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<KernelRuntimeState> CreateKernelRuntimeStateDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t device_id = 0,
    bool has_dynamic_transform_info = false,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>> *runtimes = nullptr) {
  auto runtimes__ = runtimes ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>>(*runtimes) : 0;
  return nvfuser::serde::CreateKernelRuntimeState(
      _fbb,
      device_id,
      has_dynamic_transform_info,
      runtimes__);
}

::flatbuffers::Offset<KernelRuntimeState> CreateKernelRuntimeState(::flatbuffers::FlatBufferBuilder &_fbb, const KernelRuntimeStateT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct FusionExecutorCacheT : public ::flatbuffers::NativeTable {
  typedef FusionExecutorCache TableType;
  std::unique_ptr<nvfuser::serde::InputsIdLookupT> inputs_cache{};
  std::vector<std::unique_ptr<nvfuser::serde::KernelRuntimeStateT>> kernel_runtimes_map{};
  std::vector<uint64_t> kernel_cache_keys{};
  std::vector<uint64_t> kernel_cache_values{};
  FusionExecutorCacheT() = default;
  FusionExecutorCacheT(const FusionExecutorCacheT &o);
  FusionExecutorCacheT(FusionExecutorCacheT&&) FLATBUFFERS_NOEXCEPT = default;
  FusionExecutorCacheT &operator=(FusionExecutorCacheT o) FLATBUFFERS_NOEXCEPT;
};

struct FusionExecutorCache FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionExecutorCacheT NativeTableType;
  typedef FusionExecutorCacheBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INPUTS_CACHE = 4,
    VT_KERNEL_RUNTIMES_MAP = 6,
    VT_KERNEL_CACHE_KEYS = 8,
    VT_KERNEL_CACHE_VALUES = 10
  };
  const nvfuser::serde::InputsIdLookup *inputs_cache() const {
    return GetPointer<const nvfuser::serde::InputsIdLookup *>(VT_INPUTS_CACHE);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>> *kernel_runtimes_map() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>> *>(VT_KERNEL_RUNTIMES_MAP);
  }
  const ::flatbuffers::Vector<uint64_t> *kernel_cache_keys() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_KERNEL_CACHE_KEYS);
  }
  const ::flatbuffers::Vector<uint64_t> *kernel_cache_values() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_KERNEL_CACHE_VALUES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_INPUTS_CACHE) &&
           verifier.VerifyTable(inputs_cache()) &&
           VerifyOffset(verifier, VT_KERNEL_RUNTIMES_MAP) &&
           verifier.VerifyVector(kernel_runtimes_map()) &&
           verifier.VerifyVectorOfTables(kernel_runtimes_map()) &&
           VerifyOffset(verifier, VT_KERNEL_CACHE_KEYS) &&
           verifier.VerifyVector(kernel_cache_keys()) &&
           VerifyOffset(verifier, VT_KERNEL_CACHE_VALUES) &&
           verifier.VerifyVector(kernel_cache_values()) &&
           verifier.EndTable();
  }
  FusionExecutorCacheT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(FusionExecutorCacheT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<FusionExecutorCache> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const FusionExecutorCacheT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct FusionExecutorCacheBuilder {
  typedef FusionExecutorCache Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_inputs_cache(::flatbuffers::Offset<nvfuser::serde::InputsIdLookup> inputs_cache) {
    fbb_.AddOffset(FusionExecutorCache::VT_INPUTS_CACHE, inputs_cache);
  }
  void add_kernel_runtimes_map(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>>> kernel_runtimes_map) {
    fbb_.AddOffset(FusionExecutorCache::VT_KERNEL_RUNTIMES_MAP, kernel_runtimes_map);
  }
  void add_kernel_cache_keys(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_keys) {
    fbb_.AddOffset(FusionExecutorCache::VT_KERNEL_CACHE_KEYS, kernel_cache_keys);
  }
  void add_kernel_cache_values(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_values) {
    fbb_.AddOffset(FusionExecutorCache::VT_KERNEL_CACHE_VALUES, kernel_cache_values);
  }
  explicit FusionExecutorCacheBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionExecutorCache> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionExecutorCache>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionExecutorCache> CreateFusionExecutorCache(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::InputsIdLookup> inputs_cache = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>>> kernel_runtimes_map = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_keys = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_values = 0) {
  FusionExecutorCacheBuilder builder_(_fbb);
  builder_.add_kernel_cache_values(kernel_cache_values);
  builder_.add_kernel_cache_keys(kernel_cache_keys);
  builder_.add_kernel_runtimes_map(kernel_runtimes_map);
  builder_.add_inputs_cache(inputs_cache);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionExecutorCache> CreateFusionExecutorCacheDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::InputsIdLookup> inputs_cache = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>> *kernel_runtimes_map = nullptr,
    const std::vector<uint64_t> *kernel_cache_keys = nullptr,
    const std::vector<uint64_t> *kernel_cache_values = nullptr) {
  auto kernel_runtimes_map__ = kernel_runtimes_map ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>>(*kernel_runtimes_map) : 0;
  auto kernel_cache_keys__ = kernel_cache_keys ? _fbb.CreateVector<uint64_t>(*kernel_cache_keys) : 0;
  auto kernel_cache_values__ = kernel_cache_values ? _fbb.CreateVector<uint64_t>(*kernel_cache_values) : 0;
  return nvfuser::serde::CreateFusionExecutorCache(
      _fbb,
      inputs_cache,
      kernel_runtimes_map__,
      kernel_cache_keys__,
      kernel_cache_values__);
}

::flatbuffers::Offset<FusionExecutorCache> CreateFusionExecutorCache(::flatbuffers::FlatBufferBuilder &_fbb, const FusionExecutorCacheT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct RecordFunctorT : public ::flatbuffers::NativeTable {
  typedef RecordFunctor TableType;
  std::vector<nvfuser::serde::State> args{};
  std::vector<nvfuser::serde::State> outputs{};
  std::string name{};
  nvfuser::serde::RecordType type = nvfuser::serde::RecordType_Base;
  nvfuser::serde::RecordDataUnion data{};
};

struct RecordFunctor FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef RecordFunctorT NativeTableType;
  typedef RecordFunctorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ARGS = 4,
    VT_OUTPUTS = 6,
    VT_NAME = 8,
    VT_TYPE = 10,
    VT_DATA_TYPE = 12,
    VT_DATA = 14
  };
  const ::flatbuffers::Vector<const nvfuser::serde::State *> *args() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::State *> *>(VT_ARGS);
  }
  const ::flatbuffers::Vector<const nvfuser::serde::State *> *outputs() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::State *> *>(VT_OUTPUTS);
  }
  const ::flatbuffers::String *name() const {
    return GetPointer<const ::flatbuffers::String *>(VT_NAME);
  }
  nvfuser::serde::RecordType type() const {
    return static_cast<nvfuser::serde::RecordType>(GetField<int32_t>(VT_TYPE, 0));
  }
  nvfuser::serde::RecordData data_type() const {
    return static_cast<nvfuser::serde::RecordData>(GetField<uint8_t>(VT_DATA_TYPE, 0));
  }
  const void *data() const {
    return GetPointer<const void *>(VT_DATA);
  }
  template<typename T> const T *data_as() const;
  const nvfuser::serde::At *data_as_At() const {
    return data_type() == nvfuser::serde::RecordData_At ? static_cast<const nvfuser::serde::At *>(data()) : nullptr;
  }
  const nvfuser::serde::BatchNorm *data_as_BatchNorm() const {
    return data_type() == nvfuser::serde::RecordData_BatchNorm ? static_cast<const nvfuser::serde::BatchNorm *>(data()) : nullptr;
  }
  const nvfuser::serde::Broadcast *data_as_Broadcast() const {
    return data_type() == nvfuser::serde::RecordData_Broadcast ? static_cast<const nvfuser::serde::Broadcast *>(data()) : nullptr;
  }
  const nvfuser::serde::BroadcastInDim *data_as_BroadcastInDim() const {
    return data_type() == nvfuser::serde::RecordData_BroadcastInDim ? static_cast<const nvfuser::serde::BroadcastInDim *>(data()) : nullptr;
  }
  const nvfuser::serde::Dimension *data_as_Dimension() const {
    return data_type() == nvfuser::serde::RecordData_Dimension ? static_cast<const nvfuser::serde::Dimension *>(data()) : nullptr;
  }
  const nvfuser::serde::Dtype *data_as_Dtype() const {
    return data_type() == nvfuser::serde::RecordData_Dtype ? static_cast<const nvfuser::serde::Dtype *>(data()) : nullptr;
  }
  const nvfuser::serde::Norm *data_as_Norm() const {
    return data_type() == nvfuser::serde::RecordData_Norm ? static_cast<const nvfuser::serde::Norm *>(data()) : nullptr;
  }
  const nvfuser::serde::Output *data_as_Output() const {
    return data_type() == nvfuser::serde::RecordData_Output ? static_cast<const nvfuser::serde::Output *>(data()) : nullptr;
  }
  const nvfuser::serde::Pad *data_as_Pad() const {
    return data_type() == nvfuser::serde::RecordData_Pad ? static_cast<const nvfuser::serde::Pad *>(data()) : nullptr;
  }
  const nvfuser::serde::Permute *data_as_Permute() const {
    return data_type() == nvfuser::serde::RecordData_Permute ? static_cast<const nvfuser::serde::Permute *>(data()) : nullptr;
  }
  const nvfuser::serde::Slice *data_as_Slice() const {
    return data_type() == nvfuser::serde::RecordData_Slice ? static_cast<const nvfuser::serde::Slice *>(data()) : nullptr;
  }
  const nvfuser::serde::Squeeze *data_as_Squeeze() const {
    return data_type() == nvfuser::serde::RecordData_Squeeze ? static_cast<const nvfuser::serde::Squeeze *>(data()) : nullptr;
  }
  const nvfuser::serde::Reduction *data_as_Reduction() const {
    return data_type() == nvfuser::serde::RecordData_Reduction ? static_cast<const nvfuser::serde::Reduction *>(data()) : nullptr;
  }
  const nvfuser::serde::Reshape *data_as_Reshape() const {
    return data_type() == nvfuser::serde::RecordData_Reshape ? static_cast<const nvfuser::serde::Reshape *>(data()) : nullptr;
  }
  const nvfuser::serde::Scalar *data_as_Scalar() const {
    return data_type() == nvfuser::serde::RecordData_Scalar ? static_cast<const nvfuser::serde::Scalar *>(data()) : nullptr;
  }
  const nvfuser::serde::Size *data_as_Size() const {
    return data_type() == nvfuser::serde::RecordData_Size ? static_cast<const nvfuser::serde::Size *>(data()) : nullptr;
  }
  const nvfuser::serde::Tensor *data_as_Tensor() const {
    return data_type() == nvfuser::serde::RecordData_Tensor ? static_cast<const nvfuser::serde::Tensor *>(data()) : nullptr;
  }
  const nvfuser::serde::TensorCreation *data_as_TensorCreation() const {
    return data_type() == nvfuser::serde::RecordData_TensorCreation ? static_cast<const nvfuser::serde::TensorCreation *>(data()) : nullptr;
  }
  const nvfuser::serde::TensorCreationSymbolic *data_as_TensorCreationSymbolic() const {
    return data_type() == nvfuser::serde::RecordData_TensorCreationSymbolic ? static_cast<const nvfuser::serde::TensorCreationSymbolic *>(data()) : nullptr;
  }
  const nvfuser::serde::Vector *data_as_Vector() const {
    return data_type() == nvfuser::serde::RecordData_Vector ? static_cast<const nvfuser::serde::Vector *>(data()) : nullptr;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ARGS) &&
           verifier.VerifyVector(args()) &&
           VerifyOffset(verifier, VT_OUTPUTS) &&
           verifier.VerifyVector(outputs()) &&
           VerifyOffset(verifier, VT_NAME) &&
           verifier.VerifyString(name()) &&
           VerifyField<int32_t>(verifier, VT_TYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_DATA_TYPE, 1) &&
           VerifyOffset(verifier, VT_DATA) &&
           VerifyRecordData(verifier, data(), data_type()) &&
           verifier.EndTable();
  }
  RecordFunctorT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(RecordFunctorT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<RecordFunctor> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const RecordFunctorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

template<> inline const nvfuser::serde::At *RecordFunctor::data_as<nvfuser::serde::At>() const {
  return data_as_At();
}

template<> inline const nvfuser::serde::BatchNorm *RecordFunctor::data_as<nvfuser::serde::BatchNorm>() const {
  return data_as_BatchNorm();
}

template<> inline const nvfuser::serde::Broadcast *RecordFunctor::data_as<nvfuser::serde::Broadcast>() const {
  return data_as_Broadcast();
}

template<> inline const nvfuser::serde::BroadcastInDim *RecordFunctor::data_as<nvfuser::serde::BroadcastInDim>() const {
  return data_as_BroadcastInDim();
}

template<> inline const nvfuser::serde::Dimension *RecordFunctor::data_as<nvfuser::serde::Dimension>() const {
  return data_as_Dimension();
}

template<> inline const nvfuser::serde::Dtype *RecordFunctor::data_as<nvfuser::serde::Dtype>() const {
  return data_as_Dtype();
}

template<> inline const nvfuser::serde::Norm *RecordFunctor::data_as<nvfuser::serde::Norm>() const {
  return data_as_Norm();
}

template<> inline const nvfuser::serde::Output *RecordFunctor::data_as<nvfuser::serde::Output>() const {
  return data_as_Output();
}

template<> inline const nvfuser::serde::Pad *RecordFunctor::data_as<nvfuser::serde::Pad>() const {
  return data_as_Pad();
}

template<> inline const nvfuser::serde::Permute *RecordFunctor::data_as<nvfuser::serde::Permute>() const {
  return data_as_Permute();
}

template<> inline const nvfuser::serde::Slice *RecordFunctor::data_as<nvfuser::serde::Slice>() const {
  return data_as_Slice();
}

template<> inline const nvfuser::serde::Squeeze *RecordFunctor::data_as<nvfuser::serde::Squeeze>() const {
  return data_as_Squeeze();
}

template<> inline const nvfuser::serde::Reduction *RecordFunctor::data_as<nvfuser::serde::Reduction>() const {
  return data_as_Reduction();
}

template<> inline const nvfuser::serde::Reshape *RecordFunctor::data_as<nvfuser::serde::Reshape>() const {
  return data_as_Reshape();
}

template<> inline const nvfuser::serde::Scalar *RecordFunctor::data_as<nvfuser::serde::Scalar>() const {
  return data_as_Scalar();
}

template<> inline const nvfuser::serde::Size *RecordFunctor::data_as<nvfuser::serde::Size>() const {
  return data_as_Size();
}

template<> inline const nvfuser::serde::Tensor *RecordFunctor::data_as<nvfuser::serde::Tensor>() const {
  return data_as_Tensor();
}

template<> inline const nvfuser::serde::TensorCreation *RecordFunctor::data_as<nvfuser::serde::TensorCreation>() const {
  return data_as_TensorCreation();
}

template<> inline const nvfuser::serde::TensorCreationSymbolic *RecordFunctor::data_as<nvfuser::serde::TensorCreationSymbolic>() const {
  return data_as_TensorCreationSymbolic();
}

template<> inline const nvfuser::serde::Vector *RecordFunctor::data_as<nvfuser::serde::Vector>() const {
  return data_as_Vector();
}

struct RecordFunctorBuilder {
  typedef RecordFunctor Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_args(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> args) {
    fbb_.AddOffset(RecordFunctor::VT_ARGS, args);
  }
  void add_outputs(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> outputs) {
    fbb_.AddOffset(RecordFunctor::VT_OUTPUTS, outputs);
  }
  void add_name(::flatbuffers::Offset<::flatbuffers::String> name) {
    fbb_.AddOffset(RecordFunctor::VT_NAME, name);
  }
  void add_type(nvfuser::serde::RecordType type) {
    fbb_.AddElement<int32_t>(RecordFunctor::VT_TYPE, static_cast<int32_t>(type), 0);
  }
  void add_data_type(nvfuser::serde::RecordData data_type) {
    fbb_.AddElement<uint8_t>(RecordFunctor::VT_DATA_TYPE, static_cast<uint8_t>(data_type), 0);
  }
  void add_data(::flatbuffers::Offset<void> data) {
    fbb_.AddOffset(RecordFunctor::VT_DATA, data);
  }
  explicit RecordFunctorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<RecordFunctor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<RecordFunctor>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<RecordFunctor> CreateRecordFunctor(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> args = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> outputs = 0,
    ::flatbuffers::Offset<::flatbuffers::String> name = 0,
    nvfuser::serde::RecordType type = nvfuser::serde::RecordType_Base,
    nvfuser::serde::RecordData data_type = nvfuser::serde::RecordData_NONE,
    ::flatbuffers::Offset<void> data = 0) {
  RecordFunctorBuilder builder_(_fbb);
  builder_.add_data(data);
  builder_.add_type(type);
  builder_.add_name(name);
  builder_.add_outputs(outputs);
  builder_.add_args(args);
  builder_.add_data_type(data_type);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<RecordFunctor> CreateRecordFunctorDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<nvfuser::serde::State> *args = nullptr,
    const std::vector<nvfuser::serde::State> *outputs = nullptr,
    const char *name = nullptr,
    nvfuser::serde::RecordType type = nvfuser::serde::RecordType_Base,
    nvfuser::serde::RecordData data_type = nvfuser::serde::RecordData_NONE,
    ::flatbuffers::Offset<void> data = 0) {
  auto args__ = args ? _fbb.CreateVectorOfStructs<nvfuser::serde::State>(*args) : 0;
  auto outputs__ = outputs ? _fbb.CreateVectorOfStructs<nvfuser::serde::State>(*outputs) : 0;
  auto name__ = name ? _fbb.CreateString(name) : 0;
  return nvfuser::serde::CreateRecordFunctor(
      _fbb,
      args__,
      outputs__,
      name__,
      type,
      data_type,
      data);
}

::flatbuffers::Offset<RecordFunctor> CreateRecordFunctor(::flatbuffers::FlatBufferBuilder &_fbb, const RecordFunctorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct TrieNodeT : public ::flatbuffers::NativeTable {
  typedef TrieNode TableType;
  std::unique_ptr<nvfuser::serde::RecordFunctorT> record{};
  std::vector<uint64_t> children{};
  uint64_t fusion_id = 0;
  uint64_t visits = 0;
  bool is_terminal = false;
  TrieNodeT() = default;
  TrieNodeT(const TrieNodeT &o);
  TrieNodeT(TrieNodeT&&) FLATBUFFERS_NOEXCEPT = default;
  TrieNodeT &operator=(TrieNodeT o) FLATBUFFERS_NOEXCEPT;
};

struct TrieNode FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TrieNodeT NativeTableType;
  typedef TrieNodeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_RECORD = 4,
    VT_CHILDREN = 6,
    VT_FUSION_ID = 8,
    VT_VISITS = 10,
    VT_IS_TERMINAL = 12
  };
  const nvfuser::serde::RecordFunctor *record() const {
    return GetPointer<const nvfuser::serde::RecordFunctor *>(VT_RECORD);
  }
  const ::flatbuffers::Vector<uint64_t> *children() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_CHILDREN);
  }
  uint64_t fusion_id() const {
    return GetField<uint64_t>(VT_FUSION_ID, 0);
  }
  uint64_t visits() const {
    return GetField<uint64_t>(VT_VISITS, 0);
  }
  bool is_terminal() const {
    return GetField<uint8_t>(VT_IS_TERMINAL, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_RECORD) &&
           verifier.VerifyTable(record()) &&
           VerifyOffset(verifier, VT_CHILDREN) &&
           verifier.VerifyVector(children()) &&
           VerifyField<uint64_t>(verifier, VT_FUSION_ID, 8) &&
           VerifyField<uint64_t>(verifier, VT_VISITS, 8) &&
           VerifyField<uint8_t>(verifier, VT_IS_TERMINAL, 1) &&
           verifier.EndTable();
  }
  TrieNodeT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(TrieNodeT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<TrieNode> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TrieNodeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct TrieNodeBuilder {
  typedef TrieNode Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_record(::flatbuffers::Offset<nvfuser::serde::RecordFunctor> record) {
    fbb_.AddOffset(TrieNode::VT_RECORD, record);
  }
  void add_children(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> children) {
    fbb_.AddOffset(TrieNode::VT_CHILDREN, children);
  }
  void add_fusion_id(uint64_t fusion_id) {
    fbb_.AddElement<uint64_t>(TrieNode::VT_FUSION_ID, fusion_id, 0);
  }
  void add_visits(uint64_t visits) {
    fbb_.AddElement<uint64_t>(TrieNode::VT_VISITS, visits, 0);
  }
  void add_is_terminal(bool is_terminal) {
    fbb_.AddElement<uint8_t>(TrieNode::VT_IS_TERMINAL, static_cast<uint8_t>(is_terminal), 0);
  }
  explicit TrieNodeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TrieNode> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TrieNode>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TrieNode> CreateTrieNode(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::RecordFunctor> record = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> children = 0,
    uint64_t fusion_id = 0,
    uint64_t visits = 0,
    bool is_terminal = false) {
  TrieNodeBuilder builder_(_fbb);
  builder_.add_visits(visits);
  builder_.add_fusion_id(fusion_id);
  builder_.add_children(children);
  builder_.add_record(record);
  builder_.add_is_terminal(is_terminal);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TrieNode> CreateTrieNodeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::RecordFunctor> record = 0,
    const std::vector<uint64_t> *children = nullptr,
    uint64_t fusion_id = 0,
    uint64_t visits = 0,
    bool is_terminal = false) {
  auto children__ = children ? _fbb.CreateVector<uint64_t>(*children) : 0;
  return nvfuser::serde::CreateTrieNode(
      _fbb,
      record,
      children__,
      fusion_id,
      visits,
      is_terminal);
}

::flatbuffers::Offset<TrieNode> CreateTrieNode(::flatbuffers::FlatBufferBuilder &_fbb, const TrieNodeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct FusionCacheT : public ::flatbuffers::NativeTable {
  typedef FusionCache TableType;
  uint64_t max_fusions = 0;
  std::vector<std::unique_ptr<nvfuser::serde::TrieNodeT>> structure{};
  std::vector<uint64_t> terminal_nodes{};
  std::vector<std::unique_ptr<nvfuser::serde::FusionExecutorCacheT>> auto_gen_schedules{};
  FusionCacheT() = default;
  FusionCacheT(const FusionCacheT &o);
  FusionCacheT(FusionCacheT&&) FLATBUFFERS_NOEXCEPT = default;
  FusionCacheT &operator=(FusionCacheT o) FLATBUFFERS_NOEXCEPT;
};

struct FusionCache FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionCacheT NativeTableType;
  typedef FusionCacheBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_MAX_FUSIONS = 4,
    VT_STRUCTURE = 6,
    VT_TERMINAL_NODES = 8,
    VT_AUTO_GEN_SCHEDULES = 10
  };
  uint64_t max_fusions() const {
    return GetField<uint64_t>(VT_MAX_FUSIONS, 0);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>> *structure() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>> *>(VT_STRUCTURE);
  }
  const ::flatbuffers::Vector<uint64_t> *terminal_nodes() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_TERMINAL_NODES);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>> *auto_gen_schedules() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>> *>(VT_AUTO_GEN_SCHEDULES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_MAX_FUSIONS, 8) &&
           VerifyOffset(verifier, VT_STRUCTURE) &&
           verifier.VerifyVector(structure()) &&
           verifier.VerifyVectorOfTables(structure()) &&
           VerifyOffset(verifier, VT_TERMINAL_NODES) &&
           verifier.VerifyVector(terminal_nodes()) &&
           VerifyOffset(verifier, VT_AUTO_GEN_SCHEDULES) &&
           verifier.VerifyVector(auto_gen_schedules()) &&
           verifier.VerifyVectorOfTables(auto_gen_schedules()) &&
           verifier.EndTable();
  }
  FusionCacheT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(FusionCacheT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<FusionCache> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const FusionCacheT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct FusionCacheBuilder {
  typedef FusionCache Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_max_fusions(uint64_t max_fusions) {
    fbb_.AddElement<uint64_t>(FusionCache::VT_MAX_FUSIONS, max_fusions, 0);
  }
  void add_structure(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>>> structure) {
    fbb_.AddOffset(FusionCache::VT_STRUCTURE, structure);
  }
  void add_terminal_nodes(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> terminal_nodes) {
    fbb_.AddOffset(FusionCache::VT_TERMINAL_NODES, terminal_nodes);
  }
  void add_auto_gen_schedules(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>>> auto_gen_schedules) {
    fbb_.AddOffset(FusionCache::VT_AUTO_GEN_SCHEDULES, auto_gen_schedules);
  }
  explicit FusionCacheBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionCache> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionCache>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionCache> CreateFusionCache(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_fusions = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>>> structure = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> terminal_nodes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>>> auto_gen_schedules = 0) {
  FusionCacheBuilder builder_(_fbb);
  builder_.add_max_fusions(max_fusions);
  builder_.add_auto_gen_schedules(auto_gen_schedules);
  builder_.add_terminal_nodes(terminal_nodes);
  builder_.add_structure(structure);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionCache> CreateFusionCacheDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_fusions = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>> *structure = nullptr,
    const std::vector<uint64_t> *terminal_nodes = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>> *auto_gen_schedules = nullptr) {
  auto structure__ = structure ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::TrieNode>>(*structure) : 0;
  auto terminal_nodes__ = terminal_nodes ? _fbb.CreateVector<uint64_t>(*terminal_nodes) : 0;
  auto auto_gen_schedules__ = auto_gen_schedules ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>>(*auto_gen_schedules) : 0;
  return nvfuser::serde::CreateFusionCache(
      _fbb,
      max_fusions,
      structure__,
      terminal_nodes__,
      auto_gen_schedules__);
}

::flatbuffers::Offset<FusionCache> CreateFusionCache(::flatbuffers::FlatBufferBuilder &_fbb, const FusionCacheT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

inline ScalarT *Scalar::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<ScalarT>(new ScalarT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Scalar::UnPackTo(ScalarT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = dtype(); _o->dtype = _e; }
  { auto _e = has_value(); _o->has_value = _e; }
  { auto _e = value_type(); _o->value_type = _e; }
  { auto _e = bool_value(); _o->bool_value = _e; }
  { auto _e = long_value(); _o->long_value = _e; }
  { auto _e = double_value(); _o->double_value = _e; }
  { auto _e = real_value(); _o->real_value = _e; }
  { auto _e = imag_value(); _o->imag_value = _e; }
}

inline ::flatbuffers::Offset<Scalar> Scalar::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ScalarT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateScalar(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Scalar> CreateScalar(::flatbuffers::FlatBufferBuilder &_fbb, const ScalarT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const ScalarT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _dtype = _o->dtype;
  auto _has_value = _o->has_value;
  auto _value_type = _o->value_type;
  auto _bool_value = _o->bool_value;
  auto _long_value = _o->long_value;
  auto _double_value = _o->double_value;
  auto _real_value = _o->real_value;
  auto _imag_value = _o->imag_value;
  return nvfuser::serde::CreateScalar(
      _fbb,
      _dtype,
      _has_value,
      _value_type,
      _bool_value,
      _long_value,
      _double_value,
      _real_value,
      _imag_value);
}

inline BinaryOpT *BinaryOp::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<BinaryOpT>(new BinaryOpT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void BinaryOp::UnPackTo(BinaryOpT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = binary_type(); _o->binary_type = _e; }
  { auto _e = src0(); _o->src0 = _e; }
  { auto _e = src1(); _o->src1 = _e; }
  { auto _e = out(); _o->out = _e; }
  { auto _e = name(); if (_e) _o->name = _e->str(); }
}

inline ::flatbuffers::Offset<BinaryOp> BinaryOp::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const BinaryOpT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateBinaryOp(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<BinaryOp> CreateBinaryOp(::flatbuffers::FlatBufferBuilder &_fbb, const BinaryOpT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const BinaryOpT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _binary_type = _o->binary_type;
  auto _src0 = _o->src0;
  auto _src1 = _o->src1;
  auto _out = _o->out;
  auto _name = _o->name.empty() ? 0 : _fbb.CreateString(_o->name);
  return nvfuser::serde::CreateBinaryOp(
      _fbb,
      _binary_type,
      _src0,
      _src1,
      _out,
      _name);
}

inline GetAttrT *GetAttr::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<GetAttrT>(new GetAttrT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void GetAttr::UnPackTo(GetAttrT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = struct_(); _o->struct_ = _e; }
  { auto _e = attr(); if (_e) _o->attr = _e->str(); }
  { auto _e = out(); _o->out = _e; }
}

inline ::flatbuffers::Offset<GetAttr> GetAttr::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const GetAttrT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateGetAttr(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<GetAttr> CreateGetAttr(::flatbuffers::FlatBufferBuilder &_fbb, const GetAttrT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const GetAttrT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _struct_ = _o->struct_;
  auto _attr = _o->attr.empty() ? 0 : _fbb.CreateString(_o->attr);
  auto _out = _o->out;
  return nvfuser::serde::CreateGetAttr(
      _fbb,
      _struct_,
      _attr,
      _out);
}

inline GetItemT *GetItem::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<GetItemT>(new GetItemT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void GetItem::UnPackTo(GetItemT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = array(); _o->array = _e; }
  { auto _e = index(); _o->index = _e; }
  { auto _e = out(); _o->out = _e; }
}

inline ::flatbuffers::Offset<GetItem> GetItem::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const GetItemT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateGetItem(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<GetItem> CreateGetItem(::flatbuffers::FlatBufferBuilder &_fbb, const GetItemT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const GetItemT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _array = _o->array;
  auto _index = _o->index;
  auto _out = _o->out;
  return nvfuser::serde::CreateGetItem(
      _fbb,
      _array,
      _index,
      _out);
}

inline GetMetaDataT *GetMetaData::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<GetMetaDataT>(new GetMetaDataT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void GetMetaData::UnPackTo(GetMetaDataT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = in(); _o->in = _e; }
  { auto _e = out(); _o->out = _e; }
}

inline ::flatbuffers::Offset<GetMetaData> GetMetaData::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const GetMetaDataT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateGetMetaData(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<GetMetaData> CreateGetMetaData(::flatbuffers::FlatBufferBuilder &_fbb, const GetMetaDataT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const GetMetaDataT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _in = _o->in;
  auto _out = _o->out;
  return nvfuser::serde::CreateGetMetaData(
      _fbb,
      _in,
      _out);
}

inline MergeT *Merge::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<MergeT>(new MergeT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Merge::UnPackTo(MergeT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = inner(); _o->inner = _e; }
  { auto _e = outer(); _o->outer = _e; }
  { auto _e = out(); _o->out = _e; }
}

inline ::flatbuffers::Offset<Merge> Merge::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const MergeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateMerge(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Merge> CreateMerge(::flatbuffers::FlatBufferBuilder &_fbb, const MergeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const MergeT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _inner = _o->inner;
  auto _outer = _o->outer;
  auto _out = _o->out;
  return nvfuser::serde::CreateMerge(
      _fbb,
      _inner,
      _outer,
      _out);
}

inline NamedScalarT *NamedScalar::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<NamedScalarT>(new NamedScalarT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void NamedScalar::UnPackTo(NamedScalarT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = name(); if (_e) _o->name = _e->str(); }
}

inline ::flatbuffers::Offset<NamedScalar> NamedScalar::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const NamedScalarT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateNamedScalar(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<NamedScalar> CreateNamedScalar(::flatbuffers::FlatBufferBuilder &_fbb, const NamedScalarT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const NamedScalarT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _name = _o->name.empty() ? 0 : _fbb.CreateString(_o->name);
  return nvfuser::serde::CreateNamedScalar(
      _fbb,
      _name);
}

inline ResizeT *Resize::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<ResizeT>(new ResizeT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Resize::UnPackTo(ResizeT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = in(); _o->in = _e; }
  { auto _e = left_expansion(); _o->left_expansion = _e; }
  { auto _e = right_expansion(); _o->right_expansion = _e; }
  { auto _e = out(); _o->out = _e; }
}

inline ::flatbuffers::Offset<Resize> Resize::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ResizeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateResize(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Resize> CreateResize(::flatbuffers::FlatBufferBuilder &_fbb, const ResizeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const ResizeT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _in = _o->in;
  auto _left_expansion = _o->left_expansion;
  auto _right_expansion = _o->right_expansion;
  auto _out = _o->out;
  return nvfuser::serde::CreateResize(
      _fbb,
      _in,
      _left_expansion,
      _right_expansion,
      _out);
}

inline SplitT *Split::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<SplitT>(new SplitT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Split::UnPackTo(SplitT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = in(); _o->in = _e; }
  { auto _e = factor(); _o->factor = _e; }
  { auto _e = inner(); _o->inner = _e; }
  { auto _e = outer(); _o->outer = _e; }
  { auto _e = inner_split(); _o->inner_split = _e; }
  { auto _e = trim_out_of_bounds(); _o->trim_out_of_bounds = _e; }
}

inline ::flatbuffers::Offset<Split> Split::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const SplitT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateSplit(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Split> CreateSplit(::flatbuffers::FlatBufferBuilder &_fbb, const SplitT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const SplitT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _in = _o->in;
  auto _factor = _o->factor;
  auto _inner = _o->inner;
  auto _outer = _o->outer;
  auto _inner_split = _o->inner_split;
  auto _trim_out_of_bounds = _o->trim_out_of_bounds;
  return nvfuser::serde::CreateSplit(
      _fbb,
      _in,
      _factor,
      _inner,
      _outer,
      _inner_split,
      _trim_out_of_bounds);
}

inline Swizzle2DT *Swizzle2D::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<Swizzle2DT>(new Swizzle2DT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Swizzle2D::UnPackTo(Swizzle2DT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = in_x(); _o->in_x = _e; }
  { auto _e = in_y(); _o->in_y = _e; }
  { auto _e = swizzle_type(); _o->swizzle_type = _e; }
  { auto _e = swizzle_mode(); _o->swizzle_mode = _e; }
  { auto _e = out_x(); _o->out_x = _e; }
  { auto _e = out_y(); _o->out_y = _e; }
}

inline ::flatbuffers::Offset<Swizzle2D> Swizzle2D::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const Swizzle2DT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateSwizzle2D(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Swizzle2D> CreateSwizzle2D(::flatbuffers::FlatBufferBuilder &_fbb, const Swizzle2DT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const Swizzle2DT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _in_x = _o->in_x;
  auto _in_y = _o->in_y;
  auto _swizzle_type = _o->swizzle_type;
  auto _swizzle_mode = _o->swizzle_mode;
  auto _out_x = _o->out_x;
  auto _out_y = _o->out_y;
  return nvfuser::serde::CreateSwizzle2D(
      _fbb,
      _in_x,
      _in_y,
      _swizzle_type,
      _swizzle_mode,
      _out_x,
      _out_y);
}

inline SymbolicT *Symbolic::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<SymbolicT>(new SymbolicT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Symbolic::UnPackTo(SymbolicT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = src0(); _o->src0 = _e; }
  { auto _e = name(); if (_e) _o->name = _e->str(); }
}

inline ::flatbuffers::Offset<Symbolic> Symbolic::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const SymbolicT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateSymbolic(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Symbolic> CreateSymbolic(::flatbuffers::FlatBufferBuilder &_fbb, const SymbolicT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const SymbolicT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _src0 = _o->src0;
  auto _name = _o->name.empty() ? 0 : _fbb.CreateString(_o->name);
  return nvfuser::serde::CreateSymbolic(
      _fbb,
      _src0,
      _name);
}

inline UnaryOpT *UnaryOp::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<UnaryOpT>(new UnaryOpT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void UnaryOp::UnPackTo(UnaryOpT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = unary_type(); _o->unary_type = _e; }
  { auto _e = data_type(); _o->data_type = _e; }
  { auto _e = src0(); _o->src0 = _e; }
  { auto _e = out(); _o->out = _e; }
  { auto _e = name(); if (_e) _o->name = _e->str(); }
}

inline ::flatbuffers::Offset<UnaryOp> UnaryOp::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const UnaryOpT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateUnaryOp(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<UnaryOp> CreateUnaryOp(::flatbuffers::FlatBufferBuilder &_fbb, const UnaryOpT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const UnaryOpT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _unary_type = _o->unary_type;
  auto _data_type = _o->data_type;
  auto _src0 = _o->src0;
  auto _out = _o->out;
  auto _name = _o->name.empty() ? 0 : _fbb.CreateString(_o->name);
  return nvfuser::serde::CreateUnaryOp(
      _fbb,
      _unary_type,
      _data_type,
      _src0,
      _out,
      _name);
}

inline InstructionT *Instruction::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<InstructionT>(new InstructionT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Instruction::UnPackTo(InstructionT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = data_type(); _o->data.type = _e; }
  { auto _e = data(); if (_e) _o->data.value = nvfuser::serde::InstructionDataUnion::UnPack(_e, data_type(), _resolver); }
}

inline ::flatbuffers::Offset<Instruction> Instruction::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const InstructionT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateInstruction(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Instruction> CreateInstruction(::flatbuffers::FlatBufferBuilder &_fbb, const InstructionT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const InstructionT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _data_type = _o->data.type;
  auto _data = _o->data.Pack(_fbb);
  return nvfuser::serde::CreateInstruction(
      _fbb,
      _data_type,
      _data);
}

inline NaiveValueGeneratorT::NaiveValueGeneratorT(const NaiveValueGeneratorT &o) {
  instructions.reserve(o.instructions.size());
  for (const auto &instructions_ : o.instructions) { instructions.emplace_back((instructions_) ? new nvfuser::serde::InstructionT(*instructions_) : nullptr); }
}

inline NaiveValueGeneratorT &NaiveValueGeneratorT::operator=(NaiveValueGeneratorT o) FLATBUFFERS_NOEXCEPT {
  std::swap(instructions, o.instructions);
  return *this;
}

inline NaiveValueGeneratorT *NaiveValueGenerator::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<NaiveValueGeneratorT>(new NaiveValueGeneratorT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void NaiveValueGenerator::UnPackTo(NaiveValueGeneratorT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = instructions(); if (_e) { _o->instructions.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->instructions[_i]) { _e->Get(_i)->UnPackTo(_o->instructions[_i].get(), _resolver); } else { _o->instructions[_i] = std::unique_ptr<nvfuser::serde::InstructionT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->instructions.resize(0); } }
}

inline ::flatbuffers::Offset<NaiveValueGenerator> NaiveValueGenerator::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const NaiveValueGeneratorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateNaiveValueGenerator(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<NaiveValueGenerator> CreateNaiveValueGenerator(::flatbuffers::FlatBufferBuilder &_fbb, const NaiveValueGeneratorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const NaiveValueGeneratorT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _instructions = _o->instructions.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::Instruction>> (_o->instructions.size(), [](size_t i, _VectorArgs *__va) { return CreateInstruction(*__va->__fbb, __va->__o->instructions[i].get(), __va->__rehasher); }, &_va ) : 0;
  return nvfuser::serde::CreateNaiveValueGenerator(
      _fbb,
      _instructions);
}

inline IterationDomainT *IterationDomain::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<IterationDomainT>(new IterationDomainT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void IterationDomain::UnPackTo(IterationDomainT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = extent(); _o->extent = _e; }
}

inline ::flatbuffers::Offset<IterationDomain> IterationDomain::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const IterationDomainT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateIterationDomain(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<IterationDomain> CreateIterationDomain(::flatbuffers::FlatBufferBuilder &_fbb, const IterationDomainT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const IterationDomainT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _extent = _o->extent;
  return nvfuser::serde::CreateIterationDomain(
      _fbb,
      _extent);
}

inline DomainT::DomainT(const DomainT &o) {
  dims.reserve(o.dims.size());
  for (const auto &dims_ : o.dims) { dims.emplace_back((dims_) ? new nvfuser::serde::IterationDomainT(*dims_) : nullptr); }
}

inline DomainT &DomainT::operator=(DomainT o) FLATBUFFERS_NOEXCEPT {
  std::swap(dims, o.dims);
  return *this;
}

inline DomainT *Domain::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<DomainT>(new DomainT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Domain::UnPackTo(DomainT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = dims(); if (_e) { _o->dims.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->dims[_i]) { _e->Get(_i)->UnPackTo(_o->dims[_i].get(), _resolver); } else { _o->dims[_i] = std::unique_ptr<nvfuser::serde::IterationDomainT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->dims.resize(0); } }
}

inline ::flatbuffers::Offset<Domain> Domain::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const DomainT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateDomain(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Domain> CreateDomain(::flatbuffers::FlatBufferBuilder &_fbb, const DomainT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const DomainT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _dims = _o->dims.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::IterationDomain>> (_o->dims.size(), [](size_t i, _VectorArgs *__va) { return CreateIterationDomain(*__va->__fbb, __va->__o->dims[i].get(), __va->__rehasher); }, &_va ) : 0;
  return nvfuser::serde::CreateDomain(
      _fbb,
      _dims);
}

inline SymbolicTensorT::SymbolicTensorT(const SymbolicTensorT &o)
      : dtype(o.dtype),
        root((o.root) ? new nvfuser::serde::DomainT(*o.root) : nullptr),
        rfactor((o.rfactor) ? new nvfuser::serde::DomainT(*o.rfactor) : nullptr),
        allocate((o.allocate) ? new nvfuser::serde::DomainT(*o.allocate) : nullptr),
        leaf((o.leaf) ? new nvfuser::serde::DomainT(*o.leaf) : nullptr) {
}

inline SymbolicTensorT &SymbolicTensorT::operator=(SymbolicTensorT o) FLATBUFFERS_NOEXCEPT {
  std::swap(dtype, o.dtype);
  std::swap(root, o.root);
  std::swap(rfactor, o.rfactor);
  std::swap(allocate, o.allocate);
  std::swap(leaf, o.leaf);
  return *this;
}

inline SymbolicTensorT *SymbolicTensor::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<SymbolicTensorT>(new SymbolicTensorT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void SymbolicTensor::UnPackTo(SymbolicTensorT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = dtype(); _o->dtype = _e; }
  { auto _e = root(); if (_e) { if(_o->root) { _e->UnPackTo(_o->root.get(), _resolver); } else { _o->root = std::unique_ptr<nvfuser::serde::DomainT>(_e->UnPack(_resolver)); } } else if (_o->root) { _o->root.reset(); } }
  { auto _e = rfactor(); if (_e) { if(_o->rfactor) { _e->UnPackTo(_o->rfactor.get(), _resolver); } else { _o->rfactor = std::unique_ptr<nvfuser::serde::DomainT>(_e->UnPack(_resolver)); } } else if (_o->rfactor) { _o->rfactor.reset(); } }
  { auto _e = allocate(); if (_e) { if(_o->allocate) { _e->UnPackTo(_o->allocate.get(), _resolver); } else { _o->allocate = std::unique_ptr<nvfuser::serde::DomainT>(_e->UnPack(_resolver)); } } else if (_o->allocate) { _o->allocate.reset(); } }
  { auto _e = leaf(); if (_e) { if(_o->leaf) { _e->UnPackTo(_o->leaf.get(), _resolver); } else { _o->leaf = std::unique_ptr<nvfuser::serde::DomainT>(_e->UnPack(_resolver)); } } else if (_o->leaf) { _o->leaf.reset(); } }
}

inline ::flatbuffers::Offset<SymbolicTensor> SymbolicTensor::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const SymbolicTensorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateSymbolicTensor(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<SymbolicTensor> CreateSymbolicTensor(::flatbuffers::FlatBufferBuilder &_fbb, const SymbolicTensorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const SymbolicTensorT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _dtype = _o->dtype;
  auto _root = _o->root ? CreateDomain(_fbb, _o->root.get(), _rehasher) : 0;
  auto _rfactor = _o->rfactor ? CreateDomain(_fbb, _o->rfactor.get(), _rehasher) : 0;
  auto _allocate = _o->allocate ? CreateDomain(_fbb, _o->allocate.get(), _rehasher) : 0;
  auto _leaf = _o->leaf ? CreateDomain(_fbb, _o->leaf.get(), _rehasher) : 0;
  return nvfuser::serde::CreateSymbolicTensor(
      _fbb,
      _dtype,
      _root,
      _rfactor,
      _allocate,
      _leaf);
}

inline AllocateBufferT::AllocateBufferT(const AllocateBufferT &o)
      : tv((o.tv) ? new nvfuser::serde::SymbolicTensorT(*o.tv) : nullptr),
        shape(o.shape),
        zero_init(o.zero_init) {
}

inline AllocateBufferT &AllocateBufferT::operator=(AllocateBufferT o) FLATBUFFERS_NOEXCEPT {
  std::swap(tv, o.tv);
  std::swap(shape, o.shape);
  std::swap(zero_init, o.zero_init);
  return *this;
}

inline AllocateBufferT *AllocateBuffer::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<AllocateBufferT>(new AllocateBufferT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void AllocateBuffer::UnPackTo(AllocateBufferT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = tv(); if (_e) { if(_o->tv) { _e->UnPackTo(_o->tv.get(), _resolver); } else { _o->tv = std::unique_ptr<nvfuser::serde::SymbolicTensorT>(_e->UnPack(_resolver)); } } else if (_o->tv) { _o->tv.reset(); } }
  { auto _e = shape(); if (_e) { _o->shape.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->shape[_i] = _e->Get(_i); } } else { _o->shape.resize(0); } }
  { auto _e = zero_init(); _o->zero_init = _e; }
}

inline ::flatbuffers::Offset<AllocateBuffer> AllocateBuffer::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const AllocateBufferT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateAllocateBuffer(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<AllocateBuffer> CreateAllocateBuffer(::flatbuffers::FlatBufferBuilder &_fbb, const AllocateBufferT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const AllocateBufferT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _tv = _o->tv ? CreateSymbolicTensor(_fbb, _o->tv.get(), _rehasher) : 0;
  auto _shape = _o->shape.size() ? _fbb.CreateVector(_o->shape) : 0;
  auto _zero_init = _o->zero_init;
  return nvfuser::serde::CreateAllocateBuffer(
      _fbb,
      _tv,
      _shape,
      _zero_init);
}

inline ScalarCpuT::ScalarCpuT(const ScalarCpuT &o)
      : scalar_value((o.scalar_value) ? new nvfuser::serde::ScalarT(*o.scalar_value) : nullptr) {
}

inline ScalarCpuT &ScalarCpuT::operator=(ScalarCpuT o) FLATBUFFERS_NOEXCEPT {
  std::swap(scalar_value, o.scalar_value);
  return *this;
}

inline ScalarCpuT *ScalarCpu::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<ScalarCpuT>(new ScalarCpuT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void ScalarCpu::UnPackTo(ScalarCpuT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = scalar_value(); if (_e) { if(_o->scalar_value) { _e->UnPackTo(_o->scalar_value.get(), _resolver); } else { _o->scalar_value = std::unique_ptr<nvfuser::serde::ScalarT>(_e->UnPack(_resolver)); } } else if (_o->scalar_value) { _o->scalar_value.reset(); } }
}

inline ::flatbuffers::Offset<ScalarCpu> ScalarCpu::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ScalarCpuT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateScalarCpu(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<ScalarCpu> CreateScalarCpu(::flatbuffers::FlatBufferBuilder &_fbb, const ScalarCpuT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const ScalarCpuT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _scalar_value = _o->scalar_value ? CreateScalar(_fbb, _o->scalar_value.get(), _rehasher) : 0;
  return nvfuser::serde::CreateScalarCpu(
      _fbb,
      _scalar_value);
}

inline TensorArgT *TensorArg::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<TensorArgT>(new TensorArgT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void TensorArg::UnPackTo(TensorArgT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = ptr(); _o->ptr = _e; }
  { auto _e = sizes(); if (_e) { _o->sizes.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->sizes[_i] = _e->Get(_i); } } else { _o->sizes.resize(0); } }
  { auto _e = strides(); if (_e) { _o->strides.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->strides[_i] = _e->Get(_i); } } else { _o->strides.resize(0); } }
  { auto _e = dtype(); _o->dtype = _e; }
}

inline ::flatbuffers::Offset<TensorArg> TensorArg::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorArgT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateTensorArg(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<TensorArg> CreateTensorArg(::flatbuffers::FlatBufferBuilder &_fbb, const TensorArgT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const TensorArgT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _ptr = _o->ptr;
  auto _sizes = _o->sizes.size() ? _fbb.CreateVector(_o->sizes) : 0;
  auto _strides = _o->strides.size() ? _fbb.CreateVector(_o->strides) : 0;
  auto _dtype = _o->dtype;
  return nvfuser::serde::CreateTensorArg(
      _fbb,
      _ptr,
      _sizes,
      _strides,
      _dtype);
}

inline PolymorphicValueT *PolymorphicValue::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<PolymorphicValueT>(new PolymorphicValueT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void PolymorphicValue::UnPackTo(PolymorphicValueT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = data_type(); _o->data.type = _e; }
  { auto _e = data(); if (_e) _o->data.value = nvfuser::serde::PolymorphicValueDataUnion::UnPack(_e, data_type(), _resolver); }
}

inline ::flatbuffers::Offset<PolymorphicValue> PolymorphicValue::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const PolymorphicValueT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreatePolymorphicValue(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<PolymorphicValue> CreatePolymorphicValue(::flatbuffers::FlatBufferBuilder &_fbb, const PolymorphicValueT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const PolymorphicValueT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _data_type = _o->data.type;
  auto _data = _o->data.Pack(_fbb);
  return nvfuser::serde::CreatePolymorphicValue(
      _fbb,
      _data_type,
      _data);
}

inline KernelArgumentHolderT::KernelArgumentHolderT(const KernelArgumentHolderT &o)
      : device_index(o.device_index),
        cache_id(o.cache_id) {
  arguments.reserve(o.arguments.size());
  for (const auto &arguments_ : o.arguments) { arguments.emplace_back((arguments_) ? new nvfuser::serde::PolymorphicValueT(*arguments_) : nullptr); }
}

inline KernelArgumentHolderT &KernelArgumentHolderT::operator=(KernelArgumentHolderT o) FLATBUFFERS_NOEXCEPT {
  std::swap(arguments, o.arguments);
  std::swap(device_index, o.device_index);
  std::swap(cache_id, o.cache_id);
  return *this;
}

inline KernelArgumentHolderT *KernelArgumentHolder::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<KernelArgumentHolderT>(new KernelArgumentHolderT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void KernelArgumentHolder::UnPackTo(KernelArgumentHolderT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = arguments(); if (_e) { _o->arguments.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->arguments[_i]) { _e->Get(_i)->UnPackTo(_o->arguments[_i].get(), _resolver); } else { _o->arguments[_i] = std::unique_ptr<nvfuser::serde::PolymorphicValueT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->arguments.resize(0); } }
  { auto _e = device_index(); _o->device_index = _e; }
  { auto _e = cache_id(); _o->cache_id = _e; }
}

inline ::flatbuffers::Offset<KernelArgumentHolder> KernelArgumentHolder::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const KernelArgumentHolderT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateKernelArgumentHolder(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<KernelArgumentHolder> CreateKernelArgumentHolder(::flatbuffers::FlatBufferBuilder &_fbb, const KernelArgumentHolderT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const KernelArgumentHolderT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _arguments = _o->arguments.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>> (_o->arguments.size(), [](size_t i, _VectorArgs *__va) { return CreatePolymorphicValue(*__va->__fbb, __va->__o->arguments[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _device_index = _o->device_index;
  auto _cache_id = _o->cache_id;
  return nvfuser::serde::CreateKernelArgumentHolder(
      _fbb,
      _arguments,
      _device_index,
      _cache_id);
}

inline TensorShapeT *TensorShape::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<TensorShapeT>(new TensorShapeT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void TensorShape::UnPackTo(TensorShapeT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = shape(); if (_e) { _o->shape.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->shape[_i] = _e->Get(_i); } } else { _o->shape.resize(0); } }
}

inline ::flatbuffers::Offset<TensorShape> TensorShape::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorShapeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateTensorShape(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<TensorShape> CreateTensorShape(::flatbuffers::FlatBufferBuilder &_fbb, const TensorShapeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const TensorShapeT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _shape = _o->shape.size() ? _fbb.CreateVector(_o->shape) : 0;
  return nvfuser::serde::CreateTensorShape(
      _fbb,
      _shape);
}

inline LaunchParamsT::LaunchParamsT(const LaunchParamsT &o)
      : gdimx(o.gdimx),
        gdimy(o.gdimy),
        gdimz(o.gdimz),
        bdimx(o.bdimx),
        bdimy(o.bdimy),
        bdimz(o.bdimz),
        smem(o.smem) {
  output_sizes.reserve(o.output_sizes.size());
  for (const auto &output_sizes_ : o.output_sizes) { output_sizes.emplace_back((output_sizes_) ? new nvfuser::serde::TensorShapeT(*output_sizes_) : nullptr); }
}

inline LaunchParamsT &LaunchParamsT::operator=(LaunchParamsT o) FLATBUFFERS_NOEXCEPT {
  std::swap(gdimx, o.gdimx);
  std::swap(gdimy, o.gdimy);
  std::swap(gdimz, o.gdimz);
  std::swap(bdimx, o.bdimx);
  std::swap(bdimy, o.bdimy);
  std::swap(bdimz, o.bdimz);
  std::swap(smem, o.smem);
  std::swap(output_sizes, o.output_sizes);
  return *this;
}

inline LaunchParamsT *LaunchParams::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<LaunchParamsT>(new LaunchParamsT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void LaunchParams::UnPackTo(LaunchParamsT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = gdimx(); _o->gdimx = _e; }
  { auto _e = gdimy(); _o->gdimy = _e; }
  { auto _e = gdimz(); _o->gdimz = _e; }
  { auto _e = bdimx(); _o->bdimx = _e; }
  { auto _e = bdimy(); _o->bdimy = _e; }
  { auto _e = bdimz(); _o->bdimz = _e; }
  { auto _e = smem(); _o->smem = _e; }
  { auto _e = output_sizes(); if (_e) { _o->output_sizes.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->output_sizes[_i]) { _e->Get(_i)->UnPackTo(_o->output_sizes[_i].get(), _resolver); } else { _o->output_sizes[_i] = std::unique_ptr<nvfuser::serde::TensorShapeT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->output_sizes.resize(0); } }
}

inline ::flatbuffers::Offset<LaunchParams> LaunchParams::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const LaunchParamsT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateLaunchParams(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<LaunchParams> CreateLaunchParams(::flatbuffers::FlatBufferBuilder &_fbb, const LaunchParamsT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const LaunchParamsT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _gdimx = _o->gdimx;
  auto _gdimy = _o->gdimy;
  auto _gdimz = _o->gdimz;
  auto _bdimx = _o->bdimx;
  auto _bdimy = _o->bdimy;
  auto _bdimz = _o->bdimz;
  auto _smem = _o->smem;
  auto _output_sizes = _o->output_sizes.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::TensorShape>> (_o->output_sizes.size(), [](size_t i, _VectorArgs *__va) { return CreateTensorShape(*__va->__fbb, __va->__o->output_sizes[i].get(), __va->__rehasher); }, &_va ) : 0;
  return nvfuser::serde::CreateLaunchParams(
      _fbb,
      _gdimx,
      _gdimy,
      _gdimz,
      _bdimx,
      _bdimy,
      _bdimz,
      _smem,
      _output_sizes);
}

inline GlobalBufferInfoT *GlobalBufferInfo::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<GlobalBufferInfoT>(new GlobalBufferInfoT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void GlobalBufferInfo::UnPackTo(GlobalBufferInfoT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = tv(); _o->tv = _e; }
  { auto _e = sizes(); if (_e) { _o->sizes.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->sizes[_i] = _e->Get(_i); } } else { _o->sizes.resize(0); } }
  { auto _e = strides(); if (_e) { _o->strides.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->strides[_i] = _e->Get(_i); } } else { _o->strides.resize(0); } }
  { auto _e = dtype(); _o->dtype = _e; }
  { auto _e = zero_init(); _o->zero_init = _e; }
  { auto _e = is_profile_buffer(); _o->is_profile_buffer = _e; }
  { auto _e = is_fusion_output(); _o->is_fusion_output = _e; }
}

inline ::flatbuffers::Offset<GlobalBufferInfo> GlobalBufferInfo::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const GlobalBufferInfoT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateGlobalBufferInfo(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<GlobalBufferInfo> CreateGlobalBufferInfo(::flatbuffers::FlatBufferBuilder &_fbb, const GlobalBufferInfoT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const GlobalBufferInfoT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _tv = _o->tv;
  auto _sizes = _o->sizes.size() ? _fbb.CreateVector(_o->sizes) : 0;
  auto _strides = _o->strides.size() ? _fbb.CreateVector(_o->strides) : 0;
  auto _dtype = _o->dtype;
  auto _zero_init = _o->zero_init;
  auto _is_profile_buffer = _o->is_profile_buffer;
  auto _is_fusion_output = _o->is_fusion_output;
  return nvfuser::serde::CreateGlobalBufferInfo(
      _fbb,
      _tv,
      _sizes,
      _strides,
      _dtype,
      _zero_init,
      _is_profile_buffer,
      _is_fusion_output);
}

inline ExecutorEntryT::ExecutorEntryT(const ExecutorEntryT &o)
      : init(o.init),
        launch_params((o.launch_params) ? new nvfuser::serde::LaunchParamsT(*o.launch_params) : nullptr),
        output_aliases(o.output_aliases),
        input_aliases(o.input_aliases) {
  outputs.reserve(o.outputs.size());
  for (const auto &outputs_ : o.outputs) { outputs.emplace_back((outputs_) ? new nvfuser::serde::GlobalBufferInfoT(*outputs_) : nullptr); }
  intermediates.reserve(o.intermediates.size());
  for (const auto &intermediates_ : o.intermediates) { intermediates.emplace_back((intermediates_) ? new nvfuser::serde::GlobalBufferInfoT(*intermediates_) : nullptr); }
}

inline ExecutorEntryT &ExecutorEntryT::operator=(ExecutorEntryT o) FLATBUFFERS_NOEXCEPT {
  std::swap(init, o.init);
  std::swap(launch_params, o.launch_params);
  std::swap(output_aliases, o.output_aliases);
  std::swap(input_aliases, o.input_aliases);
  std::swap(outputs, o.outputs);
  std::swap(intermediates, o.intermediates);
  return *this;
}

inline ExecutorEntryT *ExecutorEntry::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<ExecutorEntryT>(new ExecutorEntryT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void ExecutorEntry::UnPackTo(ExecutorEntryT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = init(); _o->init = _e; }
  { auto _e = launch_params(); if (_e) { if(_o->launch_params) { _e->UnPackTo(_o->launch_params.get(), _resolver); } else { _o->launch_params = std::unique_ptr<nvfuser::serde::LaunchParamsT>(_e->UnPack(_resolver)); } } else if (_o->launch_params) { _o->launch_params.reset(); } }
  { auto _e = output_aliases(); if (_e) { _o->output_aliases.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->output_aliases[_i] = _e->Get(_i); } } else { _o->output_aliases.resize(0); } }
  { auto _e = input_aliases(); if (_e) { _o->input_aliases.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->input_aliases[_i] = _e->Get(_i); } } else { _o->input_aliases.resize(0); } }
  { auto _e = outputs(); if (_e) { _o->outputs.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->outputs[_i]) { _e->Get(_i)->UnPackTo(_o->outputs[_i].get(), _resolver); } else { _o->outputs[_i] = std::unique_ptr<nvfuser::serde::GlobalBufferInfoT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->outputs.resize(0); } }
  { auto _e = intermediates(); if (_e) { _o->intermediates.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->intermediates[_i]) { _e->Get(_i)->UnPackTo(_o->intermediates[_i].get(), _resolver); } else { _o->intermediates[_i] = std::unique_ptr<nvfuser::serde::GlobalBufferInfoT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->intermediates.resize(0); } }
}

inline ::flatbuffers::Offset<ExecutorEntry> ExecutorEntry::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ExecutorEntryT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateExecutorEntry(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<ExecutorEntry> CreateExecutorEntry(::flatbuffers::FlatBufferBuilder &_fbb, const ExecutorEntryT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const ExecutorEntryT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _init = _o->init;
  auto _launch_params = _o->launch_params ? CreateLaunchParams(_fbb, _o->launch_params.get(), _rehasher) : 0;
  auto _output_aliases = _o->output_aliases.size() ? _fbb.CreateVector(_o->output_aliases) : 0;
  auto _input_aliases = _o->input_aliases.size() ? _fbb.CreateVector(_o->input_aliases) : 0;
  auto _outputs = _o->outputs.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> (_o->outputs.size(), [](size_t i, _VectorArgs *__va) { return CreateGlobalBufferInfo(*__va->__fbb, __va->__o->outputs[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _intermediates = _o->intermediates.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> (_o->intermediates.size(), [](size_t i, _VectorArgs *__va) { return CreateGlobalBufferInfo(*__va->__fbb, __va->__o->intermediates[i].get(), __va->__rehasher); }, &_va ) : 0;
  return nvfuser::serde::CreateExecutorEntry(
      _fbb,
      _init,
      _launch_params,
      _output_aliases,
      _input_aliases,
      _outputs,
      _intermediates);
}

inline AtT *At::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<AtT>(new AtT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void At::UnPackTo(AtT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = index(); _o->index = _e; }
}

inline ::flatbuffers::Offset<At> At::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const AtT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateAt(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<At> CreateAt(::flatbuffers::FlatBufferBuilder &_fbb, const AtT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const AtT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _index = _o->index;
  return nvfuser::serde::CreateAt(
      _fbb,
      _index);
}

inline BatchNormT *BatchNorm::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<BatchNormT>(new BatchNormT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void BatchNorm::UnPackTo(BatchNormT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = training(); _o->training = _e; }
  { auto _e = channels_last(); _o->channels_last = _e; }
}

inline ::flatbuffers::Offset<BatchNorm> BatchNorm::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const BatchNormT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateBatchNorm(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<BatchNorm> CreateBatchNorm(::flatbuffers::FlatBufferBuilder &_fbb, const BatchNormT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const BatchNormT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _training = _o->training;
  auto _channels_last = _o->channels_last;
  return nvfuser::serde::CreateBatchNorm(
      _fbb,
      _training,
      _channels_last);
}

inline BroadcastT *Broadcast::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<BroadcastT>(new BroadcastT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Broadcast::UnPackTo(BroadcastT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = broadcast_dims(); if (_e) { _o->broadcast_dims.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->broadcast_dims[_i] = _e->Get(_i) != 0; } } else { _o->broadcast_dims.resize(0); } }
}

inline ::flatbuffers::Offset<Broadcast> Broadcast::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const BroadcastT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateBroadcast(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Broadcast> CreateBroadcast(::flatbuffers::FlatBufferBuilder &_fbb, const BroadcastT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const BroadcastT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _broadcast_dims = _o->broadcast_dims.size() ? _fbb.CreateVector(_o->broadcast_dims) : 0;
  return nvfuser::serde::CreateBroadcast(
      _fbb,
      _broadcast_dims);
}

inline BroadcastInDimT *BroadcastInDim::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<BroadcastInDimT>(new BroadcastInDimT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void BroadcastInDim::UnPackTo(BroadcastInDimT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = output_size(); _o->output_size = _e; }
  { auto _e = broadcast_dims(); if (_e) { _o->broadcast_dims.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->broadcast_dims[_i] = _e->Get(_i); } } else { _o->broadcast_dims.resize(0); } }
}

inline ::flatbuffers::Offset<BroadcastInDim> BroadcastInDim::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const BroadcastInDimT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateBroadcastInDim(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<BroadcastInDim> CreateBroadcastInDim(::flatbuffers::FlatBufferBuilder &_fbb, const BroadcastInDimT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const BroadcastInDimT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _output_size = _o->output_size;
  auto _broadcast_dims = _o->broadcast_dims.size() ? _fbb.CreateVector(_o->broadcast_dims) : 0;
  return nvfuser::serde::CreateBroadcastInDim(
      _fbb,
      _output_size,
      _broadcast_dims);
}

inline DtypeT *Dtype::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<DtypeT>(new DtypeT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Dtype::UnPackTo(DtypeT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = dtype(); _o->dtype = _e; }
}

inline ::flatbuffers::Offset<Dtype> Dtype::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const DtypeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateDtype(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Dtype> CreateDtype(::flatbuffers::FlatBufferBuilder &_fbb, const DtypeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const DtypeT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _dtype = _o->dtype;
  return nvfuser::serde::CreateDtype(
      _fbb,
      _dtype);
}

inline DimensionT *Dimension::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<DimensionT>(new DimensionT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Dimension::UnPackTo(DimensionT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = dim(); _o->dim = _e; }
}

inline ::flatbuffers::Offset<Dimension> Dimension::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const DimensionT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateDimension(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Dimension> CreateDimension(::flatbuffers::FlatBufferBuilder &_fbb, const DimensionT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const DimensionT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _dim = _o->dim;
  return nvfuser::serde::CreateDimension(
      _fbb,
      _dim);
}

inline NormT *Norm::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<NormT>(new NormT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Norm::UnPackTo(NormT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = axes(); if (_e) { _o->axes.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->axes[_i] = _e->Get(_i); } } else { _o->axes.resize(0); } }
  { auto _e = correction(); _o->correction = _e; }
  { auto _e = keep_dim(); _o->keep_dim = _e; }
}

inline ::flatbuffers::Offset<Norm> Norm::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const NormT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateNorm(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Norm> CreateNorm(::flatbuffers::FlatBufferBuilder &_fbb, const NormT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const NormT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _axes = _o->axes.size() ? _fbb.CreateVector(_o->axes) : 0;
  auto _correction = _o->correction;
  auto _keep_dim = _o->keep_dim;
  return nvfuser::serde::CreateNorm(
      _fbb,
      _axes,
      _correction,
      _keep_dim);
}

inline OutputT *Output::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<OutputT>(new OutputT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Output::UnPackTo(OutputT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = stride_order(); if (_e) { _o->stride_order.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->stride_order[_i] = _e->Get(_i); } } else { _o->stride_order.resize(0); } }
}

inline ::flatbuffers::Offset<Output> Output::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const OutputT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateOutput(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Output> CreateOutput(::flatbuffers::FlatBufferBuilder &_fbb, const OutputT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const OutputT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _stride_order = _o->stride_order.size() ? _fbb.CreateVector(_o->stride_order) : 0;
  return nvfuser::serde::CreateOutput(
      _fbb,
      _stride_order);
}

inline PadT *Pad::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<PadT>(new PadT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Pad::UnPackTo(PadT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = pad_widths(); if (_e) { _o->pad_widths.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->pad_widths[_i] = _e->Get(_i); } } else { _o->pad_widths.resize(0); } }
}

inline ::flatbuffers::Offset<Pad> Pad::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const PadT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreatePad(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Pad> CreatePad(::flatbuffers::FlatBufferBuilder &_fbb, const PadT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const PadT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _pad_widths = _o->pad_widths.size() ? _fbb.CreateVector(_o->pad_widths) : 0;
  return nvfuser::serde::CreatePad(
      _fbb,
      _pad_widths);
}

inline PermuteT *Permute::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<PermuteT>(new PermuteT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Permute::UnPackTo(PermuteT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = dims(); if (_e) { _o->dims.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->dims[_i] = _e->Get(_i); } } else { _o->dims.resize(0); } }
}

inline ::flatbuffers::Offset<Permute> Permute::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const PermuteT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreatePermute(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Permute> CreatePermute(::flatbuffers::FlatBufferBuilder &_fbb, const PermuteT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const PermuteT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _dims = _o->dims.size() ? _fbb.CreateVector(_o->dims) : 0;
  return nvfuser::serde::CreatePermute(
      _fbb,
      _dims);
}

inline ReductionT *Reduction::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<ReductionT>(new ReductionT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Reduction::UnPackTo(ReductionT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = axes(); if (_e) { _o->axes.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->axes[_i] = _e->Get(_i); } } else { _o->axes.resize(0); } }
  { auto _e = keep_dim(); _o->keep_dim = _e; }
  { auto _e = dtype(); _o->dtype = _e; }
}

inline ::flatbuffers::Offset<Reduction> Reduction::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ReductionT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateReduction(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Reduction> CreateReduction(::flatbuffers::FlatBufferBuilder &_fbb, const ReductionT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const ReductionT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _axes = _o->axes.size() ? _fbb.CreateVector(_o->axes) : 0;
  auto _keep_dim = _o->keep_dim;
  auto _dtype = _o->dtype;
  return nvfuser::serde::CreateReduction(
      _fbb,
      _axes,
      _keep_dim,
      _dtype);
}

inline ReshapeT *Reshape::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<ReshapeT>(new ReshapeT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Reshape::UnPackTo(ReshapeT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = original_shape(); if (_e) { _o->original_shape.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->original_shape[_i] = _e->Get(_i); } } else { _o->original_shape.resize(0); } }
  { auto _e = new_shape(); if (_e) { _o->new_shape.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->new_shape[_i] = _e->Get(_i); } } else { _o->new_shape.resize(0); } }
}

inline ::flatbuffers::Offset<Reshape> Reshape::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ReshapeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateReshape(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Reshape> CreateReshape(::flatbuffers::FlatBufferBuilder &_fbb, const ReshapeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const ReshapeT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _original_shape = _o->original_shape.size() ? _fbb.CreateVector(_o->original_shape) : 0;
  auto _new_shape = _o->new_shape.size() ? _fbb.CreateVector(_o->new_shape) : 0;
  return nvfuser::serde::CreateReshape(
      _fbb,
      _original_shape,
      _new_shape);
}

inline SizeT *Size::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<SizeT>(new SizeT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Size::UnPackTo(SizeT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = dim(); _o->dim = _e; }
}

inline ::flatbuffers::Offset<Size> Size::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const SizeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateSize(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Size> CreateSize(::flatbuffers::FlatBufferBuilder &_fbb, const SizeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const SizeT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _dim = _o->dim;
  return nvfuser::serde::CreateSize(
      _fbb,
      _dim);
}

inline SliceT *Slice::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<SliceT>(new SliceT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Slice::UnPackTo(SliceT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = start_indices(); if (_e) { _o->start_indices.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->start_indices[_i] = _e->Get(_i); } } else { _o->start_indices.resize(0); } }
  { auto _e = end_indices(); if (_e) { _o->end_indices.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->end_indices[_i] = _e->Get(_i); } } else { _o->end_indices.resize(0); } }
  { auto _e = strides(); if (_e) { _o->strides.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->strides[_i] = _e->Get(_i); } } else { _o->strides.resize(0); } }
}

inline ::flatbuffers::Offset<Slice> Slice::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const SliceT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateSlice(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Slice> CreateSlice(::flatbuffers::FlatBufferBuilder &_fbb, const SliceT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const SliceT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _start_indices = _o->start_indices.size() ? _fbb.CreateVector(_o->start_indices) : 0;
  auto _end_indices = _o->end_indices.size() ? _fbb.CreateVector(_o->end_indices) : 0;
  auto _strides = _o->strides.size() ? _fbb.CreateVector(_o->strides) : 0;
  return nvfuser::serde::CreateSlice(
      _fbb,
      _start_indices,
      _end_indices,
      _strides);
}

inline SqueezeT *Squeeze::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<SqueezeT>(new SqueezeT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Squeeze::UnPackTo(SqueezeT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = original_shape(); if (_e) { _o->original_shape.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->original_shape[_i] = _e->Get(_i); } } else { _o->original_shape.resize(0); } }
  { auto _e = squeeze_dims(); if (_e) { _o->squeeze_dims.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->squeeze_dims[_i] = _e->Get(_i); } } else { _o->squeeze_dims.resize(0); } }
}

inline ::flatbuffers::Offset<Squeeze> Squeeze::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const SqueezeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateSqueeze(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Squeeze> CreateSqueeze(::flatbuffers::FlatBufferBuilder &_fbb, const SqueezeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const SqueezeT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _original_shape = _o->original_shape.size() ? _fbb.CreateVector(_o->original_shape) : 0;
  auto _squeeze_dims = _o->squeeze_dims.size() ? _fbb.CreateVector(_o->squeeze_dims) : 0;
  return nvfuser::serde::CreateSqueeze(
      _fbb,
      _original_shape,
      _squeeze_dims);
}

inline TensorT *Tensor::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<TensorT>(new TensorT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Tensor::UnPackTo(TensorT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = sizes(); if (_e) { _o->sizes.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->sizes[_i] = _e->Get(_i); } } else { _o->sizes.resize(0); } }
  { auto _e = contiguity(); if (_e) { _o->contiguity.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->contiguity[_i] = static_cast<nvfuser::serde::Contiguity>(_e->Get(_i)); } } else { _o->contiguity.resize(0); } }
  { auto _e = dtype(); _o->dtype = _e; }
  { auto _e = is_cpu(); _o->is_cpu = _e; }
}

inline ::flatbuffers::Offset<Tensor> Tensor::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateTensor(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Tensor> CreateTensor(::flatbuffers::FlatBufferBuilder &_fbb, const TensorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const TensorT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _sizes = _o->sizes.size() ? _fbb.CreateVector(_o->sizes) : 0;
  auto _contiguity = _o->contiguity.size() ? _fbb.CreateVectorScalarCast<int32_t>(::flatbuffers::data(_o->contiguity), _o->contiguity.size()) : 0;
  auto _dtype = _o->dtype;
  auto _is_cpu = _o->is_cpu;
  return nvfuser::serde::CreateTensor(
      _fbb,
      _sizes,
      _contiguity,
      _dtype,
      _is_cpu);
}

inline TensorCreationT *TensorCreation::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<TensorCreationT>(new TensorCreationT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void TensorCreation::UnPackTo(TensorCreationT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = shape(); if (_e) { _o->shape.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->shape[_i] = _e->Get(_i); } } else { _o->shape.resize(0); } }
  { auto _e = dtype(); _o->dtype = _e; }
}

inline ::flatbuffers::Offset<TensorCreation> TensorCreation::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorCreationT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateTensorCreation(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<TensorCreation> CreateTensorCreation(::flatbuffers::FlatBufferBuilder &_fbb, const TensorCreationT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const TensorCreationT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _shape = _o->shape.size() ? _fbb.CreateVector(_o->shape) : 0;
  auto _dtype = _o->dtype;
  return nvfuser::serde::CreateTensorCreation(
      _fbb,
      _shape,
      _dtype);
}

inline TensorCreationSymbolicT *TensorCreationSymbolic::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<TensorCreationSymbolicT>(new TensorCreationSymbolicT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void TensorCreationSymbolic::UnPackTo(TensorCreationSymbolicT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = shape(); if (_e) { _o->shape.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->shape[_i] = *_e->Get(_i); } } else { _o->shape.resize(0); } }
  { auto _e = dtype(); _o->dtype = _e; }
}

inline ::flatbuffers::Offset<TensorCreationSymbolic> TensorCreationSymbolic::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorCreationSymbolicT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateTensorCreationSymbolic(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<TensorCreationSymbolic> CreateTensorCreationSymbolic(::flatbuffers::FlatBufferBuilder &_fbb, const TensorCreationSymbolicT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const TensorCreationSymbolicT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _shape = _o->shape.size() ? _fbb.CreateVectorOfStructs(_o->shape) : 0;
  auto _dtype = _o->dtype;
  return nvfuser::serde::CreateTensorCreationSymbolic(
      _fbb,
      _shape,
      _dtype);
}

inline VectorT *Vector::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<VectorT>(new VectorT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Vector::UnPackTo(VectorT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = dtype(); _o->dtype = _e; }
}

inline ::flatbuffers::Offset<Vector> Vector::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const VectorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateVector(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Vector> CreateVector(::flatbuffers::FlatBufferBuilder &_fbb, const VectorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const VectorT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _dtype = _o->dtype;
  return nvfuser::serde::CreateVector(
      _fbb,
      _dtype);
}

inline KernelSummaryT::KernelSummaryT(const KernelSummaryT &o)
      : has_cooperative_grid_reduction(o.has_cooperative_grid_reduction),
        has_dynamic_local_memory_allocations(o.has_dynamic_local_memory_allocations),
        has_block_reductions(o.has_block_reductions),
        has_grid_reductions(o.has_grid_reductions),
        has_block_broadcasts(o.has_block_broadcasts),
        has_grid_broadcasts(o.has_grid_broadcasts),
        has_block_welford(o.has_block_welford),
        has_grid_welford(o.has_grid_welford),
        has_outer_grouped_grid_welford(o.has_outer_grouped_grid_welford),
        largest_smem_data_type(o.largest_smem_data_type),
        outer_grouped_grid_welford_largest_smem_size(o.outer_grouped_grid_welford_largest_smem_size),
        generator((o.generator) ? new nvfuser::serde::NaiveValueGeneratorT(*o.generator) : nullptr) {
  global_allocations.reserve(o.global_allocations.size());
  for (const auto &global_allocations_ : o.global_allocations) { global_allocations.emplace_back((global_allocations_) ? new nvfuser::serde::AllocateBufferT(*global_allocations_) : nullptr); }
}

inline KernelSummaryT &KernelSummaryT::operator=(KernelSummaryT o) FLATBUFFERS_NOEXCEPT {
  std::swap(has_cooperative_grid_reduction, o.has_cooperative_grid_reduction);
  std::swap(has_dynamic_local_memory_allocations, o.has_dynamic_local_memory_allocations);
  std::swap(has_block_reductions, o.has_block_reductions);
  std::swap(has_grid_reductions, o.has_grid_reductions);
  std::swap(has_block_broadcasts, o.has_block_broadcasts);
  std::swap(has_grid_broadcasts, o.has_grid_broadcasts);
  std::swap(has_block_welford, o.has_block_welford);
  std::swap(has_grid_welford, o.has_grid_welford);
  std::swap(has_outer_grouped_grid_welford, o.has_outer_grouped_grid_welford);
  std::swap(largest_smem_data_type, o.largest_smem_data_type);
  std::swap(outer_grouped_grid_welford_largest_smem_size, o.outer_grouped_grid_welford_largest_smem_size);
  std::swap(generator, o.generator);
  std::swap(global_allocations, o.global_allocations);
  return *this;
}

inline KernelSummaryT *KernelSummary::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<KernelSummaryT>(new KernelSummaryT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void KernelSummary::UnPackTo(KernelSummaryT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = has_cooperative_grid_reduction(); _o->has_cooperative_grid_reduction = _e; }
  { auto _e = has_dynamic_local_memory_allocations(); _o->has_dynamic_local_memory_allocations = _e; }
  { auto _e = has_block_reductions(); _o->has_block_reductions = _e; }
  { auto _e = has_grid_reductions(); _o->has_grid_reductions = _e; }
  { auto _e = has_block_broadcasts(); _o->has_block_broadcasts = _e; }
  { auto _e = has_grid_broadcasts(); _o->has_grid_broadcasts = _e; }
  { auto _e = has_block_welford(); _o->has_block_welford = _e; }
  { auto _e = has_grid_welford(); _o->has_grid_welford = _e; }
  { auto _e = has_outer_grouped_grid_welford(); _o->has_outer_grouped_grid_welford = _e; }
  { auto _e = largest_smem_data_type(); _o->largest_smem_data_type = _e; }
  { auto _e = outer_grouped_grid_welford_largest_smem_size(); _o->outer_grouped_grid_welford_largest_smem_size = _e; }
  { auto _e = generator(); if (_e) { if(_o->generator) { _e->UnPackTo(_o->generator.get(), _resolver); } else { _o->generator = std::unique_ptr<nvfuser::serde::NaiveValueGeneratorT>(_e->UnPack(_resolver)); } } else if (_o->generator) { _o->generator.reset(); } }
  { auto _e = global_allocations(); if (_e) { _o->global_allocations.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->global_allocations[_i]) { _e->Get(_i)->UnPackTo(_o->global_allocations[_i].get(), _resolver); } else { _o->global_allocations[_i] = std::unique_ptr<nvfuser::serde::AllocateBufferT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->global_allocations.resize(0); } }
}

inline ::flatbuffers::Offset<KernelSummary> KernelSummary::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const KernelSummaryT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateKernelSummary(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<KernelSummary> CreateKernelSummary(::flatbuffers::FlatBufferBuilder &_fbb, const KernelSummaryT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const KernelSummaryT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _has_cooperative_grid_reduction = _o->has_cooperative_grid_reduction;
  auto _has_dynamic_local_memory_allocations = _o->has_dynamic_local_memory_allocations;
  auto _has_block_reductions = _o->has_block_reductions;
  auto _has_grid_reductions = _o->has_grid_reductions;
  auto _has_block_broadcasts = _o->has_block_broadcasts;
  auto _has_grid_broadcasts = _o->has_grid_broadcasts;
  auto _has_block_welford = _o->has_block_welford;
  auto _has_grid_welford = _o->has_grid_welford;
  auto _has_outer_grouped_grid_welford = _o->has_outer_grouped_grid_welford;
  auto _largest_smem_data_type = _o->largest_smem_data_type;
  auto _outer_grouped_grid_welford_largest_smem_size = _o->outer_grouped_grid_welford_largest_smem_size;
  auto _generator = _o->generator ? CreateNaiveValueGenerator(_fbb, _o->generator.get(), _rehasher) : 0;
  auto _global_allocations = _o->global_allocations.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>> (_o->global_allocations.size(), [](size_t i, _VectorArgs *__va) { return CreateAllocateBuffer(*__va->__fbb, __va->__o->global_allocations[i].get(), __va->__rehasher); }, &_va ) : 0;
  return nvfuser::serde::CreateKernelSummary(
      _fbb,
      _has_cooperative_grid_reduction,
      _has_dynamic_local_memory_allocations,
      _has_block_reductions,
      _has_grid_reductions,
      _has_block_broadcasts,
      _has_grid_broadcasts,
      _has_block_welford,
      _has_grid_welford,
      _has_outer_grouped_grid_welford,
      _largest_smem_data_type,
      _outer_grouped_grid_welford_largest_smem_size,
      _generator,
      _global_allocations);
}

inline FusionExecutorT::FusionExecutorT(const FusionExecutorT &o)
      : device_smem_limit(o.device_smem_limit),
        block_size_high_water_mark(o.block_size_high_water_mark),
        maxrregcount_high_water_mark(o.maxrregcount_high_water_mark),
        warp_size(o.warp_size),
        fusion_id(o.fusion_id),
        fusion_id_counter(o.fusion_id_counter),
        kernel_code(o.kernel_code),
        executor_entry_lookup_keys(o.executor_entry_lookup_keys),
        index_type(o.index_type),
        summary((o.summary) ? new nvfuser::serde::KernelSummaryT(*o.summary) : nullptr) {
  executor_entry_lookup_values.reserve(o.executor_entry_lookup_values.size());
  for (const auto &executor_entry_lookup_values_ : o.executor_entry_lookup_values) { executor_entry_lookup_values.emplace_back((executor_entry_lookup_values_) ? new nvfuser::serde::ExecutorEntryT(*executor_entry_lookup_values_) : nullptr); }
}

inline FusionExecutorT &FusionExecutorT::operator=(FusionExecutorT o) FLATBUFFERS_NOEXCEPT {
  std::swap(device_smem_limit, o.device_smem_limit);
  std::swap(block_size_high_water_mark, o.block_size_high_water_mark);
  std::swap(maxrregcount_high_water_mark, o.maxrregcount_high_water_mark);
  std::swap(warp_size, o.warp_size);
  std::swap(fusion_id, o.fusion_id);
  std::swap(fusion_id_counter, o.fusion_id_counter);
  std::swap(kernel_code, o.kernel_code);
  std::swap(executor_entry_lookup_keys, o.executor_entry_lookup_keys);
  std::swap(executor_entry_lookup_values, o.executor_entry_lookup_values);
  std::swap(index_type, o.index_type);
  std::swap(summary, o.summary);
  return *this;
}

inline FusionExecutorT *FusionExecutor::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<FusionExecutorT>(new FusionExecutorT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void FusionExecutor::UnPackTo(FusionExecutorT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = device_smem_limit(); _o->device_smem_limit = _e; }
  { auto _e = block_size_high_water_mark(); _o->block_size_high_water_mark = _e; }
  { auto _e = maxrregcount_high_water_mark(); _o->maxrregcount_high_water_mark = _e; }
  { auto _e = warp_size(); _o->warp_size = _e; }
  { auto _e = fusion_id(); _o->fusion_id = _e; }
  { auto _e = fusion_id_counter(); _o->fusion_id_counter = _e; }
  { auto _e = kernel_code(); if (_e) _o->kernel_code = _e->str(); }
  { auto _e = executor_entry_lookup_keys(); if (_e) { _o->executor_entry_lookup_keys.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->executor_entry_lookup_keys[_i] = _e->Get(_i); } } else { _o->executor_entry_lookup_keys.resize(0); } }
  { auto _e = executor_entry_lookup_values(); if (_e) { _o->executor_entry_lookup_values.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->executor_entry_lookup_values[_i]) { _e->Get(_i)->UnPackTo(_o->executor_entry_lookup_values[_i].get(), _resolver); } else { _o->executor_entry_lookup_values[_i] = std::unique_ptr<nvfuser::serde::ExecutorEntryT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->executor_entry_lookup_values.resize(0); } }
  { auto _e = index_type(); _o->index_type = _e; }
  { auto _e = summary(); if (_e) { if(_o->summary) { _e->UnPackTo(_o->summary.get(), _resolver); } else { _o->summary = std::unique_ptr<nvfuser::serde::KernelSummaryT>(_e->UnPack(_resolver)); } } else if (_o->summary) { _o->summary.reset(); } }
}

inline ::flatbuffers::Offset<FusionExecutor> FusionExecutor::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const FusionExecutorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateFusionExecutor(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<FusionExecutor> CreateFusionExecutor(::flatbuffers::FlatBufferBuilder &_fbb, const FusionExecutorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const FusionExecutorT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _device_smem_limit = _o->device_smem_limit;
  auto _block_size_high_water_mark = _o->block_size_high_water_mark;
  auto _maxrregcount_high_water_mark = _o->maxrregcount_high_water_mark;
  auto _warp_size = _o->warp_size;
  auto _fusion_id = _o->fusion_id;
  auto _fusion_id_counter = _o->fusion_id_counter;
  auto _kernel_code = _o->kernel_code.empty() ? 0 : _fbb.CreateString(_o->kernel_code);
  auto _executor_entry_lookup_keys = _o->executor_entry_lookup_keys.size() ? _fbb.CreateVector(_o->executor_entry_lookup_keys) : 0;
  auto _executor_entry_lookup_values = _o->executor_entry_lookup_values.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>> (_o->executor_entry_lookup_values.size(), [](size_t i, _VectorArgs *__va) { return CreateExecutorEntry(*__va->__fbb, __va->__o->executor_entry_lookup_values[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _index_type = _o->index_type;
  auto _summary = _o->summary ? CreateKernelSummary(_fbb, _o->summary.get(), _rehasher) : 0;
  return nvfuser::serde::CreateFusionExecutor(
      _fbb,
      _device_smem_limit,
      _block_size_high_water_mark,
      _maxrregcount_high_water_mark,
      _warp_size,
      _fusion_id,
      _fusion_id_counter,
      _kernel_code,
      _executor_entry_lookup_keys,
      _executor_entry_lookup_values,
      _index_type,
      _summary);
}

inline FusionKernelRuntimeT::FusionKernelRuntimeT(const FusionKernelRuntimeT &o)
      : args((o.args) ? new nvfuser::serde::KernelArgumentHolderT(*o.args) : nullptr) {
  executors.reserve(o.executors.size());
  for (const auto &executors_ : o.executors) { executors.emplace_back((executors_) ? new nvfuser::serde::FusionExecutorT(*executors_) : nullptr); }
}

inline FusionKernelRuntimeT &FusionKernelRuntimeT::operator=(FusionKernelRuntimeT o) FLATBUFFERS_NOEXCEPT {
  std::swap(args, o.args);
  std::swap(executors, o.executors);
  return *this;
}

inline FusionKernelRuntimeT *FusionKernelRuntime::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<FusionKernelRuntimeT>(new FusionKernelRuntimeT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void FusionKernelRuntime::UnPackTo(FusionKernelRuntimeT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = args(); if (_e) { if(_o->args) { _e->UnPackTo(_o->args.get(), _resolver); } else { _o->args = std::unique_ptr<nvfuser::serde::KernelArgumentHolderT>(_e->UnPack(_resolver)); } } else if (_o->args) { _o->args.reset(); } }
  { auto _e = executors(); if (_e) { _o->executors.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->executors[_i]) { _e->Get(_i)->UnPackTo(_o->executors[_i].get(), _resolver); } else { _o->executors[_i] = std::unique_ptr<nvfuser::serde::FusionExecutorT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->executors.resize(0); } }
}

inline ::flatbuffers::Offset<FusionKernelRuntime> FusionKernelRuntime::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const FusionKernelRuntimeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateFusionKernelRuntime(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<FusionKernelRuntime> CreateFusionKernelRuntime(::flatbuffers::FlatBufferBuilder &_fbb, const FusionKernelRuntimeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const FusionKernelRuntimeT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _args = _o->args ? CreateKernelArgumentHolder(_fbb, _o->args.get(), _rehasher) : 0;
  auto _executors = _o->executors.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>> (_o->executors.size(), [](size_t i, _VectorArgs *__va) { return CreateFusionExecutor(*__va->__fbb, __va->__o->executors[i].get(), __va->__rehasher); }, &_va ) : 0;
  return nvfuser::serde::CreateFusionKernelRuntime(
      _fbb,
      _args,
      _executors);
}

inline InputsIdLookupT *InputsIdLookup::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<InputsIdLookupT>(new InputsIdLookupT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void InputsIdLookup::UnPackTo(InputsIdLookupT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = max_cache_size(); _o->max_cache_size = _e; }
  { auto _e = current_id(); _o->current_id = _e; }
  { auto _e = lru_cache(); if (_e) { _o->lru_cache.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->lru_cache[_i] = _e->Get(_i)->str(); } } else { _o->lru_cache.resize(0); } }
  { auto _e = encoding_lookup_keys(); if (_e) { _o->encoding_lookup_keys.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->encoding_lookup_keys[_i] = _e->Get(_i)->str(); } } else { _o->encoding_lookup_keys.resize(0); } }
  { auto _e = encoding_lookup_values(); if (_e) { _o->encoding_lookup_values.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->encoding_lookup_values[_i] = *_e->Get(_i); } } else { _o->encoding_lookup_values.resize(0); } }
}

inline ::flatbuffers::Offset<InputsIdLookup> InputsIdLookup::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const InputsIdLookupT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateInputsIdLookup(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<InputsIdLookup> CreateInputsIdLookup(::flatbuffers::FlatBufferBuilder &_fbb, const InputsIdLookupT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const InputsIdLookupT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _max_cache_size = _o->max_cache_size;
  auto _current_id = _o->current_id;
  auto _lru_cache = _o->lru_cache.size() ? _fbb.CreateVectorOfStrings(_o->lru_cache) : 0;
  auto _encoding_lookup_keys = _o->encoding_lookup_keys.size() ? _fbb.CreateVectorOfStrings(_o->encoding_lookup_keys) : 0;
  auto _encoding_lookup_values = _o->encoding_lookup_values.size() ? _fbb.CreateVectorOfStructs(_o->encoding_lookup_values) : 0;
  return nvfuser::serde::CreateInputsIdLookup(
      _fbb,
      _max_cache_size,
      _current_id,
      _lru_cache,
      _encoding_lookup_keys,
      _encoding_lookup_values);
}

inline KernelRuntimeStateT::KernelRuntimeStateT(const KernelRuntimeStateT &o)
      : device_id(o.device_id),
        has_dynamic_transform_info(o.has_dynamic_transform_info) {
  runtimes.reserve(o.runtimes.size());
  for (const auto &runtimes_ : o.runtimes) { runtimes.emplace_back((runtimes_) ? new nvfuser::serde::FusionKernelRuntimeT(*runtimes_) : nullptr); }
}

inline KernelRuntimeStateT &KernelRuntimeStateT::operator=(KernelRuntimeStateT o) FLATBUFFERS_NOEXCEPT {
  std::swap(device_id, o.device_id);
  std::swap(has_dynamic_transform_info, o.has_dynamic_transform_info);
  std::swap(runtimes, o.runtimes);
  return *this;
}

inline KernelRuntimeStateT *KernelRuntimeState::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<KernelRuntimeStateT>(new KernelRuntimeStateT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void KernelRuntimeState::UnPackTo(KernelRuntimeStateT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = device_id(); _o->device_id = _e; }
  { auto _e = has_dynamic_transform_info(); _o->has_dynamic_transform_info = _e; }
  { auto _e = runtimes(); if (_e) { _o->runtimes.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->runtimes[_i]) { _e->Get(_i)->UnPackTo(_o->runtimes[_i].get(), _resolver); } else { _o->runtimes[_i] = std::unique_ptr<nvfuser::serde::FusionKernelRuntimeT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->runtimes.resize(0); } }
}

inline ::flatbuffers::Offset<KernelRuntimeState> KernelRuntimeState::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const KernelRuntimeStateT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateKernelRuntimeState(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<KernelRuntimeState> CreateKernelRuntimeState(::flatbuffers::FlatBufferBuilder &_fbb, const KernelRuntimeStateT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const KernelRuntimeStateT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _device_id = _o->device_id;
  auto _has_dynamic_transform_info = _o->has_dynamic_transform_info;
  auto _runtimes = _o->runtimes.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>> (_o->runtimes.size(), [](size_t i, _VectorArgs *__va) { return CreateFusionKernelRuntime(*__va->__fbb, __va->__o->runtimes[i].get(), __va->__rehasher); }, &_va ) : 0;
  return nvfuser::serde::CreateKernelRuntimeState(
      _fbb,
      _device_id,
      _has_dynamic_transform_info,
      _runtimes);
}

inline FusionExecutorCacheT::FusionExecutorCacheT(const FusionExecutorCacheT &o)
      : inputs_cache((o.inputs_cache) ? new nvfuser::serde::InputsIdLookupT(*o.inputs_cache) : nullptr),
        kernel_cache_keys(o.kernel_cache_keys),
        kernel_cache_values(o.kernel_cache_values) {
  kernel_runtimes_map.reserve(o.kernel_runtimes_map.size());
  for (const auto &kernel_runtimes_map_ : o.kernel_runtimes_map) { kernel_runtimes_map.emplace_back((kernel_runtimes_map_) ? new nvfuser::serde::KernelRuntimeStateT(*kernel_runtimes_map_) : nullptr); }
}

inline FusionExecutorCacheT &FusionExecutorCacheT::operator=(FusionExecutorCacheT o) FLATBUFFERS_NOEXCEPT {
  std::swap(inputs_cache, o.inputs_cache);
  std::swap(kernel_runtimes_map, o.kernel_runtimes_map);
  std::swap(kernel_cache_keys, o.kernel_cache_keys);
  std::swap(kernel_cache_values, o.kernel_cache_values);
  return *this;
}

inline FusionExecutorCacheT *FusionExecutorCache::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<FusionExecutorCacheT>(new FusionExecutorCacheT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void FusionExecutorCache::UnPackTo(FusionExecutorCacheT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = inputs_cache(); if (_e) { if(_o->inputs_cache) { _e->UnPackTo(_o->inputs_cache.get(), _resolver); } else { _o->inputs_cache = std::unique_ptr<nvfuser::serde::InputsIdLookupT>(_e->UnPack(_resolver)); } } else if (_o->inputs_cache) { _o->inputs_cache.reset(); } }
  { auto _e = kernel_runtimes_map(); if (_e) { _o->kernel_runtimes_map.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->kernel_runtimes_map[_i]) { _e->Get(_i)->UnPackTo(_o->kernel_runtimes_map[_i].get(), _resolver); } else { _o->kernel_runtimes_map[_i] = std::unique_ptr<nvfuser::serde::KernelRuntimeStateT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->kernel_runtimes_map.resize(0); } }
  { auto _e = kernel_cache_keys(); if (_e) { _o->kernel_cache_keys.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->kernel_cache_keys[_i] = _e->Get(_i); } } else { _o->kernel_cache_keys.resize(0); } }
  { auto _e = kernel_cache_values(); if (_e) { _o->kernel_cache_values.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->kernel_cache_values[_i] = _e->Get(_i); } } else { _o->kernel_cache_values.resize(0); } }
}

inline ::flatbuffers::Offset<FusionExecutorCache> FusionExecutorCache::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const FusionExecutorCacheT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateFusionExecutorCache(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<FusionExecutorCache> CreateFusionExecutorCache(::flatbuffers::FlatBufferBuilder &_fbb, const FusionExecutorCacheT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const FusionExecutorCacheT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _inputs_cache = _o->inputs_cache ? CreateInputsIdLookup(_fbb, _o->inputs_cache.get(), _rehasher) : 0;
  auto _kernel_runtimes_map = _o->kernel_runtimes_map.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>> (_o->kernel_runtimes_map.size(), [](size_t i, _VectorArgs *__va) { return CreateKernelRuntimeState(*__va->__fbb, __va->__o->kernel_runtimes_map[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _kernel_cache_keys = _o->kernel_cache_keys.size() ? _fbb.CreateVector(_o->kernel_cache_keys) : 0;
  auto _kernel_cache_values = _o->kernel_cache_values.size() ? _fbb.CreateVector(_o->kernel_cache_values) : 0;
  return nvfuser::serde::CreateFusionExecutorCache(
      _fbb,
      _inputs_cache,
      _kernel_runtimes_map,
      _kernel_cache_keys,
      _kernel_cache_values);
}

inline RecordFunctorT *RecordFunctor::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<RecordFunctorT>(new RecordFunctorT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void RecordFunctor::UnPackTo(RecordFunctorT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = args(); if (_e) { _o->args.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->args[_i] = *_e->Get(_i); } } else { _o->args.resize(0); } }
  { auto _e = outputs(); if (_e) { _o->outputs.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->outputs[_i] = *_e->Get(_i); } } else { _o->outputs.resize(0); } }
  { auto _e = name(); if (_e) _o->name = _e->str(); }
  { auto _e = type(); _o->type = _e; }
  { auto _e = data_type(); _o->data.type = _e; }
  { auto _e = data(); if (_e) _o->data.value = nvfuser::serde::RecordDataUnion::UnPack(_e, data_type(), _resolver); }
}

inline ::flatbuffers::Offset<RecordFunctor> RecordFunctor::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const RecordFunctorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateRecordFunctor(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<RecordFunctor> CreateRecordFunctor(::flatbuffers::FlatBufferBuilder &_fbb, const RecordFunctorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const RecordFunctorT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _args = _o->args.size() ? _fbb.CreateVectorOfStructs(_o->args) : 0;
  auto _outputs = _o->outputs.size() ? _fbb.CreateVectorOfStructs(_o->outputs) : 0;
  auto _name = _o->name.empty() ? 0 : _fbb.CreateString(_o->name);
  auto _type = _o->type;
  auto _data_type = _o->data.type;
  auto _data = _o->data.Pack(_fbb);
  return nvfuser::serde::CreateRecordFunctor(
      _fbb,
      _args,
      _outputs,
      _name,
      _type,
      _data_type,
      _data);
}

inline TrieNodeT::TrieNodeT(const TrieNodeT &o)
      : record((o.record) ? new nvfuser::serde::RecordFunctorT(*o.record) : nullptr),
        children(o.children),
        fusion_id(o.fusion_id),
        visits(o.visits),
        is_terminal(o.is_terminal) {
}

inline TrieNodeT &TrieNodeT::operator=(TrieNodeT o) FLATBUFFERS_NOEXCEPT {
  std::swap(record, o.record);
  std::swap(children, o.children);
  std::swap(fusion_id, o.fusion_id);
  std::swap(visits, o.visits);
  std::swap(is_terminal, o.is_terminal);
  return *this;
}

inline TrieNodeT *TrieNode::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<TrieNodeT>(new TrieNodeT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void TrieNode::UnPackTo(TrieNodeT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = record(); if (_e) { if(_o->record) { _e->UnPackTo(_o->record.get(), _resolver); } else { _o->record = std::unique_ptr<nvfuser::serde::RecordFunctorT>(_e->UnPack(_resolver)); } } else if (_o->record) { _o->record.reset(); } }
  { auto _e = children(); if (_e) { _o->children.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->children[_i] = _e->Get(_i); } } else { _o->children.resize(0); } }
  { auto _e = fusion_id(); _o->fusion_id = _e; }
  { auto _e = visits(); _o->visits = _e; }
  { auto _e = is_terminal(); _o->is_terminal = _e; }
}

inline ::flatbuffers::Offset<TrieNode> TrieNode::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TrieNodeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateTrieNode(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<TrieNode> CreateTrieNode(::flatbuffers::FlatBufferBuilder &_fbb, const TrieNodeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const TrieNodeT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _record = _o->record ? CreateRecordFunctor(_fbb, _o->record.get(), _rehasher) : 0;
  auto _children = _o->children.size() ? _fbb.CreateVector(_o->children) : 0;
  auto _fusion_id = _o->fusion_id;
  auto _visits = _o->visits;
  auto _is_terminal = _o->is_terminal;
  return nvfuser::serde::CreateTrieNode(
      _fbb,
      _record,
      _children,
      _fusion_id,
      _visits,
      _is_terminal);
}

inline FusionCacheT::FusionCacheT(const FusionCacheT &o)
      : max_fusions(o.max_fusions),
        terminal_nodes(o.terminal_nodes) {
  structure.reserve(o.structure.size());
  for (const auto &structure_ : o.structure) { structure.emplace_back((structure_) ? new nvfuser::serde::TrieNodeT(*structure_) : nullptr); }
  auto_gen_schedules.reserve(o.auto_gen_schedules.size());
  for (const auto &auto_gen_schedules_ : o.auto_gen_schedules) { auto_gen_schedules.emplace_back((auto_gen_schedules_) ? new nvfuser::serde::FusionExecutorCacheT(*auto_gen_schedules_) : nullptr); }
}

inline FusionCacheT &FusionCacheT::operator=(FusionCacheT o) FLATBUFFERS_NOEXCEPT {
  std::swap(max_fusions, o.max_fusions);
  std::swap(structure, o.structure);
  std::swap(terminal_nodes, o.terminal_nodes);
  std::swap(auto_gen_schedules, o.auto_gen_schedules);
  return *this;
}

inline FusionCacheT *FusionCache::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<FusionCacheT>(new FusionCacheT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void FusionCache::UnPackTo(FusionCacheT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = max_fusions(); _o->max_fusions = _e; }
  { auto _e = structure(); if (_e) { _o->structure.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->structure[_i]) { _e->Get(_i)->UnPackTo(_o->structure[_i].get(), _resolver); } else { _o->structure[_i] = std::unique_ptr<nvfuser::serde::TrieNodeT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->structure.resize(0); } }
  { auto _e = terminal_nodes(); if (_e) { _o->terminal_nodes.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->terminal_nodes[_i] = _e->Get(_i); } } else { _o->terminal_nodes.resize(0); } }
  { auto _e = auto_gen_schedules(); if (_e) { _o->auto_gen_schedules.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->auto_gen_schedules[_i]) { _e->Get(_i)->UnPackTo(_o->auto_gen_schedules[_i].get(), _resolver); } else { _o->auto_gen_schedules[_i] = std::unique_ptr<nvfuser::serde::FusionExecutorCacheT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->auto_gen_schedules.resize(0); } }
}

inline ::flatbuffers::Offset<FusionCache> FusionCache::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const FusionCacheT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateFusionCache(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<FusionCache> CreateFusionCache(::flatbuffers::FlatBufferBuilder &_fbb, const FusionCacheT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const FusionCacheT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _max_fusions = _o->max_fusions;
  auto _structure = _o->structure.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::TrieNode>> (_o->structure.size(), [](size_t i, _VectorArgs *__va) { return CreateTrieNode(*__va->__fbb, __va->__o->structure[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _terminal_nodes = _o->terminal_nodes.size() ? _fbb.CreateVector(_o->terminal_nodes) : 0;
  auto _auto_gen_schedules = _o->auto_gen_schedules.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>> (_o->auto_gen_schedules.size(), [](size_t i, _VectorArgs *__va) { return CreateFusionExecutorCache(*__va->__fbb, __va->__o->auto_gen_schedules[i].get(), __va->__rehasher); }, &_va ) : 0;
  return nvfuser::serde::CreateFusionCache(
      _fbb,
      _max_fusions,
      _structure,
      _terminal_nodes,
      _auto_gen_schedules);
}

inline bool VerifyRecordData(::flatbuffers::Verifier &verifier, const void *obj, RecordData type) {
  switch (type) {
    case RecordData_NONE: {
      return true;
    }
    case RecordData_At: {
      auto ptr = reinterpret_cast<const nvfuser::serde::At *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_BatchNorm: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BatchNorm *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Broadcast: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Broadcast *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_BroadcastInDim: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BroadcastInDim *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Dimension: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Dimension *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Dtype: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Dtype *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Norm: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Norm *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Output: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Output *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Pad: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Pad *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Permute: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Permute *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Slice: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Slice *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Squeeze: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Squeeze *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Reduction: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Reduction *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Reshape: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Reshape *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Scalar *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Size: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Size *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Tensor: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Tensor *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_TensorCreation: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorCreation *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_TensorCreationSymbolic: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorCreationSymbolic *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Vector: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Vector *>(obj);
      return verifier.VerifyTable(ptr);
    }
    default: return true;
  }
}

inline bool VerifyRecordDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types) {
  if (!values || !types) return !values && !types;
  if (values->size() != types->size()) return false;
  for (::flatbuffers::uoffset_t i = 0; i < values->size(); ++i) {
    if (!VerifyRecordData(
        verifier,  values->Get(i), types->GetEnum<RecordData>(i))) {
      return false;
    }
  }
  return true;
}

inline void *RecordDataUnion::UnPack(const void *obj, RecordData type, const ::flatbuffers::resolver_function_t *resolver) {
  (void)resolver;
  switch (type) {
    case RecordData_At: {
      auto ptr = reinterpret_cast<const nvfuser::serde::At *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_BatchNorm: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BatchNorm *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Broadcast: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Broadcast *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_BroadcastInDim: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BroadcastInDim *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Dimension: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Dimension *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Dtype: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Dtype *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Norm: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Norm *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Output: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Output *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Pad: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Pad *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Permute: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Permute *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Slice: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Slice *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Squeeze: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Squeeze *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Reduction: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Reduction *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Reshape: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Reshape *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Scalar *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Size: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Size *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Tensor: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Tensor *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_TensorCreation: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorCreation *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_TensorCreationSymbolic: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorCreationSymbolic *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Vector: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Vector *>(obj);
      return ptr->UnPack(resolver);
    }
    default: return nullptr;
  }
}

inline ::flatbuffers::Offset<void> RecordDataUnion::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ::flatbuffers::rehasher_function_t *_rehasher) const {
  (void)_rehasher;
  switch (type) {
    case RecordData_At: {
      auto ptr = reinterpret_cast<const nvfuser::serde::AtT *>(value);
      return CreateAt(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_BatchNorm: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BatchNormT *>(value);
      return CreateBatchNorm(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Broadcast: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BroadcastT *>(value);
      return CreateBroadcast(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_BroadcastInDim: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BroadcastInDimT *>(value);
      return CreateBroadcastInDim(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Dimension: {
      auto ptr = reinterpret_cast<const nvfuser::serde::DimensionT *>(value);
      return CreateDimension(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Dtype: {
      auto ptr = reinterpret_cast<const nvfuser::serde::DtypeT *>(value);
      return CreateDtype(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Norm: {
      auto ptr = reinterpret_cast<const nvfuser::serde::NormT *>(value);
      return CreateNorm(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Output: {
      auto ptr = reinterpret_cast<const nvfuser::serde::OutputT *>(value);
      return CreateOutput(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Pad: {
      auto ptr = reinterpret_cast<const nvfuser::serde::PadT *>(value);
      return CreatePad(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Permute: {
      auto ptr = reinterpret_cast<const nvfuser::serde::PermuteT *>(value);
      return CreatePermute(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Slice: {
      auto ptr = reinterpret_cast<const nvfuser::serde::SliceT *>(value);
      return CreateSlice(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Squeeze: {
      auto ptr = reinterpret_cast<const nvfuser::serde::SqueezeT *>(value);
      return CreateSqueeze(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Reduction: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ReductionT *>(value);
      return CreateReduction(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Reshape: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ReshapeT *>(value);
      return CreateReshape(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ScalarT *>(value);
      return CreateScalar(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Size: {
      auto ptr = reinterpret_cast<const nvfuser::serde::SizeT *>(value);
      return CreateSize(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Tensor: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorT *>(value);
      return CreateTensor(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_TensorCreation: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorCreationT *>(value);
      return CreateTensorCreation(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_TensorCreationSymbolic: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorCreationSymbolicT *>(value);
      return CreateTensorCreationSymbolic(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Vector: {
      auto ptr = reinterpret_cast<const nvfuser::serde::VectorT *>(value);
      return CreateVector(_fbb, ptr, _rehasher).Union();
    }
    default: return 0;
  }
}

inline RecordDataUnion::RecordDataUnion(const RecordDataUnion &u) : type(u.type), value(nullptr) {
  switch (type) {
    case RecordData_At: {
      value = new nvfuser::serde::AtT(*reinterpret_cast<nvfuser::serde::AtT *>(u.value));
      break;
    }
    case RecordData_BatchNorm: {
      value = new nvfuser::serde::BatchNormT(*reinterpret_cast<nvfuser::serde::BatchNormT *>(u.value));
      break;
    }
    case RecordData_Broadcast: {
      value = new nvfuser::serde::BroadcastT(*reinterpret_cast<nvfuser::serde::BroadcastT *>(u.value));
      break;
    }
    case RecordData_BroadcastInDim: {
      value = new nvfuser::serde::BroadcastInDimT(*reinterpret_cast<nvfuser::serde::BroadcastInDimT *>(u.value));
      break;
    }
    case RecordData_Dimension: {
      value = new nvfuser::serde::DimensionT(*reinterpret_cast<nvfuser::serde::DimensionT *>(u.value));
      break;
    }
    case RecordData_Dtype: {
      value = new nvfuser::serde::DtypeT(*reinterpret_cast<nvfuser::serde::DtypeT *>(u.value));
      break;
    }
    case RecordData_Norm: {
      value = new nvfuser::serde::NormT(*reinterpret_cast<nvfuser::serde::NormT *>(u.value));
      break;
    }
    case RecordData_Output: {
      value = new nvfuser::serde::OutputT(*reinterpret_cast<nvfuser::serde::OutputT *>(u.value));
      break;
    }
    case RecordData_Pad: {
      value = new nvfuser::serde::PadT(*reinterpret_cast<nvfuser::serde::PadT *>(u.value));
      break;
    }
    case RecordData_Permute: {
      value = new nvfuser::serde::PermuteT(*reinterpret_cast<nvfuser::serde::PermuteT *>(u.value));
      break;
    }
    case RecordData_Slice: {
      value = new nvfuser::serde::SliceT(*reinterpret_cast<nvfuser::serde::SliceT *>(u.value));
      break;
    }
    case RecordData_Squeeze: {
      value = new nvfuser::serde::SqueezeT(*reinterpret_cast<nvfuser::serde::SqueezeT *>(u.value));
      break;
    }
    case RecordData_Reduction: {
      value = new nvfuser::serde::ReductionT(*reinterpret_cast<nvfuser::serde::ReductionT *>(u.value));
      break;
    }
    case RecordData_Reshape: {
      value = new nvfuser::serde::ReshapeT(*reinterpret_cast<nvfuser::serde::ReshapeT *>(u.value));
      break;
    }
    case RecordData_Scalar: {
      value = new nvfuser::serde::ScalarT(*reinterpret_cast<nvfuser::serde::ScalarT *>(u.value));
      break;
    }
    case RecordData_Size: {
      value = new nvfuser::serde::SizeT(*reinterpret_cast<nvfuser::serde::SizeT *>(u.value));
      break;
    }
    case RecordData_Tensor: {
      value = new nvfuser::serde::TensorT(*reinterpret_cast<nvfuser::serde::TensorT *>(u.value));
      break;
    }
    case RecordData_TensorCreation: {
      value = new nvfuser::serde::TensorCreationT(*reinterpret_cast<nvfuser::serde::TensorCreationT *>(u.value));
      break;
    }
    case RecordData_TensorCreationSymbolic: {
      value = new nvfuser::serde::TensorCreationSymbolicT(*reinterpret_cast<nvfuser::serde::TensorCreationSymbolicT *>(u.value));
      break;
    }
    case RecordData_Vector: {
      value = new nvfuser::serde::VectorT(*reinterpret_cast<nvfuser::serde::VectorT *>(u.value));
      break;
    }
    default:
      break;
  }
}

inline void RecordDataUnion::Reset() {
  switch (type) {
    case RecordData_At: {
      auto ptr = reinterpret_cast<nvfuser::serde::AtT *>(value);
      delete ptr;
      break;
    }
    case RecordData_BatchNorm: {
      auto ptr = reinterpret_cast<nvfuser::serde::BatchNormT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Broadcast: {
      auto ptr = reinterpret_cast<nvfuser::serde::BroadcastT *>(value);
      delete ptr;
      break;
    }
    case RecordData_BroadcastInDim: {
      auto ptr = reinterpret_cast<nvfuser::serde::BroadcastInDimT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Dimension: {
      auto ptr = reinterpret_cast<nvfuser::serde::DimensionT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Dtype: {
      auto ptr = reinterpret_cast<nvfuser::serde::DtypeT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Norm: {
      auto ptr = reinterpret_cast<nvfuser::serde::NormT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Output: {
      auto ptr = reinterpret_cast<nvfuser::serde::OutputT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Pad: {
      auto ptr = reinterpret_cast<nvfuser::serde::PadT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Permute: {
      auto ptr = reinterpret_cast<nvfuser::serde::PermuteT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Slice: {
      auto ptr = reinterpret_cast<nvfuser::serde::SliceT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Squeeze: {
      auto ptr = reinterpret_cast<nvfuser::serde::SqueezeT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Reduction: {
      auto ptr = reinterpret_cast<nvfuser::serde::ReductionT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Reshape: {
      auto ptr = reinterpret_cast<nvfuser::serde::ReshapeT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Scalar: {
      auto ptr = reinterpret_cast<nvfuser::serde::ScalarT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Size: {
      auto ptr = reinterpret_cast<nvfuser::serde::SizeT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Tensor: {
      auto ptr = reinterpret_cast<nvfuser::serde::TensorT *>(value);
      delete ptr;
      break;
    }
    case RecordData_TensorCreation: {
      auto ptr = reinterpret_cast<nvfuser::serde::TensorCreationT *>(value);
      delete ptr;
      break;
    }
    case RecordData_TensorCreationSymbolic: {
      auto ptr = reinterpret_cast<nvfuser::serde::TensorCreationSymbolicT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Vector: {
      auto ptr = reinterpret_cast<nvfuser::serde::VectorT *>(value);
      delete ptr;
      break;
    }
    default: break;
  }
  value = nullptr;
  type = RecordData_NONE;
}

inline bool VerifyPolymorphicValueData(::flatbuffers::Verifier &verifier, const void *obj, PolymorphicValueData type) {
  switch (type) {
    case PolymorphicValueData_NONE: {
      return true;
    }
    case PolymorphicValueData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Scalar *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case PolymorphicValueData_ScalarCpu: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ScalarCpu *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case PolymorphicValueData_TensorArg: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorArg *>(obj);
      return verifier.VerifyTable(ptr);
    }
    default: return true;
  }
}

inline bool VerifyPolymorphicValueDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types) {
  if (!values || !types) return !values && !types;
  if (values->size() != types->size()) return false;
  for (::flatbuffers::uoffset_t i = 0; i < values->size(); ++i) {
    if (!VerifyPolymorphicValueData(
        verifier,  values->Get(i), types->GetEnum<PolymorphicValueData>(i))) {
      return false;
    }
  }
  return true;
}

inline void *PolymorphicValueDataUnion::UnPack(const void *obj, PolymorphicValueData type, const ::flatbuffers::resolver_function_t *resolver) {
  (void)resolver;
  switch (type) {
    case PolymorphicValueData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Scalar *>(obj);
      return ptr->UnPack(resolver);
    }
    case PolymorphicValueData_ScalarCpu: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ScalarCpu *>(obj);
      return ptr->UnPack(resolver);
    }
    case PolymorphicValueData_TensorArg: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorArg *>(obj);
      return ptr->UnPack(resolver);
    }
    default: return nullptr;
  }
}

inline ::flatbuffers::Offset<void> PolymorphicValueDataUnion::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ::flatbuffers::rehasher_function_t *_rehasher) const {
  (void)_rehasher;
  switch (type) {
    case PolymorphicValueData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ScalarT *>(value);
      return CreateScalar(_fbb, ptr, _rehasher).Union();
    }
    case PolymorphicValueData_ScalarCpu: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ScalarCpuT *>(value);
      return CreateScalarCpu(_fbb, ptr, _rehasher).Union();
    }
    case PolymorphicValueData_TensorArg: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorArgT *>(value);
      return CreateTensorArg(_fbb, ptr, _rehasher).Union();
    }
    default: return 0;
  }
}

inline PolymorphicValueDataUnion::PolymorphicValueDataUnion(const PolymorphicValueDataUnion &u) : type(u.type), value(nullptr) {
  switch (type) {
    case PolymorphicValueData_Scalar: {
      value = new nvfuser::serde::ScalarT(*reinterpret_cast<nvfuser::serde::ScalarT *>(u.value));
      break;
    }
    case PolymorphicValueData_ScalarCpu: {
      value = new nvfuser::serde::ScalarCpuT(*reinterpret_cast<nvfuser::serde::ScalarCpuT *>(u.value));
      break;
    }
    case PolymorphicValueData_TensorArg: {
      value = new nvfuser::serde::TensorArgT(*reinterpret_cast<nvfuser::serde::TensorArgT *>(u.value));
      break;
    }
    default:
      break;
  }
}

inline void PolymorphicValueDataUnion::Reset() {
  switch (type) {
    case PolymorphicValueData_Scalar: {
      auto ptr = reinterpret_cast<nvfuser::serde::ScalarT *>(value);
      delete ptr;
      break;
    }
    case PolymorphicValueData_ScalarCpu: {
      auto ptr = reinterpret_cast<nvfuser::serde::ScalarCpuT *>(value);
      delete ptr;
      break;
    }
    case PolymorphicValueData_TensorArg: {
      auto ptr = reinterpret_cast<nvfuser::serde::TensorArgT *>(value);
      delete ptr;
      break;
    }
    default: break;
  }
  value = nullptr;
  type = PolymorphicValueData_NONE;
}

inline bool VerifyInstructionData(::flatbuffers::Verifier &verifier, const void *obj, InstructionData type) {
  switch (type) {
    case InstructionData_NONE: {
      return true;
    }
    case InstructionData_BinaryOp: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BinaryOp *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_GetAttr: {
      auto ptr = reinterpret_cast<const nvfuser::serde::GetAttr *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_GetItem: {
      auto ptr = reinterpret_cast<const nvfuser::serde::GetItem *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_GetMetaData: {
      auto ptr = reinterpret_cast<const nvfuser::serde::GetMetaData *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_Merge: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Merge *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_NamedScalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::NamedScalar *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_Resize: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Resize *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Scalar *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_Split: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Split *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_Swizzle2D: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Swizzle2D *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_Symbolic: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Symbolic *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_UnaryOp: {
      auto ptr = reinterpret_cast<const nvfuser::serde::UnaryOp *>(obj);
      return verifier.VerifyTable(ptr);
    }
    default: return true;
  }
}

inline bool VerifyInstructionDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types) {
  if (!values || !types) return !values && !types;
  if (values->size() != types->size()) return false;
  for (::flatbuffers::uoffset_t i = 0; i < values->size(); ++i) {
    if (!VerifyInstructionData(
        verifier,  values->Get(i), types->GetEnum<InstructionData>(i))) {
      return false;
    }
  }
  return true;
}

inline void *InstructionDataUnion::UnPack(const void *obj, InstructionData type, const ::flatbuffers::resolver_function_t *resolver) {
  (void)resolver;
  switch (type) {
    case InstructionData_BinaryOp: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BinaryOp *>(obj);
      return ptr->UnPack(resolver);
    }
    case InstructionData_GetAttr: {
      auto ptr = reinterpret_cast<const nvfuser::serde::GetAttr *>(obj);
      return ptr->UnPack(resolver);
    }
    case InstructionData_GetItem: {
      auto ptr = reinterpret_cast<const nvfuser::serde::GetItem *>(obj);
      return ptr->UnPack(resolver);
    }
    case InstructionData_GetMetaData: {
      auto ptr = reinterpret_cast<const nvfuser::serde::GetMetaData *>(obj);
      return ptr->UnPack(resolver);
    }
    case InstructionData_Merge: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Merge *>(obj);
      return ptr->UnPack(resolver);
    }
    case InstructionData_NamedScalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::NamedScalar *>(obj);
      return ptr->UnPack(resolver);
    }
    case InstructionData_Resize: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Resize *>(obj);
      return ptr->UnPack(resolver);
    }
    case InstructionData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Scalar *>(obj);
      return ptr->UnPack(resolver);
    }
    case InstructionData_Split: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Split *>(obj);
      return ptr->UnPack(resolver);
    }
    case InstructionData_Swizzle2D: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Swizzle2D *>(obj);
      return ptr->UnPack(resolver);
    }
    case InstructionData_Symbolic: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Symbolic *>(obj);
      return ptr->UnPack(resolver);
    }
    case InstructionData_UnaryOp: {
      auto ptr = reinterpret_cast<const nvfuser::serde::UnaryOp *>(obj);
      return ptr->UnPack(resolver);
    }
    default: return nullptr;
  }
}

inline ::flatbuffers::Offset<void> InstructionDataUnion::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ::flatbuffers::rehasher_function_t *_rehasher) const {
  (void)_rehasher;
  switch (type) {
    case InstructionData_BinaryOp: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BinaryOpT *>(value);
      return CreateBinaryOp(_fbb, ptr, _rehasher).Union();
    }
    case InstructionData_GetAttr: {
      auto ptr = reinterpret_cast<const nvfuser::serde::GetAttrT *>(value);
      return CreateGetAttr(_fbb, ptr, _rehasher).Union();
    }
    case InstructionData_GetItem: {
      auto ptr = reinterpret_cast<const nvfuser::serde::GetItemT *>(value);
      return CreateGetItem(_fbb, ptr, _rehasher).Union();
    }
    case InstructionData_GetMetaData: {
      auto ptr = reinterpret_cast<const nvfuser::serde::GetMetaDataT *>(value);
      return CreateGetMetaData(_fbb, ptr, _rehasher).Union();
    }
    case InstructionData_Merge: {
      auto ptr = reinterpret_cast<const nvfuser::serde::MergeT *>(value);
      return CreateMerge(_fbb, ptr, _rehasher).Union();
    }
    case InstructionData_NamedScalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::NamedScalarT *>(value);
      return CreateNamedScalar(_fbb, ptr, _rehasher).Union();
    }
    case InstructionData_Resize: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ResizeT *>(value);
      return CreateResize(_fbb, ptr, _rehasher).Union();
    }
    case InstructionData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ScalarT *>(value);
      return CreateScalar(_fbb, ptr, _rehasher).Union();
    }
    case InstructionData_Split: {
      auto ptr = reinterpret_cast<const nvfuser::serde::SplitT *>(value);
      return CreateSplit(_fbb, ptr, _rehasher).Union();
    }
    case InstructionData_Swizzle2D: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Swizzle2DT *>(value);
      return CreateSwizzle2D(_fbb, ptr, _rehasher).Union();
    }
    case InstructionData_Symbolic: {
      auto ptr = reinterpret_cast<const nvfuser::serde::SymbolicT *>(value);
      return CreateSymbolic(_fbb, ptr, _rehasher).Union();
    }
    case InstructionData_UnaryOp: {
      auto ptr = reinterpret_cast<const nvfuser::serde::UnaryOpT *>(value);
      return CreateUnaryOp(_fbb, ptr, _rehasher).Union();
    }
    default: return 0;
  }
}

inline InstructionDataUnion::InstructionDataUnion(const InstructionDataUnion &u) : type(u.type), value(nullptr) {
  switch (type) {
    case InstructionData_BinaryOp: {
      value = new nvfuser::serde::BinaryOpT(*reinterpret_cast<nvfuser::serde::BinaryOpT *>(u.value));
      break;
    }
    case InstructionData_GetAttr: {
      value = new nvfuser::serde::GetAttrT(*reinterpret_cast<nvfuser::serde::GetAttrT *>(u.value));
      break;
    }
    case InstructionData_GetItem: {
      value = new nvfuser::serde::GetItemT(*reinterpret_cast<nvfuser::serde::GetItemT *>(u.value));
      break;
    }
    case InstructionData_GetMetaData: {
      value = new nvfuser::serde::GetMetaDataT(*reinterpret_cast<nvfuser::serde::GetMetaDataT *>(u.value));
      break;
    }
    case InstructionData_Merge: {
      value = new nvfuser::serde::MergeT(*reinterpret_cast<nvfuser::serde::MergeT *>(u.value));
      break;
    }
    case InstructionData_NamedScalar: {
      value = new nvfuser::serde::NamedScalarT(*reinterpret_cast<nvfuser::serde::NamedScalarT *>(u.value));
      break;
    }
    case InstructionData_Resize: {
      value = new nvfuser::serde::ResizeT(*reinterpret_cast<nvfuser::serde::ResizeT *>(u.value));
      break;
    }
    case InstructionData_Scalar: {
      value = new nvfuser::serde::ScalarT(*reinterpret_cast<nvfuser::serde::ScalarT *>(u.value));
      break;
    }
    case InstructionData_Split: {
      value = new nvfuser::serde::SplitT(*reinterpret_cast<nvfuser::serde::SplitT *>(u.value));
      break;
    }
    case InstructionData_Swizzle2D: {
      value = new nvfuser::serde::Swizzle2DT(*reinterpret_cast<nvfuser::serde::Swizzle2DT *>(u.value));
      break;
    }
    case InstructionData_Symbolic: {
      value = new nvfuser::serde::SymbolicT(*reinterpret_cast<nvfuser::serde::SymbolicT *>(u.value));
      break;
    }
    case InstructionData_UnaryOp: {
      value = new nvfuser::serde::UnaryOpT(*reinterpret_cast<nvfuser::serde::UnaryOpT *>(u.value));
      break;
    }
    default:
      break;
  }
}

inline void InstructionDataUnion::Reset() {
  switch (type) {
    case InstructionData_BinaryOp: {
      auto ptr = reinterpret_cast<nvfuser::serde::BinaryOpT *>(value);
      delete ptr;
      break;
    }
    case InstructionData_GetAttr: {
      auto ptr = reinterpret_cast<nvfuser::serde::GetAttrT *>(value);
      delete ptr;
      break;
    }
    case InstructionData_GetItem: {
      auto ptr = reinterpret_cast<nvfuser::serde::GetItemT *>(value);
      delete ptr;
      break;
    }
    case InstructionData_GetMetaData: {
      auto ptr = reinterpret_cast<nvfuser::serde::GetMetaDataT *>(value);
      delete ptr;
      break;
    }
    case InstructionData_Merge: {
      auto ptr = reinterpret_cast<nvfuser::serde::MergeT *>(value);
      delete ptr;
      break;
    }
    case InstructionData_NamedScalar: {
      auto ptr = reinterpret_cast<nvfuser::serde::NamedScalarT *>(value);
      delete ptr;
      break;
    }
    case InstructionData_Resize: {
      auto ptr = reinterpret_cast<nvfuser::serde::ResizeT *>(value);
      delete ptr;
      break;
    }
    case InstructionData_Scalar: {
      auto ptr = reinterpret_cast<nvfuser::serde::ScalarT *>(value);
      delete ptr;
      break;
    }
    case InstructionData_Split: {
      auto ptr = reinterpret_cast<nvfuser::serde::SplitT *>(value);
      delete ptr;
      break;
    }
    case InstructionData_Swizzle2D: {
      auto ptr = reinterpret_cast<nvfuser::serde::Swizzle2DT *>(value);
      delete ptr;
      break;
    }
    case InstructionData_Symbolic: {
      auto ptr = reinterpret_cast<nvfuser::serde::SymbolicT *>(value);
      delete ptr;
      break;
    }
    case InstructionData_UnaryOp: {
      auto ptr = reinterpret_cast<nvfuser::serde::UnaryOpT *>(value);
      delete ptr;
      break;
    }
    default: break;
  }
  value = nullptr;
  type = InstructionData_NONE;
}

inline const nvfuser::serde::FusionCache *GetFusionCache(const void *buf) {
  return ::flatbuffers::GetRoot<nvfuser::serde::FusionCache>(buf);
}

inline const nvfuser::serde::FusionCache *GetSizePrefixedFusionCache(const void *buf) {
  return ::flatbuffers::GetSizePrefixedRoot<nvfuser::serde::FusionCache>(buf);
}

inline const char *FusionCacheIdentifier() {
  return "NV00";
}

inline bool FusionCacheBufferHasIdentifier(const void *buf) {
  return ::flatbuffers::BufferHasIdentifier(
      buf, FusionCacheIdentifier());
}

inline bool SizePrefixedFusionCacheBufferHasIdentifier(const void *buf) {
  return ::flatbuffers::BufferHasIdentifier(
      buf, FusionCacheIdentifier(), true);
}

inline bool VerifyFusionCacheBuffer(
    ::flatbuffers::Verifier &verifier) {
  return verifier.VerifyBuffer<nvfuser::serde::FusionCache>(FusionCacheIdentifier());
}

inline bool VerifySizePrefixedFusionCacheBuffer(
    ::flatbuffers::Verifier &verifier) {
  return verifier.VerifySizePrefixedBuffer<nvfuser::serde::FusionCache>(FusionCacheIdentifier());
}

inline void FinishFusionCacheBuffer(
    ::flatbuffers::FlatBufferBuilder &fbb,
    ::flatbuffers::Offset<nvfuser::serde::FusionCache> root) {
  fbb.Finish(root, FusionCacheIdentifier());
}

inline void FinishSizePrefixedFusionCacheBuffer(
    ::flatbuffers::FlatBufferBuilder &fbb,
    ::flatbuffers::Offset<nvfuser::serde::FusionCache> root) {
  fbb.FinishSizePrefixed(root, FusionCacheIdentifier());
}

inline std::unique_ptr<nvfuser::serde::FusionCacheT> UnPackFusionCache(
    const void *buf,
    const ::flatbuffers::resolver_function_t *res = nullptr) {
  return std::unique_ptr<nvfuser::serde::FusionCacheT>(GetFusionCache(buf)->UnPack(res));
}

inline std::unique_ptr<nvfuser::serde::FusionCacheT> UnPackSizePrefixedFusionCache(
    const void *buf,
    const ::flatbuffers::resolver_function_t *res = nullptr) {
  return std::unique_ptr<nvfuser::serde::FusionCacheT>(GetSizePrefixedFusionCache(buf)->UnPack(res));
}

}  // namespace serde
}  // namespace nvfuser

#endif  // FLATBUFFERS_GENERATED_FUSIONCACHE_NVFUSER_SERDE_H_
